{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":7984578,"sourceType":"datasetVersion","datasetId":4699957},{"sourceId":10634054,"sourceType":"datasetVersion","datasetId":6583945}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ResNet18 + MobileNetV2 + EfficientNetB0 /// 224 380 336\n# folcalloss is good for fer2013\n\n!pip install tensorflow\n!pip install numpy\n!pip install matplotlib\n!pip install psutil\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show tensorflow\n!pip show keras # If using Keras 3 separately\n!pip show retina-face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:58:17.110722Z","iopub.execute_input":"2025-04-06T13:58:17.111142Z","iopub.status.idle":"2025-04-06T13:58:28.706857Z","shell.execute_reply.started":"2025-04-06T13:58:17.111111Z","shell.execute_reply":"2025-04-06T13:58:28.705638Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Name: tensorflow\nVersion: 2.17.1\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\nRequired-by: dopamine_rl, retina-face, tensorflow-text, tensorflow_decision_forests, tf_keras\nName: keras\nVersion: 3.5.0\nSummary: Multi-backend Keras.\nHome-page: https://github.com/keras-team/keras\nAuthor: Keras team\nAuthor-email: keras-users@googlegroups.com\nLicense: Apache License 2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\nRequired-by: keras-tuner, tensorflow\nName: retina-face\nVersion: 0.0.17\nSummary: RetinaFace: Deep Face Detection Framework in TensorFlow for Python\nHome-page: https://github.com/serengil/retinaface\nAuthor: Sefik Ilkin Serengil\nAuthor-email: serengil@gmail.com\nLicense: UNKNOWN\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: gdown, numpy, opencv-python, Pillow, tensorflow\nRequired-by: \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install mtcnn requests opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:20:22.688365Z","iopub.execute_input":"2025-04-08T13:20:22.688659Z","iopub.status.idle":"2025-04-08T13:20:27.474482Z","shell.execute_reply.started":"2025-04-08T13:20:22.688629Z","shell.execute_reply":"2025-04-08T13:20:27.473647Z"}},"outputs":[{"name":"stdout","text":"Collecting mtcnn\n  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (1.4.2)\nCollecting lz4>=4.3.3 (from mtcnn)\n  Downloading lz4-4.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lz4-4.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lz4, mtcnn\nSuccessfully installed lz4-4.4.4 mtcnn-1.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# emotion_predictor_multi_model_dir_v2_distress.py\n\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import MTCNN\nimport matplotlib.pyplot as plt\nimport time\nimport glob\n\n# =============================================================================\n# Configuration\n# =============================================================================\n# --- Input ---\nINPUT_DIR = \"/kaggle/input/image-samples/image_samples\"\n# --- Models ---\nMODEL_PATH_B0 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\"\nMODEL_PATH_B4 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\"\nENSEMBLE_MODEL_PATH = \"/kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\"\n# --- Processing ---\nIMG_SIZE = 112\nCLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n# --- MTCNN ---\nMTCNN_MIN_CONFIDENCE = 0.90\n# --- Display ---\nMODEL_COLORS = {\n    'B0': (255, 100, 100), # Light Blue\n    'B4': (100, 255, 100), # Light Green\n    'Ensemble': (100, 100, 255), # Light Red\n    'Default': (200, 200, 200),\n    'Distress': (255, 165, 0) # Orange for Distress Index\n}\nTEXT_COLOR = (0, 0, 0)\nFONT = cv2.FONT_HERSHEY_SIMPLEX\nFONT_SCALE = 0.4\nFONT_THICKNESS = 1\nBOX_PADDING = 20\nCORNER_LENGTH_FACTOR = 0.15\nCORNER_THICKNESS = 2\nLINE_SPACING = 15\n\n# --- Output ---\nSAVE_OUTPUT = True\nOUTPUT_DIR = \"/kaggle/working/emotion_output_v2_distress\" # Changed output dir name\n\n# =============================================================================\n# Load ALL Emotion Models (Same as before)\n# =============================================================================\nprint(\"Loading emotion models...\")\nmodels = {}\ntry:\n    # ...(Loading code remains the same)...\n    print(f\"Loading Model B0: {MODEL_PATH_B0}\")\n    if not os.path.exists(MODEL_PATH_B0): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B0}\")\n    models['B0'] = tf.keras.models.load_model(MODEL_PATH_B0, compile=False)\n\n    print(f\"Loading Model B4: {MODEL_PATH_B4}\")\n    if not os.path.exists(MODEL_PATH_B4): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B4}\")\n    models['B4'] = tf.keras.models.load_model(MODEL_PATH_B4, compile=False)\n\n    print(f\"Loading Ensemble Model: {ENSEMBLE_MODEL_PATH}\")\n    if not os.path.exists(ENSEMBLE_MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {ENSEMBLE_MODEL_PATH}\")\n    models['Ensemble'] = tf.keras.models.load_model(ENSEMBLE_MODEL_PATH, compile=False)\n\n    print(\"All models loaded successfully:\")\n    for name, model in models.items():\n        print(f\"- {name}: {model.name}\")\n\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    exit()\n\n# =============================================================================\n# Initialize Face Detector (MTCNN) (Same as before)\n# =============================================================================\nprint(\"Initializing MTCNN face detector...\")\ntry:\n    detector = MTCNN()\n    print(\"MTCNN detector initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing MTCNN detector: {e}\")\n    exit()\n\n# =============================================================================\n# Helper Functions\n# =============================================================================\n\ndef preprocess_face_for_emotion(face_image, target_size):\n    # ...(Preprocessing code remains the same)...\n    try:\n        face_resized = tf.image.resize(face_image, [target_size, target_size], method='nearest')\n        face_processed = tf.cast(face_resized, tf.float32)\n        face_batch = tf.expand_dims(face_processed, axis=0)\n        return face_batch\n    except Exception as e:\n        print(f\"Error during face preprocessing: {e}\")\n        return None\n\ndef predict_emotions_all_models(face_batch, loaded_models, class_names):\n    # ...(Prediction code remains the same - returns top predictions)...\n    predictions = {}\n    if face_batch is None:\n        return {\"Error\": (\"Preprocessing failed\", 0.0)}\n    try:\n        for model_name, model in loaded_models.items():\n            preds_probs = model.predict(face_batch, verbose=0)\n            predicted_class_index = np.argmax(preds_probs[0])\n            confidence = np.max(preds_probs[0])\n            predicted_emotion = class_names[predicted_class_index]\n            predictions[model_name] = (predicted_emotion, confidence)\n        return predictions\n    except Exception as e:\n        print(f\"Error during emotion prediction: {e}\")\n        return {\"Error\": (f\"Prediction failed: {e}\", 0.0)}\n\n# --- NEW: Heuristic Distress Level Calculation Function ---\ndef calculate_heuristic_distress(emotion_predictions):\n    \"\"\"\n    Calculates a heuristic 'distress' score based on weighted emotion predictions.\n    WARNING: This is an experimental heuristic and NOT a validated measure of stress.\n    \"\"\"\n    # Define heuristic stress mappings (emotion -> weight)\n    distress_weights = {\n        'anger': 0.9,    # High associated weight\n        'fear': 0.85,    # High associated weight\n        'disgust': 0.75, # High associated weight\n        'contempt': 0.6, # Moderate associated weight\n        'sad': 0.55,     # Moderate associated weight\n        'surprise': 0.5, # Neutral associated weight (can be positive or negative stress)\n        'neutral': 0.2,  # Low associated weight\n        'happy': 0.1     # Very low associated weight\n    }\n\n    weighted_score_sum = 0.0\n    confidence_sum = 0.0\n\n    # Calculate score based on top prediction from each model\n    for model_name, prediction in emotion_predictions.items():\n         if model_name == \"Error\": continue\n         emotion, confidence = prediction\n         if emotion in distress_weights:\n             weighted_score_sum += distress_weights[emotion] * confidence\n             confidence_sum += confidence # Use confidence as the denominator weight\n\n    if confidence_sum > 0:\n        # Normalize score by sum of confidences\n        # This bounds the score between min_weight and max_weight approximately\n        normalized_score = weighted_score_sum / confidence_sum\n\n        # Map to categories based on arbitrary thresholds\n        if normalized_score > 0.65: # Threshold for HIGH\n            level = \"HIGH\"\n        elif normalized_score > 0.35: # Threshold for MODERATE\n            level = \"MODERATE\"\n        else:\n            level = \"LOW\"\n        # Return the category and the normalized score itself\n        return level, normalized_score\n    else:\n        return \"UNKNOWN\", 0.0\n\n# --- UPDATED Drawing Function ---\ndef draw_results_custom_v2_distress(frame, faces_data):\n    \"\"\"Draws corner boxes, stacked emotion labels, and heuristic distress level.\"\"\"\n    for data in faces_data:\n        box = data[\"box\"]\n        emotion_predictions = data[\"emotion_predictions\"] # Dict of top emotion preds\n        distress_info = data[\"distress\"] # Tuple (level, score)\n        x1, y1, x2, y2 = box\n\n        # --- Draw Bounding Box Corners ---\n        corner_length = int(min(x2 - x1, y2 - y1) * CORNER_LENGTH_FACTOR)\n        corner_color = MODEL_COLORS.get('Ensemble', MODEL_COLORS['Default'])\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2 - corner_length, y2), (x2, y2), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2, y2 - corner_length), (x2, y2), corner_color, CORNER_THICKNESS)\n\n        # --- Prepare and Draw Emotion Text Labels ---\n        y_offset = y1 - 7 # Starting position\n        max_w = 0 # Calculate max width needed for background\n        labels_to_draw = []\n        for model_name in ['B0', 'B4', 'Ensemble']: # Define order\n            if model_name in emotion_predictions:\n                emotion, confidence = emotion_predictions[model_name]\n                if model_name == \"Error\": continue\n                label = f\"{model_name}: {emotion} ({confidence:.2f})\"\n                labels_to_draw.append((label, model_name))\n                (w, h), _ = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n                if w > max_w: max_w = w\n\n        # Add Distress info label\n        distress_level, distress_score = distress_info\n        distress_label = f\"Distress Idx: {distress_level} ({distress_score:.2f})\" # Careful Labeling!\n        labels_to_draw.append((distress_label, 'Distress')) # Use specific key for color\n        (w, h), _ = cv2.getTextSize(distress_label, FONT, FONT_SCALE, FONT_THICKNESS)\n        if w > max_w: max_w = w\n\n        # Draw background and text for each label, moving upwards\n        num_labels = len(labels_to_draw)\n        bg_bottom = y1\n        for i, (label, label_type) in enumerate(reversed(labels_to_draw)):\n            text_y = y_offset - (i * LINE_SPACING)\n            bg_y1 = text_y - (LINE_SPACING - 3)\n            bg_y2 = text_y + 3\n\n            bg_y1 = max(0, bg_y1)\n            if bg_y1 >= bg_bottom or bg_y1 >= y1: continue\n            bg_bottom = bg_y1\n\n            # Use model color or specific distress color\n            bg_color = MODEL_COLORS.get(label_type, MODEL_COLORS['Default'])\n            cv2.rectangle(frame, (x1, bg_y1), (x1 + max_w + 5, bg_y2), bg_color, -1)\n            cv2.putText(frame, label, (x1 + 2, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS)\n\n\n# =============================================================================\n# Processing Functions for Image/Video Files (Integrate distress calc)\n# =============================================================================\n\ndef process_frame(frame, frame_rgb=None):\n    \"\"\"Detects faces, predicts emotions, calculates heuristic distress.\"\"\"\n    if frame_rgb is None:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    faces_data = []\n    try:\n        detections = detector.detect_faces(frame_rgb)\n    except Exception as e:\n        print(f\"Error during face detection: {e}\")\n        cv2.putText(frame, \"Face Detection Error\", (20, 40), FONT, 1, (0, 0, 255), 2)\n        return faces_data\n\n    if detections:\n         for face_info in detections:\n             try:\n                 confidence_face = face_info['confidence']\n                 if confidence_face < MTCNN_MIN_CONFIDENCE: continue\n\n                 x, y, w, h = face_info['box']\n                 x1 = max(0, x); y1 = max(0, y)\n                 x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                 if x1 >= x2 or y1 >= y2: continue\n\n                 x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                 x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                 if x1p >= x2p or y1p >= y2p: continue\n\n                 face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                 if face_crop_rgb.size == 0: continue\n\n                 face_batch = preprocess_face_for_emotion(face_crop_rgb, IMG_SIZE)\n                 # Get TOP prediction per model\n                 all_emotion_predictions = predict_emotions_all_models(face_batch, models, CLASS_NAMES)\n\n                 # --- Calculate Heuristic Distress ---\n                 distress_level, distress_score = calculate_heuristic_distress(all_emotion_predictions)\n\n                 faces_data.append({\n                     \"box\": (x1, y1, x2, y2),\n                     \"emotion_predictions\": all_emotion_predictions, # Store dict of top predictions\n                     \"distress\": (distress_level, distress_score) # Store distress info\n                 })\n             except Exception as e:\n                 print(f\"Error processing detected face: {e}\")\n                 try: cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                 except: pass\n    return faces_data\n\ndef process_image_file(image_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a single image file.\"\"\"\n    print(f\"\\nProcessing image: {image_path}\")\n    frame = cv2.imread(image_path)\n    if frame is None:\n        print(f\"Error: Could not read image file: {image_path}\")\n        return\n\n    faces_data = process_frame(frame)\n    # --- Use updated drawing function ---\n    draw_results_custom_v2_distress(frame, faces_data)\n\n    print(\"Prediction Results:\")\n    for i, res in enumerate(faces_data):\n        print(f\"  Face {i+1} - Box: {res['box']}, Emotions: {res['emotion_predictions']}, DistressIdx: {res['distress']}\") # Updated print\n\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(image_path))\n        try:\n            cv2.imwrite(output_filename, frame)\n            print(f\"Annotated image saved to {output_filename}\")\n        except Exception as e:\n            print(f\"Error saving image {output_filename}: {e}\")\n    else:\n        plt.figure(figsize=(12, 9))\n        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Detected Faces, Emotions & Distress Index (MTCNN - V2 Style) - {os.path.basename(image_path)}\") # Updated title\n        plt.axis('off')\n        plt.show()\n\ndef process_video_file(video_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a video file.\"\"\"\n    print(f\"\\nProcessing video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file: {video_path}\")\n        return\n\n    writer = None\n    if SAVE_OUTPUT:\n        # ...(VideoWriter setup remains the same)...\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(video_path))\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        if fps <= 0: fps = 25 # Default FPS\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        writer = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n        print(f\"Saving annotated video to {output_filename}\")\n\n\n    frame_count = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n\n        faces_data = process_frame(frame)\n        # --- Use updated drawing function ---\n        draw_results_custom_v2_distress(frame, faces_data)\n\n        # ...(FPS display remains the same)...\n        frame_count += 1\n        if frame_count > 1:\n             elapsed_time = time.time() - start_time\n             fps_display = frame_count / elapsed_time if elapsed_time > 0 else 0\n             cv2.putText(frame, f\"FPS: {fps_display:.2f}\", (10, 30), FONT, 0.7, (255, 255, 255), 2)\n\n\n        if writer:\n             writer.write(frame)\n        else:\n            cv2.imshow(f\"Emotion & Distress Index (MTCNN - V2 Style - Press 'q' to quit)\", frame) # Updated title\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    cap.release()\n    if writer:\n        writer.release()\n    cv2.destroyAllWindows()\n    print(f\"Finished processing video: {video_path}\")\n\n# =============================================================================\n# Main Execution - Iterate through Input Directory (Same as before)\n# =============================================================================\nif __name__ == \"__main__\":\n    # ...(Directory scanning logic remains the same)...\n    if not os.path.isdir(INPUT_DIR):\n        print(f\"Error: Input directory not found: {INPUT_DIR}\")\n        exit()\n\n    print(f\"Scanning directory: {INPUT_DIR}\")\n    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff')\n    video_extensions = ('*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv')\n\n    # Create output dir if saving\n    if SAVE_OUTPUT and not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if image_files:\n        print(f\"\\nFound {len(image_files)} image files to process...\")\n        for img_path in image_files:\n            process_image_file(img_path, OUTPUT_DIR)\n    else:\n        print(\"No image files found in the directory.\")\n\n    video_files = []\n    for ext in video_extensions:\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if video_files:\n        print(f\"\\nFound {len(video_files)} video files to process...\")\n        for vid_path in video_files:\n            process_video_file(vid_path, OUTPUT_DIR)\n    else:\n        print(\"No video files found in the directory.\")\n\n    print(\"\\nAll processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:20:27.475715Z","iopub.execute_input":"2025-04-08T13:20:27.475957Z"}},"outputs":[{"name":"stdout","text":"Loading emotion models...\nLoading Model B0: /kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\nLoading Model B4: /kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\nLoading Ensemble Model: /kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 24 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 26 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"All models loaded successfully:\n- B0: EfficientNetB0\n- B4: EfficientNetB4\n- Ensemble: Emotion_Ensemble_B0_B4\nInitializing MTCNN face detector...\nMTCNN detector initialized.\nScanning directory: /kaggle/input/image-samples/image_samples\nCreated output directory: /kaggle/working/emotion_output_v2_distress\n\nFound 14 image files to process...\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (3).jpg\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# emotion_predictor_multi_model_dir_v2.py\n\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import MTCNN\nimport matplotlib.pyplot as plt\nimport time\nimport glob\n\n# =============================================================================\n# Configuration\n# =============================================================================\n# --- Input ---\nINPUT_DIR = \"/kaggle/input/image-samples/image_samples\"\n# --- Models ---\nMODEL_PATH_B0 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\"\nMODEL_PATH_B4 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\"\nENSEMBLE_MODEL_PATH = \"/kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\"\n# --- Processing ---\nIMG_SIZE = 112\nCLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n# --- MTCNN ---\nMTCNN_MIN_CONFIDENCE = 0.90\n# --- Display ---\n# NEW: Define colors for each model type\nMODEL_COLORS = {\n    'B0': (255, 100, 100), # Light Blue\n    'B4': (100, 255, 100), # Light Green\n    'Ensemble': (100, 100, 255), # Light Red\n    'Default': (200, 200, 200) # Grey for others/errors\n}\nTEXT_COLOR = (0, 0, 0) # Black text for contrast on light backgrounds\nFONT = cv2.FONT_HERSHEY_SIMPLEX\n# NEW: Reduced font size\nFONT_SCALE = 0.4\nFONT_THICKNESS = 1\nBOX_PADDING = 20\nCORNER_LENGTH_FACTOR = 0.15 # Proportion of box size for corner lines\nCORNER_THICKNESS = 2 # Thickness for corner lines\n\n# --- Output ---\nSAVE_OUTPUT = True\nOUTPUT_DIR = \"/kaggle/working/emotion_output_v2\" # Changed output dir name\n\n# =============================================================================\n# Load ALL Emotion Models (Same as before)\n# =============================================================================\nprint(\"Loading emotion models...\")\nmodels = {}\ntry:\n    print(f\"Loading Model B0: {MODEL_PATH_B0}\")\n    if not os.path.exists(MODEL_PATH_B0): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B0}\")\n    models['B0'] = tf.keras.models.load_model(MODEL_PATH_B0, compile=False)\n\n    print(f\"Loading Model B4: {MODEL_PATH_B4}\")\n    if not os.path.exists(MODEL_PATH_B4): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B4}\")\n    models['B4'] = tf.keras.models.load_model(MODEL_PATH_B4, compile=False)\n\n    print(f\"Loading Ensemble Model: {ENSEMBLE_MODEL_PATH}\")\n    if not os.path.exists(ENSEMBLE_MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {ENSEMBLE_MODEL_PATH}\")\n    models['Ensemble'] = tf.keras.models.load_model(ENSEMBLE_MODEL_PATH, compile=False)\n\n    print(\"All models loaded successfully:\")\n    for name, model in models.items():\n        print(f\"- {name}: {model.name}\")\n\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    exit()\n\n# =============================================================================\n# Initialize Face Detector (MTCNN) (Same as before)\n# =============================================================================\nprint(\"Initializing MTCNN face detector...\")\ntry:\n    detector = MTCNN()\n    print(\"MTCNN detector initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing MTCNN detector: {e}\")\n    exit()\n\n# =============================================================================\n# Helper Functions (preprocess_face, predict_emotions_all_models - Same as before)\n# =============================================================================\n\ndef preprocess_face_for_emotion(face_image, target_size):\n    \"\"\"Preprocesses a face image (NumPy array) for the emotion model.\"\"\"\n    try:\n        face_resized = tf.image.resize(face_image, [target_size, target_size], method='nearest')\n        face_processed = tf.cast(face_resized, tf.float32)\n        face_batch = tf.expand_dims(face_processed, axis=0)\n        return face_batch\n    except Exception as e:\n        print(f\"Error during face preprocessing: {e}\")\n        return None\n\ndef predict_emotions_all_models(face_batch, loaded_models, class_names):\n    \"\"\"Predicts emotion using all loaded models.\"\"\"\n    predictions = {}\n    if face_batch is None:\n        return {\"Error\": (\"Preprocessing failed\", 0.0)}\n    try:\n        for model_name, model in loaded_models.items():\n            preds_probs = model.predict(face_batch, verbose=0)\n            predicted_class_index = np.argmax(preds_probs[0])\n            confidence = np.max(preds_probs[0])\n            predicted_emotion = class_names[predicted_class_index]\n            predictions[model_name] = (predicted_emotion, confidence) # Store tuple (label, confidence)\n        return predictions\n    except Exception as e:\n        print(f\"Error during emotion prediction: {e}\")\n        return {\"Error\": (f\"Prediction failed: {e}\", 0.0)}\n\n# --- *** UPDATED Drawing Function *** ---\ndef draw_results_custom(frame, faces_data):\n    \"\"\"Draws corner boxes and color-coded, aligned emotion labels.\"\"\"\n    for data in faces_data:\n        box = data[\"box\"]\n        predictions = data[\"predictions\"] # Dictionary of predictions\n        x1, y1, x2, y2 = box\n\n        # --- Draw Bounding Box Corners ---\n        corner_length = int(min(x2 - x1, y2 - y1) * CORNER_LENGTH_FACTOR)\n        # Define the main color for corners (can be model-specific if desired, but using one color is simpler)\n        corner_color = MODEL_COLORS.get('Ensemble', MODEL_COLORS['Default']) # Use Ensemble color for corners\n\n        # Top-left corner\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        # Bottom-right corner\n        cv2.line(frame, (x2 - corner_length, y2), (x2, y2), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2, y2 - corner_length), (x2, y2), corner_color, CORNER_THICKNESS)\n        # Optional: Top-right corner\n        # cv2.line(frame, (x2 - corner_length, y1), (x2, y1), corner_color, CORNER_THICKNESS)\n        # cv2.line(frame, (x2, y1), (x2, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        # Optional: Bottom-left corner\n        # cv2.line(frame, (x1, y2), (x1 + corner_length, y2), corner_color, CORNER_THICKNESS)\n        # cv2.line(frame, (x1, y2 - corner_length), (x1, y2), corner_color, CORNER_THICKNESS)\n\n        # --- Prepare and Draw Text Labels ---\n        y_offset = y1 - 7 # Starting position slightly higher for smaller font\n        line_height = int(FONT_SCALE * 30) # Adjust spacing based on font scale\n        max_w = 0 # Calculate max width needed for background\n\n        # Determine text width first\n        labels_to_draw = []\n        for model_name, (emotion, confidence) in predictions.items():\n             if model_name == \"Error\": continue\n             label = f\"{model_name}: {emotion} ({confidence:.2f})\"\n             labels_to_draw.append((label, model_name)) # Store label and model name for color lookup\n             (w, h), _ = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n             if w > max_w: max_w = w\n\n        # Draw background and text for each label, moving upwards\n        num_labels = len(labels_to_draw)\n        for i, (label, model_name) in enumerate(reversed(labels_to_draw)): # Draw from bottom up\n            text_y = y_offset - (i * line_height)\n            bg_y1 = text_y - (line_height - int(FONT_SCALE*10)) # Adjust background size based on font\n            bg_y2 = text_y + int(FONT_SCALE*5)\n\n            # Ensure background is within frame boundaries\n            bg_y1 = max(0, bg_y1)\n            if bg_y1 >= bg_y2 or bg_y1 >= y1: continue # Prevent overlap with box top or invalid rect\n\n            # Get model-specific color for background\n            bg_color = MODEL_COLORS.get(model_name, MODEL_COLORS['Default'])\n\n            # Draw background rectangle\n            cv2.rectangle(frame, (x1, bg_y1), (x1 + max_w + 5, bg_y2), bg_color, -1) # Add padding to width\n            # Draw text\n            cv2.putText(frame, label, (x1 + 2, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS) # Add small x offset\n\n\n# =============================================================================\n# Processing Functions for Image/Video Files (Updated to use draw_results_custom)\n# =============================================================================\n\ndef process_frame(frame, frame_rgb=None):\n    \"\"\"Detects faces and predicts emotions on a single frame.\"\"\"\n    if frame_rgb is None:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    faces_data = []\n    try:\n        detections = detector.detect_faces(frame_rgb)\n    except Exception as e:\n        print(f\"Error during face detection: {e}\")\n        cv2.putText(frame, \"Face Detection Error\", (20, 40), FONT, 1, (0, 0, 255), 2)\n        return faces_data\n\n    if detections:\n         for face_info in detections:\n             try:\n                 confidence_face = face_info['confidence']\n                 if confidence_face < MTCNN_MIN_CONFIDENCE: continue\n\n                 x, y, w, h = face_info['box']\n                 x1 = max(0, x); y1 = max(0, y)\n                 x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                 if x1 >= x2 or y1 >= y2: continue\n\n                 x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                 x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                 if x1p >= x2p or y1p >= y2p: continue\n\n                 face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                 if face_crop_rgb.size == 0: continue\n\n                 face_batch = preprocess_face_for_emotion(face_crop_rgb, IMG_SIZE)\n                 all_predictions = predict_emotions_all_models(face_batch, models, CLASS_NAMES)\n\n                 faces_data.append({\n                     \"box\": (x1, y1, x2, y2),\n                     \"predictions\": all_predictions\n                 })\n             except Exception as e:\n                 print(f\"Error processing detected face: {e}\")\n                 try: cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                 except: pass\n    return faces_data\n\ndef process_image_file(image_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a single image file.\"\"\"\n    print(f\"\\nProcessing image: {image_path}\")\n    frame = cv2.imread(image_path)\n    if frame is None:\n        print(f\"Error: Could not read image file: {image_path}\")\n        return\n\n    faces_data = process_frame(frame)\n    # --- Use updated drawing function ---\n    draw_results_custom(frame, faces_data)\n\n    print(\"Prediction Results:\")\n    for i, res in enumerate(faces_data):\n        print(f\"  Face {i+1} - Box: {res['box']}, Predictions: {res['predictions']}\")\n\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(image_path))\n        try:\n            cv2.imwrite(output_filename, frame)\n            print(f\"Annotated image saved to {output_filename}\")\n        except Exception as e:\n            print(f\"Error saving image {output_filename}: {e}\")\n    else:\n        plt.figure(figsize=(12, 9))\n        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Detected Faces and Emotions (MTCNN) - {os.path.basename(image_path)}\")\n        plt.axis('off')\n        plt.show()\n\ndef process_video_file(video_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a video file.\"\"\"\n    print(f\"\\nProcessing video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file: {video_path}\")\n        return\n\n    writer = None\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(video_path))\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        if fps <= 0: # Handle case where fps is not read correctly\n            print(\"Warning: Could not read video FPS, defaulting to 25.\")\n            fps = 25\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        writer = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n        print(f\"Saving annotated video to {output_filename}\")\n\n\n    frame_count = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n\n        faces_data = process_frame(frame)\n        # --- Use updated drawing function ---\n        draw_results_custom(frame, faces_data)\n\n        frame_count += 1\n        if frame_count > 1 and is_video: # Added is_video check\n             elapsed_time = time.time() - start_time\n             fps_display = frame_count / elapsed_time if elapsed_time > 0 else 0\n             cv2.putText(frame, f\"FPS: {fps_display:.2f}\", (10, 30), FONT, 0.7, (255, 255, 255), 2)\n\n        if writer:\n             writer.write(frame)\n        else:\n            cv2.imshow(f\"Emotion Recognition (MTCNN - Press 'q' to quit) - {os.path.basename(video_path)}\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    cap.release()\n    if writer:\n        writer.release()\n    cv2.destroyAllWindows()\n    print(f\"Finished processing video: {video_path}\")\n\n# =============================================================================\n# Main Execution - Iterate through Input Directory (Same as before)\n# =============================================================================\nif __name__ == \"__main__\":\n    if not os.path.isdir(INPUT_DIR):\n        print(f\"Error: Input directory not found: {INPUT_DIR}\")\n        exit()\n\n    print(f\"Scanning directory: {INPUT_DIR}\")\n    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff')\n    video_extensions = ('*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv')\n\n    # Create output dir if saving\n    if SAVE_OUTPUT and not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper()))) # Include uppercase extensions\n\n    if image_files:\n        print(f\"\\nFound {len(image_files)} image files to process...\")\n        for img_path in image_files:\n            process_image_file(img_path, OUTPUT_DIR)\n    else:\n        print(\"No image files found in the directory.\")\n\n    video_files = []\n    for ext in video_extensions:\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper()))) # Include uppercase extensions\n\n\n    if video_files:\n        print(f\"\\nFound {len(video_files)} video files to process...\")\n        for vid_path in video_files:\n            process_video_file(vid_path, OUTPUT_DIR)\n    else:\n        print(\"No video files found in the directory.\")\n\n    print(\"\\nAll processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:48:08.798369Z","iopub.execute_input":"2025-04-06T15:48:08.798863Z","iopub.status.idle":"2025-04-06T15:51:29.945822Z","shell.execute_reply.started":"2025-04-06T15:48:08.798807Z","shell.execute_reply":"2025-04-06T15:51:29.944600Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Loading emotion models...\nLoading Model B0: /kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\nLoading Model B4: /kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\nLoading Ensemble Model: /kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\nAll models loaded successfully:\n- B0: EfficientNetB0\n- B4: EfficientNetB4\n- Ensemble: Emotion_Ensemble_B0_B4\nInitializing MTCNN face detector...\nMTCNN detector initialized.\nScanning directory: /kaggle/input/image-samples/image_samples\nCreated output directory: /kaggle/working/emotion_output_v2\n\nFound 14 image files to process...\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (3).jpg\nPrediction Results:\n  Face 1 - Box: (544, 148, 617, 238), Predictions: {'B0': ('fear', 0.30842683), 'B4': ('neutral', 0.328221), 'Ensemble': ('neutral', 0.198)}\n  Face 2 - Box: (327, 0, 385, 50), Predictions: {'B0': ('fear', 0.36959267), 'B4': ('neutral', 0.44093916), 'Ensemble': ('sad', 0.2673)}\n  Face 3 - Box: (522, 255, 618, 379), Predictions: {'B0': ('disgust', 0.80638206), 'B4': ('sad', 0.41102034), 'Ensemble': ('disgust', 0.469)}\n  Face 4 - Box: (168, 279, 263, 397), Predictions: {'B0': ('contempt', 0.60939354), 'B4': ('contempt', 0.50564885), 'Ensemble': ('contempt', 0.5576)}\n  Face 5 - Box: (477, 56, 553, 156), Predictions: {'B0': ('anger', 0.34072763), 'B4': ('contempt', 0.59309644), 'Ensemble': ('contempt', 0.352)}\n  Face 6 - Box: (243, 129, 336, 254), Predictions: {'B0': ('happy', 0.41193572), 'B4': ('happy', 0.67072695), 'Ensemble': ('happy', 0.5415)}\n  Face 7 - Box: (297, 259, 449, 474), Predictions: {'B0': ('contempt', 0.7652605), 'B4': ('contempt', 0.4311538), 'Ensemble': ('contempt', 0.598)}\n  Face 8 - Box: (225, 47, 283, 123), Predictions: {'B0': ('neutral', 0.32564476), 'B4': ('fear', 0.35492992), 'Ensemble': ('fear', 0.2223)}\n  Face 9 - Box: (56, 196, 163, 331), Predictions: {'B0': ('contempt', 0.45765477), 'B4': ('contempt', 0.3995895), 'Ensemble': ('contempt', 0.4287)}\n  Face 10 - Box: (111, 109, 185, 198), Predictions: {'B0': ('surprise', 0.47807428), 'B4': ('contempt', 0.29308137), 'Ensemble': ('surprise', 0.304)}\n  Face 11 - Box: (376, 85, 454, 186), Predictions: {'B0': ('disgust', 0.30615318), 'B4': ('sad', 0.61213255), 'Ensemble': ('sad', 0.3748)}\n  Face 12 - Box: (304, 71, 356, 132), Predictions: {'B0': ('neutral', 0.36589587), 'B4': ('disgust', 0.23842023), 'Ensemble': ('neutral', 0.219)}\n  Face 13 - Box: (448, 232, 514, 326), Predictions: {'B0': ('disgust', 0.36840755), 'B4': ('neutral', 0.572632), 'Ensemble': ('neutral', 0.3577)}\n  Face 14 - Box: (406, 2, 463, 76), Predictions: {'B0': ('sad', 0.28610402), 'B4': ('happy', 0.47713763), 'Ensemble': ('happy', 0.2705)}\n  Face 15 - Box: (286, 0, 331, 40), Predictions: {'B0': ('sad', 0.792134), 'B4': ('contempt', 0.4878845), 'Ensemble': ('sad', 0.4397)}\n  Face 16 - Box: (20, 46, 83, 123), Predictions: {'B0': ('happy', 0.3485463), 'B4': ('contempt', 0.4510849), 'Ensemble': ('contempt', 0.2988)}\n  Face 17 - Box: (117, 0, 174, 66), Predictions: {'B0': ('sad', 0.32152736), 'B4': ('contempt', 0.34387743), 'Ensemble': ('contempt', 0.3154)}\n  Face 18 - Box: (188, 101, 245, 185), Predictions: {'B0': ('sad', 0.45087844), 'B4': ('anger', 0.3395815), 'Ensemble': ('sad', 0.3489)}\n  Face 19 - Box: (193, 11, 249, 83), Predictions: {'B0': ('sad', 0.36574414), 'B4': ('happy', 0.3200078), 'Ensemble': ('sad', 0.2437)}\n  Face 20 - Box: (587, 31, 639, 105), Predictions: {'B0': ('sad', 0.64983916), 'B4': ('sad', 0.2644427), 'Ensemble': ('sad', 0.457)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_groupy (3).jpg\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (2).jpg\nPrediction Results:\n  Face 1 - Box: (67, 140, 115, 199), Predictions: {'B0': ('sad', 0.30355388), 'B4': ('disgust', 0.20038925), 'Ensemble': ('disgust', 0.2336)}\n  Face 2 - Box: (238, 157, 273, 205), Predictions: {'B0': ('surprise', 0.4423589), 'B4': ('happy', 0.27900505), 'Ensemble': ('surprise', 0.2983)}\n  Face 3 - Box: (259, 101, 291, 142), Predictions: {'B0': ('happy', 0.23231757), 'B4': ('contempt', 0.5391832), 'Ensemble': ('contempt', 0.354)}\n  Face 4 - Box: (405, 165, 442, 213), Predictions: {'B0': ('happy', 0.72154444), 'B4': ('happy', 0.742383), 'Ensemble': ('happy', 0.732)}\n  Face 5 - Box: (566, 179, 604, 232), Predictions: {'B0': ('disgust', 0.4313828), 'B4': ('fear', 0.32330772), 'Ensemble': ('disgust', 0.3074)}\n  Face 6 - Box: (452, 166, 487, 209), Predictions: {'B0': ('neutral', 0.46767157), 'B4': ('neutral', 0.6876332), 'Ensemble': ('neutral', 0.5776)}\n  Face 7 - Box: (196, 147, 235, 202), Predictions: {'B0': ('happy', 0.65735614), 'B4': ('happy', 0.4460284), 'Ensemble': ('happy', 0.552)}\n  Face 8 - Box: (296, 198, 330, 247), Predictions: {'B0': ('anger', 0.37501636), 'B4': ('contempt', 0.28197345), 'Ensemble': ('anger', 0.24)}\n  Face 9 - Box: (334, 177, 374, 228), Predictions: {'B0': ('surprise', 0.25600365), 'B4': ('disgust', 0.334862), 'Ensemble': ('disgust', 0.1748)}\n  Face 10 - Box: (525, 235, 571, 297), Predictions: {'B0': ('anger', 0.6530419), 'B4': ('sad', 0.34915772), 'Ensemble': ('anger', 0.4714)}\n  Face 11 - Box: (513, 167, 568, 225), Predictions: {'B0': ('disgust', 0.8575974), 'B4': ('surprise', 0.5432999), 'Ensemble': ('disgust', 0.4875)}\n  Face 12 - Box: (112, 132, 151, 183), Predictions: {'B0': ('sad', 0.33772), 'B4': ('happy', 0.20191114), 'Ensemble': ('sad', 0.2063)}\n  Face 13 - Box: (152, 197, 198, 254), Predictions: {'B0': ('fear', 0.42171314), 'B4': ('surprise', 0.29338723), 'Ensemble': ('fear', 0.242)}\n  Face 14 - Box: (615, 185, 639, 233), Predictions: {'B0': ('surprise', 0.2326604), 'B4': ('disgust', 0.2671181), 'Ensemble': ('anger', 0.2083)}\n  Face 15 - Box: (576, 155, 612, 198), Predictions: {'B0': ('neutral', 0.41663167), 'B4': ('fear', 0.5864419), 'Ensemble': ('fear', 0.3071)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_groupy (2).jpg\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (1).jpg\nPrediction Results:\n  Face 1 - Box: (504, 191, 577, 293), Predictions: {'B0': ('surprise', 0.29889703), 'B4': ('sad', 0.56461257), 'Ensemble': ('sad', 0.3315)}\n  Face 2 - Box: (443, 201, 506, 295), Predictions: {'B0': ('disgust', 0.4288793), 'B4': ('happy', 0.72132844), 'Ensemble': ('happy', 0.3867)}\n  Face 3 - Box: (485, 466, 571, 580), Predictions: {'B0': ('disgust', 0.7202098), 'B4': ('disgust', 0.57318044), 'Ensemble': ('disgust', 0.6465)}\n  Face 4 - Box: (204, 521, 281, 587), Predictions: {'B0': ('neutral', 0.33516258), 'B4': ('neutral', 0.43285915), 'Ensemble': ('neutral', 0.384)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_groupy (1).jpg\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (4).jpg\nPrediction Results:\n  Face 1 - Box: (302, 35, 349, 103), Predictions: {'B0': ('disgust', 0.48746297), 'B4': ('surprise', 0.5294346), 'Ensemble': ('surprise', 0.32)}\n  Face 2 - Box: (61, 45, 152, 158), Predictions: {'B0': ('happy', 0.3622056), 'B4': ('happy', 0.56741285), 'Ensemble': ('happy', 0.4648)}\n  Face 3 - Box: (324, 156, 441, 311), Predictions: {'B0': ('contempt', 0.854277), 'B4': ('contempt', 0.90750366), 'Ensemble': ('contempt', 0.881)}\n  Face 4 - Box: (374, 0, 440, 79), Predictions: {'B0': ('surprise', 0.43739742), 'B4': ('contempt', 0.27065292), 'Ensemble': ('surprise', 0.3237)}\n  Face 5 - Box: (473, 102, 554, 207), Predictions: {'B0': ('anger', 0.31082785), 'B4': ('happy', 0.5356825), 'Ensemble': ('happy', 0.288)}\n  Face 6 - Box: (150, 31, 200, 101), Predictions: {'B0': ('happy', 0.30320808), 'B4': ('happy', 0.4021316), 'Ensemble': ('happy', 0.3525)}\n  Face 7 - Box: (223, 31, 282, 112), Predictions: {'B0': ('anger', 0.54681426), 'B4': ('disgust', 0.31476018), 'Ensemble': ('anger', 0.2856)}\n  Face 8 - Box: (441, 43, 489, 104), Predictions: {'B0': ('anger', 0.42242914), 'B4': ('happy', 0.3563545), 'Ensemble': ('anger', 0.3481)}\n  Face 9 - Box: (211, 143, 291, 259), Predictions: {'B0': ('surprise', 0.36175996), 'B4': ('fear', 0.5599653), 'Ensemble': ('fear', 0.3938)}\n  Face 10 - Box: (148, 124, 217, 220), Predictions: {'B0': ('fear', 0.31800002), 'B4': ('happy', 0.34374905), 'Ensemble': ('fear', 0.2446)}\n  Face 11 - Box: (500, 32, 560, 104), Predictions: {'B0': ('anger', 0.7880643), 'B4': ('surprise', 0.3705679), 'Ensemble': ('anger', 0.4558)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_groupy (4).jpg\n\nProcessing image: /kaggle/input/image-samples/image_samples/groupy (5).jpg\nPrediction Results:\n  Face 1 - Box: (174, 94, 221, 147), Predictions: {'B0': ('happy', 0.9364513), 'B4': ('happy', 0.9403455), 'Ensemble': ('happy', 0.9385)}\n  Face 2 - Box: (432, 142, 479, 197), Predictions: {'B0': ('sad', 0.36068326), 'B4': ('happy', 0.48243296), 'Ensemble': ('happy', 0.4004)}\n  Face 3 - Box: (8, 177, 56, 234), Predictions: {'B0': ('happy', 0.40786868), 'B4': ('sad', 0.40952218), 'Ensemble': ('sad', 0.2588)}\n  Face 4 - Box: (237, 128, 284, 185), Predictions: {'B0': ('neutral', 0.3123644), 'B4': ('contempt', 0.314843), 'Ensemble': ('neutral', 0.2405)}\n  Face 5 - Box: (136, 248, 187, 315), Predictions: {'B0': ('happy', 0.33462688), 'B4': ('sad', 0.23619656), 'Ensemble': ('happy', 0.2808)}\n  Face 6 - Box: (622, 165, 674, 245), Predictions: {'B0': ('disgust', 0.68902427), 'B4': ('happy', 0.21068035), 'Ensemble': ('disgust', 0.4084)}\n  Face 7 - Box: (704, 22, 745, 74), Predictions: {'B0': ('happy', 0.39893702), 'B4': ('happy', 0.5360421), 'Ensemble': ('happy', 0.4675)}\n  Face 8 - Box: (775, 126, 852, 229), Predictions: {'B0': ('anger', 0.49476266), 'B4': ('anger', 0.48019263), 'Ensemble': ('anger', 0.4875)}\n  Face 9 - Box: (551, 142, 622, 236), Predictions: {'B0': ('contempt', 0.43324968), 'B4': ('happy', 0.57310826), 'Ensemble': ('happy', 0.3794)}\n  Face 10 - Box: (463, 80, 508, 135), Predictions: {'B0': ('happy', 0.48679477), 'B4': ('neutral', 0.4602602), 'Ensemble': ('neutral', 0.462)}\n  Face 11 - Box: (672, 215, 743, 307), Predictions: {'B0': ('anger', 0.43660933), 'B4': ('sad', 0.3940853), 'Ensemble': ('anger', 0.2888)}\n  Face 12 - Box: (545, 381, 617, 480), Predictions: {'B0': ('surprise', 0.46262893), 'B4': ('anger', 0.33282006), 'Ensemble': ('surprise', 0.2542)}\n  Face 13 - Box: (1075, 57, 1117, 110), Predictions: {'B0': ('happy', 0.38492128), 'B4': ('neutral', 0.46856946), 'Ensemble': ('neutral', 0.3428)}\n  Face 14 - Box: (50, 96, 96, 155), Predictions: {'B0': ('happy', 0.79024976), 'B4': ('surprise', 0.58002037), 'Ensemble': ('happy', 0.505)}\n  Face 15 - Box: (249, 232, 305, 300), Predictions: {'B0': ('neutral', 0.38358533), 'B4': ('neutral', 0.47196466), 'Ensemble': ('neutral', 0.4277)}\n  Face 16 - Box: (1138, 115, 1185, 177), Predictions: {'B0': ('neutral', 0.32823312), 'B4': ('sad', 0.37052348), 'Ensemble': ('neutral', 0.2556)}\n  Face 17 - Box: (318, 90, 357, 140), Predictions: {'B0': ('happy', 0.6876339), 'B4': ('happy', 0.85713756), 'Ensemble': ('happy', 0.7725)}\n  Face 18 - Box: (938, 157, 1002, 248), Predictions: {'B0': ('anger', 0.34725177), 'B4': ('happy', 0.6394854), 'Ensemble': ('happy', 0.3613)}\n  Face 19 - Box: (431, 258, 500, 349), Predictions: {'B0': ('contempt', 0.55034256), 'B4': ('happy', 0.53529376), 'Ensemble': ('contempt', 0.3271)}\n  Face 20 - Box: (164, 177, 211, 232), Predictions: {'B0': ('happy', 0.3080455), 'B4': ('happy', 0.25499114), 'Ensemble': ('happy', 0.2815)}\n  Face 21 - Box: (830, 20, 875, 81), Predictions: {'B0': ('neutral', 0.28598356), 'B4': ('sad', 0.7711189), 'Ensemble': ('sad', 0.4165)}\n  Face 22 - Box: (75, 56, 115, 105), Predictions: {'B0': ('neutral', 0.6044019), 'B4': ('neutral', 0.3332138), 'Ensemble': ('neutral', 0.4688)}\n  Face 23 - Box: (444, 25, 483, 76), Predictions: {'B0': ('sad', 0.3984444), 'B4': ('happy', 0.35090965), 'Ensemble': ('sad', 0.3452)}\n  Face 24 - Box: (875, 120, 918, 176), Predictions: {'B0': ('neutral', 0.40700132), 'B4': ('neutral', 0.57219964), 'Ensemble': ('neutral', 0.4897)}\n  Face 25 - Box: (301, 273, 361, 365), Predictions: {'B0': ('happy', 0.23716584), 'B4': ('neutral', 0.7343216), 'Ensemble': ('neutral', 0.4187)}\n  Face 26 - Box: (520, 340, 582, 426), Predictions: {'B0': ('happy', 0.24826095), 'B4': ('neutral', 0.47260255), 'Ensemble': ('neutral', 0.3357)}\n  Face 27 - Box: (303, 27, 342, 73), Predictions: {'B0': ('neutral', 0.7970916), 'B4': ('sad', 0.61064404), 'Ensemble': ('neutral', 0.453)}\n  Face 28 - Box: (228, 15, 264, 62), Predictions: {'B0': ('neutral', 0.4688802), 'B4': ('sad', 0.9599127), 'Ensemble': ('sad', 0.537)}\n  Face 29 - Box: (735, 321, 803, 418), Predictions: {'B0': ('anger', 0.29315066), 'B4': ('anger', 0.44558692), 'Ensemble': ('anger', 0.3694)}\n  Face 30 - Box: (1028, 15, 1068, 67), Predictions: {'B0': ('neutral', 0.54890525), 'B4': ('neutral', 0.24802952), 'Ensemble': ('neutral', 0.3984)}\n  Face 31 - Box: (924, 35, 967, 92), Predictions: {'B0': ('sad', 0.44180873), 'B4': ('happy', 0.29601258), 'Ensemble': ('sad', 0.3184)}\n  Face 32 - Box: (108, 0, 145, 44), Predictions: {'B0': ('sad', 0.48730117), 'B4': ('sad', 0.8192765), 'Ensemble': ('sad', 0.6533)}\n  Face 33 - Box: (23, 11, 58, 55), Predictions: {'B0': ('sad', 0.84596795), 'B4': ('happy', 0.2515633), 'Ensemble': ('sad', 0.504)}\n  Face 34 - Box: (494, 144, 542, 196), Predictions: {'B0': ('neutral', 0.6188016), 'B4': ('happy', 0.37652385), 'Ensemble': ('neutral', 0.327)}\n  Face 35 - Box: (300, 194, 346, 251), Predictions: {'B0': ('neutral', 0.81015813), 'B4': ('neutral', 0.50446975), 'Ensemble': ('neutral', 0.657)}\n  Face 36 - Box: (901, 153, 958, 233), Predictions: {'B0': ('happy', 0.4409699), 'B4': ('happy', 0.80294144), 'Ensemble': ('happy', 0.622)}\n  Face 37 - Box: (573, 90, 633, 163), Predictions: {'B0': ('happy', 0.5367636), 'B4': ('happy', 0.92233527), 'Ensemble': ('happy', 0.7295)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_groupy (5).jpg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (6).jpeg\nPrediction Results:\n  Face 1 - Box: (246, 299, 314, 390), Predictions: {'B0': ('happy', 0.7912375), 'B4': ('happy', 0.92614335), 'Ensemble': ('happy', 0.8584)}\n  Face 2 - Box: (438, 322, 506, 412), Predictions: {'B0': ('disgust', 0.5282344), 'B4': ('neutral', 0.2247921), 'Ensemble': ('disgust', 0.309)}\n  Face 3 - Box: (431, 794, 492, 876), Predictions: {'B0': ('disgust', 0.33467856), 'B4': ('anger', 0.40069553), 'Ensemble': ('disgust', 0.292)}\n  Face 4 - Box: (62, 795, 123, 876), Predictions: {'B0': ('disgust', 0.3057041), 'B4': ('disgust', 0.23836249), 'Ensemble': ('disgust', 0.272)}\n  Face 5 - Box: (615, 301, 675, 387), Predictions: {'B0': ('happy', 0.55858266), 'B4': ('happy', 0.6322623), 'Ensemble': ('happy', 0.5957)}\n  Face 6 - Box: (604, 50, 657, 136), Predictions: {'B0': ('neutral', 0.25634727), 'B4': ('disgust', 0.81881684), 'Ensemble': ('disgust', 0.522)}\n  Face 7 - Box: (626, 801, 691, 889), Predictions: {'B0': ('contempt', 0.34908676), 'B4': ('contempt', 0.50983244), 'Ensemble': ('contempt', 0.4294)}\n  Face 8 - Box: (259, 549, 320, 642), Predictions: {'B0': ('disgust', 0.35013732), 'B4': ('disgust', 0.6692225), 'Ensemble': ('disgust', 0.51)}\n  Face 9 - Box: (71, 61, 134, 153), Predictions: {'B0': ('contempt', 0.5067946), 'B4': ('happy', 0.47199756), 'Ensemble': ('contempt', 0.3372)}\n  Face 10 - Box: (812, 579, 874, 668), Predictions: {'B0': ('anger', 0.3563094), 'B4': ('disgust', 0.7117546), 'Ensemble': ('disgust', 0.5063)}\n  Face 11 - Box: (430, 54, 508, 159), Predictions: {'B0': ('anger', 0.51447415), 'B4': ('disgust', 0.70847815), 'Ensemble': ('disgust', 0.3801)}\n  Face 12 - Box: (63, 301, 128, 393), Predictions: {'B0': ('fear', 0.40145293), 'B4': ('contempt', 0.20170063), 'Ensemble': ('fear', 0.2479)}\n  Face 13 - Box: (815, 58, 877, 143), Predictions: {'B0': ('contempt', 0.3280451), 'B4': ('disgust', 0.570299), 'Ensemble': ('disgust', 0.3481)}\n  Face 14 - Box: (806, 796, 865, 886), Predictions: {'B0': ('fear', 0.22695959), 'B4': ('disgust', 0.2465426), 'Ensemble': ('disgust', 0.2346)}\n  Face 15 - Box: (438, 555, 503, 652), Predictions: {'B0': ('surprise', 0.2802132), 'B4': ('disgust', 0.4670093), 'Ensemble': ('disgust', 0.3206)}\n  Face 16 - Box: (814, 303, 880, 397), Predictions: {'B0': ('contempt', 0.48432255), 'B4': ('happy', 0.4370989), 'Ensemble': ('contempt', 0.295)}\n  Face 17 - Box: (630, 558, 698, 652), Predictions: {'B0': ('contempt', 0.38438115), 'B4': ('disgust', 0.5267525), 'Ensemble': ('disgust', 0.352)}\n  Face 18 - Box: (240, 47, 314, 145), Predictions: {'B0': ('fear', 0.4661635), 'B4': ('fear', 0.25938755), 'Ensemble': ('fear', 0.3628)}\n  Face 19 - Box: (251, 815, 315, 905), Predictions: {'B0': ('contempt', 0.35177097), 'B4': ('disgust', 0.40549454), 'Ensemble': ('disgust', 0.3018)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (6).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (3).jpeg\nPrediction Results:\n  Face 1 - Box: (84, 548, 170, 656), Predictions: {'B0': ('contempt', 0.298856), 'B4': ('contempt', 0.672999), 'Ensemble': ('contempt', 0.4858)}\n  Face 2 - Box: (833, 33, 925, 158), Predictions: {'B0': ('anger', 0.24434675), 'B4': ('sad', 0.448244), 'Ensemble': ('sad', 0.291)}\n  Face 3 - Box: (88, 46, 178, 169), Predictions: {'B0': ('happy', 0.5512304), 'B4': ('happy', 0.4020746), 'Ensemble': ('happy', 0.4766)}\n  Face 4 - Box: (85, 300, 166, 407), Predictions: {'B0': ('surprise', 0.27806026), 'B4': ('disgust', 0.48923317), 'Ensemble': ('disgust', 0.3545)}\n  Face 5 - Box: (276, 58, 392, 160), Predictions: {'B0': ('neutral', 0.53772724), 'B4': ('neutral', 0.54976887), 'Ensemble': ('neutral', 0.544)}\n  Face 6 - Box: (311, 534, 401, 653), Predictions: {'B0': ('neutral', 0.21769834), 'B4': ('sad', 0.272493), 'Ensemble': ('sad', 0.1738)}\n  Face 7 - Box: (113, 812, 186, 918), Predictions: {'B0': ('surprise', 0.51290715), 'B4': ('happy', 0.26383737), 'Ensemble': ('surprise', 0.3203)}\n  Face 8 - Box: (575, 539, 667, 669), Predictions: {'B0': ('surprise', 0.5012943), 'B4': ('fear', 0.3398349), 'Ensemble': ('surprise', 0.3308)}\n  Face 9 - Box: (327, 295, 410, 422), Predictions: {'B0': ('surprise', 0.7224497), 'B4': ('contempt', 0.27077237), 'Ensemble': ('surprise', 0.4873)}\n  Face 10 - Box: (580, 81, 665, 189), Predictions: {'B0': ('happy', 0.73833835), 'B4': ('fear', 0.25575054), 'Ensemble': ('happy', 0.4294)}\n  Face 11 - Box: (827, 545, 924, 670), Predictions: {'B0': ('contempt', 0.58881587), 'B4': ('surprise', 0.30766395), 'Ensemble': ('contempt', 0.3284)}\n  Face 12 - Box: (588, 295, 671, 412), Predictions: {'B0': ('happy', 0.62196624), 'B4': ('happy', 0.37977675), 'Ensemble': ('happy', 0.501)}\n  Face 13 - Box: (820, 793, 910, 919), Predictions: {'B0': ('disgust', 0.23370521), 'B4': ('contempt', 0.6229488), 'Ensemble': ('contempt', 0.4094)}\n  Face 14 - Box: (569, 801, 658, 926), Predictions: {'B0': ('anger', 0.23348199), 'B4': ('sad', 0.67849857), 'Ensemble': ('sad', 0.3574)}\n  Face 15 - Box: (837, 303, 922, 423), Predictions: {'B0': ('disgust', 0.7778255), 'B4': ('fear', 0.50813633), 'Ensemble': ('disgust', 0.419)}\n  Face 16 - Box: (319, 795, 418, 941), Predictions: {'B0': ('surprise', 0.6469271), 'B4': ('surprise', 0.9850359), 'Ensemble': ('surprise', 0.816)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (3).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (2).jpeg\nPrediction Results:\n  Face 1 - Box: (863, 64, 960, 209), Predictions: {'B0': ('contempt', 0.28097346), 'B4': ('disgust', 0.24776116), 'Ensemble': ('contempt', 0.217)}\n  Face 2 - Box: (241, 68, 345, 198), Predictions: {'B0': ('contempt', 0.6021581), 'B4': ('contempt', 0.84441036), 'Ensemble': ('contempt', 0.723)}\n  Face 3 - Box: (236, 377, 358, 541), Predictions: {'B0': ('contempt', 0.6787056), 'B4': ('contempt', 0.30016267), 'Ensemble': ('contempt', 0.4893)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (2).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (4).jpeg\nPrediction Results:\n  Face 1 - Box: (474, 415, 541, 505), Predictions: {'B0': ('happy', 0.4473993), 'B4': ('disgust', 0.6596467), 'Ensemble': ('disgust', 0.3833)}\n  Face 2 - Box: (473, 156, 539, 238), Predictions: {'B0': ('happy', 0.45964906), 'B4': ('disgust', 0.2670072), 'Ensemble': ('happy', 0.3604)}\n  Face 3 - Box: (285, 536, 353, 633), Predictions: {'B0': ('disgust', 0.31857315), 'B4': ('surprise', 0.4656003), 'Ensemble': ('surprise', 0.3718)}\n  Face 4 - Box: (670, 150, 734, 242), Predictions: {'B0': ('fear', 0.28618118), 'B4': ('sad', 0.5033015), 'Ensemble': ('sad', 0.3027)}\n  Face 5 - Box: (85, 276, 149, 365), Predictions: {'B0': ('surprise', 0.8493715), 'B4': ('surprise', 0.32939166), 'Ensemble': ('surprise', 0.5894)}\n  Face 6 - Box: (269, 276, 336, 372), Predictions: {'B0': ('surprise', 0.5325522), 'B4': ('fear', 0.3924238), 'Ensemble': ('surprise', 0.3616)}\n  Face 7 - Box: (73, 548, 147, 652), Predictions: {'B0': ('contempt', 0.23518263), 'B4': ('sad', 0.4862929), 'Ensemble': ('sad', 0.3489)}\n  Face 8 - Box: (469, 547, 529, 629), Predictions: {'B0': ('happy', 0.61450183), 'B4': ('sad', 0.43059322), 'Ensemble': ('happy', 0.3342)}\n  Face 9 - Box: (63, 138, 123, 222), Predictions: {'B0': ('anger', 0.46377704), 'B4': ('happy', 0.4868623), 'Ensemble': ('happy', 0.3293)}\n  Face 10 - Box: (669, 412, 732, 503), Predictions: {'B0': ('anger', 0.6884065), 'B4': ('disgust', 0.60988283), 'Ensemble': ('anger', 0.4924)}\n  Face 11 - Box: (657, 273, 735, 380), Predictions: {'B0': ('anger', 0.31389436), 'B4': ('sad', 0.25047633), 'Ensemble': ('anger', 0.2063)}\n  Face 12 - Box: (53, 402, 118, 491), Predictions: {'B0': ('anger', 0.7922831), 'B4': ('contempt', 0.7550892), 'Ensemble': ('anger', 0.4294)}\n  Face 13 - Box: (469, 271, 539, 371), Predictions: {'B0': ('contempt', 0.8697336), 'B4': ('contempt', 0.31858733), 'Ensemble': ('contempt', 0.594)}\n  Face 14 - Box: (59, 8, 136, 117), Predictions: {'B0': ('contempt', 0.696805), 'B4': ('anger', 0.47002465), 'Ensemble': ('contempt', 0.5234)}\n  Face 15 - Box: (260, 6, 329, 101), Predictions: {'B0': ('anger', 0.38501558), 'B4': ('anger', 0.3780474), 'Ensemble': ('anger', 0.3813)}\n  Face 16 - Box: (882, 408, 944, 502), Predictions: {'B0': ('fear', 0.4314672), 'B4': ('anger', 0.27747014), 'Ensemble': ('fear', 0.3281)}\n  Face 17 - Box: (874, 274, 940, 370), Predictions: {'B0': ('surprise', 0.30873287), 'B4': ('fear', 0.37906995), 'Ensemble': ('fear', 0.2493)}\n  Face 18 - Box: (887, 147, 949, 238), Predictions: {'B0': ('surprise', 0.6161009), 'B4': ('surprise', 0.69778055), 'Ensemble': ('surprise', 0.657)}\n  Face 19 - Box: (879, 550, 924, 622), Predictions: {'B0': ('fear', 0.5413299), 'B4': ('fear', 0.57399774), 'Ensemble': ('fear', 0.5576)}\n  Face 20 - Box: (674, 20, 737, 101), Predictions: {'B0': ('disgust', 0.29328403), 'B4': ('disgust', 0.33197692), 'Ensemble': ('disgust', 0.3125)}\n  Face 21 - Box: (265, 411, 331, 500), Predictions: {'B0': ('anger', 0.8730384), 'B4': ('anger', 0.8170218), 'Ensemble': ('anger', 0.8447)}\n  Face 22 - Box: (863, 0, 937, 103), Predictions: {'B0': ('anger', 0.7105454), 'B4': ('anger', 0.86297923), 'Ensemble': ('anger', 0.7866)}\n  Face 23 - Box: (481, 23, 538, 104), Predictions: {'B0': ('sad', 0.56146455), 'B4': ('disgust', 0.5871645), 'Ensemble': ('disgust', 0.3906)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (4).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (1).jpeg\nPrediction Results:\n  Face 1 - Box: (297, 69, 350, 145), Predictions: {'B0': ('fear', 0.46535146), 'B4': ('surprise', 0.5482722), 'Ensemble': ('surprise', 0.4824)}\n  Face 2 - Box: (108, 69, 160, 141), Predictions: {'B0': ('surprise', 0.8271986), 'B4': ('surprise', 0.8269451), 'Ensemble': ('surprise', 0.827)}\n  Face 3 - Box: (494, 72, 547, 147), Predictions: {'B0': ('surprise', 0.8061897), 'B4': ('surprise', 0.8371549), 'Ensemble': ('surprise', 0.8213)}\n  Face 4 - Box: (686, 70, 737, 139), Predictions: {'B0': ('contempt', 0.34547094), 'B4': ('happy', 0.30974618), 'Ensemble': ('surprise', 0.2964)}\n  Face 5 - Box: (867, 71, 918, 142), Predictions: {'B0': ('surprise', 0.67288905), 'B4': ('fear', 0.5368694), 'Ensemble': ('surprise', 0.4595)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (1).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (7).jpeg\nPrediction Results:\n  Face 1 - Box: (617, 315, 667, 371), Predictions: {'B0': ('neutral', 0.33909515), 'B4': ('sad', 0.39068726), 'Ensemble': ('sad', 0.2336)}\n  Face 2 - Box: (764, 32, 812, 98), Predictions: {'B0': ('surprise', 0.59088767), 'B4': ('fear', 0.8352103), 'Ensemble': ('fear', 0.451)}\n  Face 3 - Box: (45, 171, 91, 237), Predictions: {'B0': ('neutral', 0.2985387), 'B4': ('disgust', 0.5686159), 'Ensemble': ('disgust', 0.3408)}\n  Face 4 - Box: (484, 176, 529, 242), Predictions: {'B0': ('surprise', 0.67563856), 'B4': ('surprise', 0.8672126), 'Ensemble': ('surprise', 0.7715)}\n  Face 5 - Box: (187, 328, 235, 395), Predictions: {'B0': ('sad', 0.29706734), 'B4': ('fear', 0.40844086), 'Ensemble': ('fear', 0.2532)}\n  Face 6 - Box: (476, 317, 524, 386), Predictions: {'B0': ('surprise', 0.53480506), 'B4': ('surprise', 0.41901034), 'Ensemble': ('surprise', 0.4768)}\n  Face 7 - Box: (338, 189, 388, 258), Predictions: {'B0': ('contempt', 0.49225724), 'B4': ('contempt', 0.20127134), 'Ensemble': ('contempt', 0.3467)}\n  Face 8 - Box: (51, 322, 92, 380), Predictions: {'B0': ('anger', 0.46773937), 'B4': ('surprise', 0.6698407), 'Ensemble': ('surprise', 0.3403)}\n  Face 9 - Box: (902, 27, 957, 111), Predictions: {'B0': ('surprise', 0.63566536), 'B4': ('surprise', 0.81324357), 'Ensemble': ('surprise', 0.7246)}\n  Face 10 - Box: (621, 180, 664, 239), Predictions: {'B0': ('anger', 0.2708956), 'B4': ('happy', 0.27174845), 'Ensemble': ('contempt', 0.249)}\n  Face 11 - Box: (334, 32, 382, 101), Predictions: {'B0': ('neutral', 0.38788316), 'B4': ('contempt', 0.3457542), 'Ensemble': ('neutral', 0.2598)}\n  Face 12 - Box: (618, 42, 667, 115), Predictions: {'B0': ('surprise', 0.4806149), 'B4': ('fear', 0.5582597), 'Ensemble': ('fear', 0.4849)}\n  Face 13 - Box: (913, 316, 957, 380), Predictions: {'B0': ('contempt', 0.2016713), 'B4': ('surprise', 0.38296634), 'Ensemble': ('surprise', 0.2761)}\n  Face 14 - Box: (764, 184, 814, 256), Predictions: {'B0': ('surprise', 0.63942677), 'B4': ('fear', 0.3055369), 'Ensemble': ('surprise', 0.4255)}\n  Face 15 - Box: (190, 34, 235, 103), Predictions: {'B0': ('fear', 0.33174413), 'B4': ('fear', 0.6200893), 'Ensemble': ('fear', 0.476)}\n  Face 16 - Box: (476, 34, 528, 115), Predictions: {'B0': ('surprise', 0.77350354), 'B4': ('anger', 0.32854995), 'Ensemble': ('surprise', 0.451)}\n  Face 17 - Box: (47, 35, 100, 117), Predictions: {'B0': ('surprise', 0.79382175), 'B4': ('fear', 0.47606423), 'Ensemble': ('surprise', 0.5845)}\n  Face 18 - Box: (908, 176, 957, 249), Predictions: {'B0': ('fear', 0.66310793), 'B4': ('fear', 0.4509499), 'Ensemble': ('fear', 0.557)}\n  Face 19 - Box: (757, 311, 830, 414), Predictions: {'B0': ('surprise', 0.6434712), 'B4': ('surprise', 0.51983154), 'Ensemble': ('surprise', 0.582)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (7).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (9).jpeg\nPrediction Results:\n  Face 1 - Box: (62, 259, 131, 354), Predictions: {'B0': ('surprise', 0.78323245), 'B4': ('fear', 0.5536378), 'Ensemble': ('surprise', 0.4844)}\n  Face 2 - Box: (254, 44, 321, 130), Predictions: {'B0': ('contempt', 0.5680708), 'B4': ('contempt', 0.7145788), 'Ensemble': ('contempt', 0.641)}\n  Face 3 - Box: (457, 64, 529, 156), Predictions: {'B0': ('neutral', 0.57144463), 'B4': ('neutral', 0.59445083), 'Ensemble': ('neutral', 0.583)}\n  Face 4 - Box: (867, 49, 943, 157), Predictions: {'B0': ('happy', 0.7073462), 'B4': ('happy', 0.68937767), 'Ensemble': ('happy', 0.698)}\n  Face 5 - Box: (856, 274, 922, 361), Predictions: {'B0': ('anger', 0.3316917), 'B4': ('contempt', 0.33034253), 'Ensemble': ('anger', 0.2795)}\n  Face 6 - Box: (657, 260, 727, 352), Predictions: {'B0': ('surprise', 0.61180526), 'B4': ('happy', 0.2999087), 'Ensemble': ('surprise', 0.3308)}\n  Face 7 - Box: (657, 48, 727, 147), Predictions: {'B0': ('fear', 0.37717596), 'B4': ('sad', 0.23863208), 'Ensemble': ('fear', 0.2184)}\n  Face 8 - Box: (60, 46, 137, 157), Predictions: {'B0': ('neutral', 0.3578786), 'B4': ('contempt', 0.60018444), 'Ensemble': ('contempt', 0.4055)}\n  Face 9 - Box: (462, 254, 529, 357), Predictions: {'B0': ('surprise', 0.82360077), 'B4': ('contempt', 0.25573096), 'Ensemble': ('surprise', 0.4917)}\n  Face 10 - Box: (256, 255, 322, 352), Predictions: {'B0': ('surprise', 0.935424), 'B4': ('fear', 0.70792127), 'Ensemble': ('surprise', 0.4834)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (9).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (8).jpeg\nPrediction Results:\n  Face 1 - Box: (609, 49, 672, 134), Predictions: {'B0': ('neutral', 0.6617568), 'B4': ('anger', 0.482292), 'Ensemble': ('neutral', 0.359)}\n  Face 2 - Box: (860, 41, 923, 132), Predictions: {'B0': ('happy', 0.63100386), 'B4': ('happy', 0.37413222), 'Ensemble': ('happy', 0.5024)}\n  Face 3 - Box: (844, 650, 913, 746), Predictions: {'B0': ('happy', 0.308427), 'B4': ('happy', 0.5736157), 'Ensemble': ('happy', 0.441)}\n  Face 4 - Box: (344, 17, 409, 112), Predictions: {'B0': ('surprise', 0.608749), 'B4': ('surprise', 0.32012394), 'Ensemble': ('surprise', 0.4644)}\n  Face 5 - Box: (84, 637, 153, 736), Predictions: {'B0': ('surprise', 0.52662635), 'B4': ('disgust', 0.2597331), 'Ensemble': ('surprise', 0.335)}\n  Face 6 - Box: (589, 346, 672, 457), Predictions: {'B0': ('surprise', 0.65261495), 'B4': ('surprise', 0.52174824), 'Ensemble': ('surprise', 0.5874)}\n  Face 7 - Box: (337, 334, 396, 405), Predictions: {'B0': ('disgust', 0.61877495), 'B4': ('disgust', 0.6841713), 'Ensemble': ('disgust', 0.6514)}\n  Face 8 - Box: (838, 358, 917, 471), Predictions: {'B0': ('surprise', 0.3504294), 'B4': ('contempt', 0.49976552), 'Ensemble': ('contempt', 0.3289)}\n  Face 9 - Box: (101, 344, 172, 445), Predictions: {'B0': ('surprise', 0.45492056), 'B4': ('contempt', 0.32541227), 'Ensemble': ('surprise', 0.3237)}\n  Face 10 - Box: (330, 680, 386, 758), Predictions: {'B0': ('disgust', 0.21243565), 'B4': ('disgust', 0.28877127), 'Ensemble': ('disgust', 0.2505)}\n  Face 11 - Box: (595, 655, 661, 757), Predictions: {'B0': ('anger', 0.5998413), 'B4': ('surprise', 0.6749132), 'Ensemble': ('surprise', 0.3633)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (8).jpeg\n\nProcessing image: /kaggle/input/image-samples/image_samples/faces (5).jpeg\nPrediction Results:\n  Face 1 - Box: (786, 373, 868, 486), Predictions: {'B0': ('surprise', 0.88442355), 'B4': ('fear', 0.62100124), 'Ensemble': ('surprise', 0.516)}\n  Face 2 - Box: (115, 85, 231, 234), Predictions: {'B0': ('sad', 0.577905), 'B4': ('neutral', 0.67708075), 'Ensemble': ('neutral', 0.4082)}\n  Face 3 - Box: (457, 398, 552, 518), Predictions: {'B0': ('surprise', 0.52568376), 'B4': ('surprise', 0.34760627), 'Ensemble': ('surprise', 0.4368)}\n  Face 4 - Box: (126, 379, 212, 517), Predictions: {'B0': ('surprise', 0.67368215), 'B4': ('fear', 0.4463942), 'Ensemble': ('surprise', 0.4556)}\n  Face 5 - Box: (479, 54, 556, 167), Predictions: {'B0': ('anger', 0.5874593), 'B4': ('anger', 0.70955), 'Ensemble': ('anger', 0.6484)}\n  Face 6 - Box: (780, 63, 872, 188), Predictions: {'B0': ('anger', 0.5136024), 'B4': ('anger', 0.8269213), 'Ensemble': ('anger', 0.6704)}\nAnnotated image saved to /kaggle/working/emotion_output_v2/annotated_faces (5).jpeg\nNo video files found in the directory.\n\nAll processing complete.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.3 (AdamW, Label Smoothing, Ensemble Weight Optimization)\n\n#=== FINAL RESULTS === 112\n#(Using Optimized Ensemble Weights: [0.4, 0.6])\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.7484, F1: 0.7463\n#EfficientNetB0 FER2013 Test Accuracy: 0.5571, F1: 0.5547\n#EfficientNetB4 AffectNet Test Accuracy: 0.8341, F1: 0.8344\n#EfficientNetB4 FER2013 Test Accuracy: 0.5599, F1: 0.5599\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.8508\n#Ensemble AffectNet F1 Score: 0.8511\n#Ensemble FER2013 Test Accuracy: 0.5915\n#Ensemble FER2013 F1 Score: 0.5906\n\n#=== FINAL RESULTS === 160\n#(Using Optimized Ensemble Weights: [0.35000000000000003, 0.6499999999999999])\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.7124, F1: 0.7089\n#EfficientNetB0 FER2013 Test Accuracy: 0.5722, F1: 0.5702\n#EfficientNetB4 AffectNet Test Accuracy: 0.7912, F1: 0.7912\n#EfficientNetB4 FER2013 Test Accuracy: 0.5400, F1: 0.5400\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.8090\n#Ensemble AffectNet F1 Score: 0.8083\n#Ensemble FER2013 Test Accuracy: 0.5784\n#Ensemble FER2013 F1 Score: 0.5761\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score # Added accuracy_score\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\nimport math\nimport glob # Needed for finding files\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path [cite: 1]\n    \"TRAIN_DIR\": \"Train\", # Combined train dir name [cite: 1]\n    \"TEST_DIR\": \"Test\",   # Base test dir name [cite: 2]\n    \"IMG_SIZE\": 160, # 224= accuracy 0.49 / 380=x / 336=x (160px, 192px, or 256px)\n    \"BATCH_SIZE\": 32, # Keep user's tuned value [cite: 2]\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE, # [cite: 2]\n    \"EPOCHS_HEAD\": 50, # Keep user's tuned value [cite: 2]\n    \"EPOCHS_FINE_TUNE\": 100, # Keep user's tuned value [cite: 2]\n    \"LR_HEAD\": 1e-3, # [cite: 2]\n    \"LR_FINE_TUNE_START\": 1e-4, # [cite: 2]\n    \"DROPOUT_RATE\": 0.4, # [cite: 2]\n    \"NUM_CLASSES\": 8, # [cite: 2]\n    \"MODEL_ARCH_1\": \"EfficientNetB0\", # [cite: 2]\n    \"MODEL_ARCH_2\": \"EfficientNetB4\", # [cite: 2]\n    \"ENSEMBLE_WEIGHTS\": [0.5, 0.5], # Initial value, will be optimized later [cite: 2]\n    \"FINE_TUNE_LAYERS_B0\": 15, # Keep user's tuned value [cite: 3]\n    \"FINE_TUNE_LAYERS_B4\": 20, # Keep user's tuned value [cite: 3]\n    \"LOG_DIR_BASE\": \"logs/fit/\", # [cite: 3]\n\n    # --- NEW: Added Parameters ---\n    \"WEIGHT_DECAY\": 1e-5, # Recommended value for AdamW, tune if needed\n    \"LABEL_SMOOTHING\": 0.1, # Common value for label smoothing\n}\n\n# Set Seed\ntf.random.set_seed(CONFIG[\"SEED\"]) # [cite: 3]\nnp.random.seed(CONFIG[\"SEED\"]) # [cite: 3]\n\n# =============================================================================\n# GPU Configuration & Mixed Precision (Same as before)\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU') # [cite: 3]\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True) # [cite: 3]\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\") # [cite: 3]\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\") # [cite: 3]\nelse:\n    print(\"No GPU detected. Running on CPU.\") # [cite: 3]\n\npolicy = tf.keras.mixed_precision.Policy('mixed_float16') # [cite: 4]\ntf.keras.mixed_precision.set_global_policy(policy) # [cite: 4]\nprint(\"Mixed precision enabled ('mixed_float16')\") # [cite: 4]\n\n# =============================================================================\n# Data Loading (Train/Validation - Standard; Test - Custom)\n# =============================================================================\n\ndef create_train_val_tf_dataset(directory, image_size, batch_size, validation_split=0.2): # [cite: 4]\n    \"\"\"Creates tf.data.Dataset for training and validation using image_dataset_from_directory.\"\"\"\n    print(f\"Loading training/validation data from: {directory}\") # [cite: 4]\n\n    # Create the training dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory( # [cite: 4]\n        directory,\n        labels='inferred', # [cite: 4]\n        label_mode='categorical', # [cite: 4]\n        image_size=(image_size, image_size), # [cite: 4]\n        interpolation='nearest', # [cite: 4]\n        batch_size=batch_size, # [cite: 5]\n        shuffle=True, # [cite: 5]\n        seed=CONFIG[\"SEED\"], # [cite: 5]\n        validation_split=validation_split, # [cite: 5]\n        subset=\"training\", # [cite: 5]\n    )\n\n    # Create the validation dataset\n    val_ds = tf.keras.utils.image_dataset_from_directory( # [cite: 5]\n        directory,\n        labels='inferred', # [cite: 5]\n        label_mode='categorical', # [cite: 5]\n        image_size=(image_size, image_size), # [cite: 5]\n        interpolation='nearest', # [cite: 5]\n        batch_size=batch_size, # [cite: 6]\n        shuffle=False, # No need to shuffle validation [cite: 6]\n        seed=CONFIG[\"SEED\"], # [cite: 6]\n        validation_split=validation_split, # [cite: 6]\n        subset=\"validation\", # [cite: 6]\n    )\n\n    # Get class names from the training dataset BEFORE optimization\n    class_names = train_ds.class_names # [cite: 6]\n    print(f\"Dataset loaded with classes: {class_names}\") # [cite: 6]\n\n    # Configure performance for both datasets\n    train_ds = train_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 6]\n    val_ds = val_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 6]\n\n    return train_ds, val_ds, class_names # [cite: 6]\n\n# --- NEW: Function to load test data from the specific structure ---\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size): # [cite: 7]\n    \"\"\"\n    Creates a tf.data.Dataset for testing by manually finding files in the Test/<emotion>/<target_dataset> structure. [cite: 7]\n    \"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\") # [cite: 8]\n    all_image_paths = [] # [cite: 8]\n    all_labels = [] # [cite: 8]\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) if tf.io.gfile.isdir(os.path.join(base_test_dir, d))] # [cite: 8]\n    if not emotion_dirs: # [cite: 8]\n         print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\") # [cite: 8]\n         return None # [cite: 8]\n\n    print(f\"Found emotion folders: {emotion_dirs}\") # [cite: 8]\n\n    for emotion in emotion_dirs: # [cite: 8]\n         if emotion not in class_names_map: # [cite: 9]\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\") # [cite: 9]\n            continue # [cite: 9]\n\n         label_index = class_names_map[emotion] # Get the integer label [cite: 9]\n         target_path = os.path.join(base_test_dir, emotion, target_dataset) # [cite: 9]\n\n         if not tf.io.gfile.exists(target_path): # [cite: 9]\n             print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\") # [cite: 9]\n             continue # Skip if the specific dataset subdir doesn't exist for this emotion [cite: 10]\n\n        # Find all image files (adjust extensions if needed)\n         image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpeg')) # [cite: 10]\n\n         if not image_files: # [cite: 11]\n             print(f\"Warning: No image files found in '{target_path}'.\") # [cite: 11]\n             continue # [cite: 11]\n\n         all_image_paths.extend(image_files) # [cite: 11]\n         all_labels.extend([label_index] * len(image_files)) # [cite: 11]\n         print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\") # [cite: 11]\n\n    if not all_image_paths: # [cite: 11]\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\") # [cite: 12]\n        return None # [cite: 12]\n\n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\") # [cite: 12]\n\n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths) # [cite: 12]\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=CONFIG[\"NUM_CLASSES\"])) # Convert labels to one-hot [cite: 12]\n    image_ds = path_ds.map(lambda x: load_and_preprocess_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE) # [cite: 12]\n\n    # Combine images and labels\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds)) # [cite: 12]\n\n    # Batch and prefetch\n    test_ds = image_label_ds.batch(batch_size).prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 12]\n\n    return test_ds # [cite: 12]\n\n# --- NEW: Helper function to load and preprocess images from paths ---\ndef load_and_preprocess_image(path, image_size): # [cite: 13]\n    \"\"\"Loads and preprocesses a single image file.\"\"\"\n    image = tf.io.read_file(path) # [cite: 13]\n    image = tf.image.decode_image(image, channels=3, expand_animations=False) # Decode any format [cite: 13]\n    image = tf.image.resize(image, [image_size, image_size], method='nearest') # [cite: 13]\n    image.set_shape((image_size, image_size, 3)) # [cite: 13]\n    # Rescaling: EfficientNet generally handles this internally, but if needed:\n    # image = image / 255.0\n    return image # [cite: 13]\n\n# --- Create Datasets ---\n# Training and Validation Data\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"]) # [cite: 13]\ntrain_ds, val_ds, class_names = create_train_val_tf_dataset( # [cite: 13]\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n)\n\n# Create a mapping from class name to integer index for test set loading\nclass_names_map = {name: i for i, name in enumerate(class_names)} # [cite: 14]\n\n# Test Datasets (using the new custom function)\nbase_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"]) # [cite: 14]\naffectnet_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]) # [cite: 14]\nfer2013_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]) # [cite: 14]\n\n\n# =============================================================================\n# Class Weights Calculation (Same as V3.2)\n# =============================================================================\ndef get_class_weights(dataset, class_names_list): # [cite: 14]\n    print(\"Calculating class weights...\") # [cite: 14]\n    all_labels = [] # [cite: 14]\n    num_batches = tf.data.experimental.cardinality(dataset) # [cite: 14]\n    print(f\"Approximate number of batches in training dataset: {num_batches}\") # [cite: 14]\n\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY: # [cite: 14]\n         print(\"Warning: Cannot determine dataset cardinality accurately. Iterating...\") # [cite: 16]\n         for _, labels_batch in dataset: # Iterate batches # [cite: 16]\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 16]\n         if not all_labels: # [cite: 16]\n             print(\"Error: Could not extract labels. Using uniform weights.\") # [cite: 16]\n             return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 16]\n    else:\n        for _, labels_batch in dataset: # [cite: 16]\n             all_labels.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 17]\n\n    unique_classes, counts = np.unique(all_labels, return_counts=True) # [cite: 17]\n    print(f\"Unique labels found for weight calculation: {unique_classes} with counts {counts}\") # [cite: 17]\n\n    if len(unique_classes) == 0: # [cite: 17]\n        print(\"Error: No labels found. Using uniform weights.\") # [cite: 17]\n        return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 17]\n\n    class_weights = compute_class_weight( # [cite: 17]\n        class_weight='balanced', # [cite: 17]\n        classes=unique_classes, # [cite: 17]\n        y=all_labels # [cite: 17]\n    )\n\n    class_weights_dict = {i: 0.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 18]\n    for i, cls_label in enumerate(unique_classes): # [cite: 18]\n        if cls_label < CONFIG[\"NUM_CLASSES\"]: # [cite: 18]\n             class_weights_dict[cls_label] = class_weights[i] # [cite: 18]\n        else:\n             print(f\"Warning: Label {cls_label} >= NUM_CLASSES ({CONFIG['NUM_CLASSES']}). Ignoring.\") # [cite: 19]\n\n    for i in range(CONFIG[\"NUM_CLASSES\"]): # [cite: 19]\n        if class_weights_dict[i] == 0.0 and i in unique_classes: # Check if it was actually missing or just had 0 weight [cite: 19]\n            print(f\"Warning: Class {i} ({class_names_list[i]}) had 0 weight initially. Assigning 1.0.\") # [cite: 19]\n            class_weights_dict[i] = 1.0 # Avoid 0 weight if class exists but wasn't found / calculated [cite: 19]\n        elif class_weights_dict[i] == 0.0: # [cite: 19]\n             print(f\"Info: Class {i} ({class_names_list[i]}) not found in iterated samples. Assigning weight 1.0.\") # [cite: 20]\n             class_weights_dict[i] = 1.0 # [cite: 20]\n\n\n    print(\"Class weights calculated:\", class_weights_dict) # [cite: 20]\n    return class_weights_dict # [cite: 20]\n\nclass_weights = get_class_weights(train_ds, class_names) # [cite: 20]\n\n# =============================================================================\n# Data Augmentation Layer (Same as before)\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([ # [cite: 20]\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]), # [cite: 20]\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]), # [cite: 20]\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]), # [cite: 20]\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building (Same as V3.2)\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate): # [cite: 20]\n    input_shape = (img_size, img_size, 3) # [cite: 20]\n    inputs = Input(shape=input_shape, name=\"input_layer\") # [cite: 21]\n    x = data_augmentation(inputs) # Apply augmentation first [cite: 21]\n\n    if model_arch == \"EfficientNetB0\": # [cite: 21]\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # Let EffNet handle input scaling [cite: 21]\n        x_processed = base_model(x, training=False) # Pass augmented data to base model [cite: 22]\n    elif model_arch == \"EfficientNetB4\": # [cite: 22]\n         print(f\"Warning: Building {model_arch} with image size {img_size}.\") # [cite: 22]\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # [cite: 22]\n         x_processed = base_model(x, training=False) # Pass augmented data to base model [cite: 22]\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\") # [cite: 22]\n\n    base_model.trainable = False # Freeze base model [cite: 22]\n\n    # Add classification head\n    output = Dropout(dropout_rate, name=\"top_dropout\")(x_processed) # Use output from base model [cite: 23]\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(output) # [cite: 23]\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch) # [cite: 3]\n    print(f\"{model_arch} model built successfully.\") # [cite: 23]\n    return model # [cite: 23]\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"]) # [cite: 23]\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"]) # [cite: 23]\n\n# =============================================================================\n# Training Functions (Using AdamW and Label Smoothing Loss)\n# =============================================================================\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0): # [cite: 23]\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name # [cite: 23]\n    checkpoint_path = f\"best_{model.name}.keras\" # [cite: 24]\n\n    # --- Callbacks ---\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=1) # [cite: 24]\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=9, min_lr=1e-6, verbose=1) # Keep user's patience [cite: 24]\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=18, restore_best_weights=True, verbose=1) # Keep user's patience [cite: 24]\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) # [cite: 24]\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback] # [cite: 24]\n    # --- CHANGE: Use AdamW ---\n    optimizer_choice = tf.keras.optimizers.AdamW # [cite: 24]\n\n    # Find the base model layer to control its trainability\n    base_model_layer = None # [cite: 24]\n    for layer in model.layers: # [cite: 24]\n         if layer.name.startswith(\"efficientnet\"): # Find the actual base model layer by name [cite: 25]\n            base_model_layer = layer # [cite: 25]\n            break # [cite: 25]\n    if base_model_layer is None and fine_tune: # [cite: 25]\n        print(\"Warning: Could not automatically find base model layer for fine-tuning.\") # [cite: 25]\n\n    # --- Compile Step ---\n    if fine_tune: # [cite: 25]\n        print(f\"Setting up fine-tuning for {model.name}...\") # [cite: 25]\n        if base_model_layer: # [cite: 25]\n             base_model_layer.trainable = True # Unfreeze the base model layer [cite: 26]\n             # Fine-tune only the top 'fine_tune_layers' layers within the base model\n             num_base_layers = len(base_model_layer.layers) # [cite: 26]\n             freeze_until = num_base_layers - fine_tune_layers # [cite: 26]\n             print(f\"Unfreezing top {fine_tune_layers} layers of {base_model_layer.name} (out of {num_base_layers}). Freezing up to layer {freeze_until}.\") # [cite: 27]\n             if freeze_until < 0: freeze_until = 0 # [cite: 27]\n\n             for layer in base_model_layer.layers[:freeze_until]: # [cite: 27]\n                 # Keep Batch Norm layers frozen\n                 if isinstance(layer, tf.keras.layers.BatchNormalization): # [cite: 28]\n                      layer.trainable = False # [cite: 28]\n                 else:\n                      layer.trainable = False # [cite: 29]\n\n             for layer in base_model_layer.layers[freeze_until:]: # [cite: 29]\n                 # Keep Batch Norm frozen\n                 if isinstance(layer, tf.keras.layers.BatchNormalization): # [cite: 30]\n                     layer.trainable = False # [cite: 30]\n                 else:\n                     layer.trainable = True # Unfreeze the top layers [cite: 30]\n        else: # Fallback if base model layer not found # [cite: 30]\n             model.trainable = True # Unfreeze everything if base layer not identified [cite: 31]\n\n        learning_rate_schedule = learning_rate # Use starting LR for fine-tune [cite: 31]\n        callbacks.append(reduce_lr) # [cite: 31]\n        # --- CHANGE: AdamW with weight decay ---\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule, weight_decay=CONFIG[\"WEIGHT_DECAY\"]) # [cite: 31]\n    else: # Head training # [cite: 31]\n        print(f\"Setting up head training for {model.name}.\") # [cite: 31]\n        if base_model_layer: # [cite: 31]\n            base_model_layer.trainable = False # Ensure base is frozen # [cite: 31]\n        else:\n             print(\"Warning: Could not find base model layer to freeze for head training.\") # [cite: 32]\n\n        # Cosine Decay for head training\n        train_cardinality = tf.data.experimental.cardinality(train_dataset) # [cite: 32]\n        if train_cardinality == tf.data.experimental.UNKNOWN_CARDINALITY: # [cite: 32]\n            print(\"Warning: Unknown training steps for CosineDecay. Estimating.\") # [cite: 33]\n            try: # Estimate steps # [cite: 33]\n                 # This estimate might be inaccurate if dataset not fully listed/cached\n                 steps_per_epoch = math.ceil(num_batches / CONFIG[\"BATCH_SIZE\"]) # Use num_batches if known\n            except: steps_per_epoch = 1000 # Fallback # [cite: 33]\n            total_steps = steps_per_epoch * epochs # [cite: 33]\n        else:\n            total_steps = train_cardinality.numpy() * epochs # [cite: 33]\n\n        warmup_steps = int(total_steps * 0.1) # [cite: 34]\n        learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay( # [cite: 34]\n             initial_learning_rate=learning_rate, decay_steps=max(1, total_steps - warmup_steps), alpha=0.0 # [cite: 34]\n        )\n        # --- CHANGE: AdamW with weight decay ---\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule, weight_decay=CONFIG[\"WEIGHT_DECAY\"]) # [cite: 34]\n\n    # --- CHANGE: Use Label Smoothing in Loss ---\n    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG[\"LABEL_SMOOTHING\"]) # [cite: 34]\n\n    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy']) # [cite: 34]\n    print(\"\\n--- Model Summary After Compilation ---\") # [cite: 34]\n    model.summary(line_length=120) # Print summary after compilation and setting trainability # [cite: 34]\n\n    print(f\"\\n--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} (Epochs {initial_epoch} to {initial_epoch+epochs}) ---\") # [cite: 35]\n    history = model.fit( # [cite: 35]\n        train_dataset,\n        epochs=initial_epoch + epochs, # End epoch # [cite: 35]\n        validation_data=validation_dataset, # [cite: 35]\n        class_weight=class_weights_dict, # [cite: 35]\n        callbacks=callbacks, # [cite: 35]\n        initial_epoch=initial_epoch, # Start epoch # [cite: 35]\n        verbose=1 # [cite: 35]\n    )\n    return history, model # [cite: 35]\n\n\n# =============================================================================\n# Evaluation Function (Same as V3.2)\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names_list, dataset_name): # [cite: 36]\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    if test_dataset is None: # [cite: 36]\n        print(f\"Skipping evaluation on {dataset_name}: Dataset not loaded.\") # [cite: 36]\n        return None # [cite: 36]\n\n    print(f\"\\n--- Evaluating {model.name} on {dataset_name} ---\") # [cite: 36]\n    results = model.evaluate(test_dataset, verbose=1) # [cite: 36]\n    print(f\"{model.name} {dataset_name} Test Loss: {results[0]:.4f}\") # [cite: 36]\n    print(f\"{model.name} {dataset_name} Test Accuracy: {results[1]:.4f}\") # [cite: 36]\n\n    y_pred_probs = model.predict(test_dataset) # [cite: 36]\n    y_pred = np.argmax(y_pred_probs, axis=1) # [cite: 36]\n\n    y_true = [] # [cite: 36]\n    for _, labels_batch in test_dataset: # [cite: 37]\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 37]\n    y_true = np.array(y_true) # [cite: 37]\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int) # [cite: 37]\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)] # [cite: 37]\n\n    if not present_class_names: # [cite: 37]\n         print(\"Warning: No predictable classes found in evaluation data.\") # [cite: 37]\n         return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": 0} # [cite: 37]\n\n\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0) # [cite: 37]\n    print(f\"{model.name} {dataset_name} F1 Score (Weighted): {f1:.4f}\") # [cite: 37]\n    print(f\"{model.name} {dataset_name} Classification Report:\") # [cite: 38]\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0)) # [cite: 38]\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data) # [cite: 38]\n    plt.figure(figsize=(10, 8)) # [cite: 38]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_class_names, yticklabels=present_class_names) # [cite: 38]\n    plt.title(f'{model.name} {dataset_name} Confusion Matrix') # [cite: 38]\n    plt.ylabel('Actual') # [cite: 38]\n    plt.xlabel('Predicted') # [cite: 38]\n    cm_filename = f\"{model.name}_{dataset_name}_confusion_matrix.png\" # [cite: 38]\n    plt.savefig(cm_filename) # [cite: 38]\n    print(f\"Saved confusion matrix to {cm_filename}\") # [cite: 38]\n    plt.close() # [cite: 38]\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1} # [cite: 38]\n\n# =============================================================================\n# Main Training Pipeline\n# =============================================================================\n\nprint(f\"\\nClass names being used: {class_names}\") # [cite: 38]\nprint(f\"Number of classes for models: {CONFIG['NUM_CLASSES']}\") # [cite: 39]\nif len(class_names) != CONFIG['NUM_CLASSES']: # [cite: 39]\n     print(f\"Warning: Number of classes found ({len(class_names)}) differs from CONFIG['NUM_CLASSES'] ({CONFIG['NUM_CLASSES']})\") # [cite: 39]\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\") # [cite: 39]\nhistory1_head, model1 = train_model( # [cite: 39]\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0 # [cite: 39]\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\") # [cite: 39]\nhistory2_head, model2 = train_model( # [cite: 39]\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0 # [cite: 39]\n)\n\n# Load best weights from head training before fine-tuning\nprint(\"\\n--- Loading best weights after head training ---\") # [cite: 39]\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best head weights for {model1.name}\") # [cite: 40]\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model1.name} - {e}.\") # [cite: 40]\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best head weights for {model2.name}\") # [cite: 41]\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model2.name} - {e}.\") # [cite: 41]\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\") # [cite: 41]\nhistory1_ft, model1 = train_model( # [cite: 41]\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase # [cite: 41]\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"], # [cite: 41]\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"], initial_epoch=0 # Start fine-tune epochs at 0 # [cite: 41]\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\") # [cite: 41]\nhistory2_ft, model2 = train_model( # [cite: 41]\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase # [cite: 42]\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"], # [cite: 42]\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"], initial_epoch=0 # Start fine-tune epochs at 0 # [cite: 42]\n)\n\n# Load best weights saved during the entire process\nprint(\"\\n--- Loading potentially best fine-tuned weights ---\") # [cite: 42]\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best weights for {model1.name}\") # [cite: 43]\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model1.name} - {e}\") # [cite: 43]\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best weights for {model2.name}\") # [cite: 44]\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model2.name} - {e}\") # [cite: 44]\n\n# =============================================================================\n# --- NEW: Ensemble Weight Optimization ---\n# =============================================================================\nprint(\"\\n--- Optimizing Ensemble Weights on Validation Set ---\")\n\n# Get predictions from both models on the validation set\nprint(\"Predicting on validation set with Model 1...\")\nval_preds1 = model1.predict(val_ds)\nprint(\"Predicting on validation set with Model 2...\")\nval_preds2 = model2.predict(val_ds)\n\n# Get true labels from the validation set\nprint(\"Extracting true labels from validation set...\")\ny_val_true = []\nfor _, labels_batch in val_ds: # Assuming val_ds is repeatable or cached\n    y_val_true.extend(np.argmax(labels_batch.numpy(), axis=1))\ny_val_true = np.array(y_val_true)\n\n# Ensure predictions match the number of true labels\nif len(val_preds1) != len(y_val_true) or len(val_preds2) != len(y_val_true):\n    print(f\"Warning: Validation prediction count ({len(val_preds1)}, {len(val_preds2)}) does not match true label count ({len(y_val_true)}). Skipping weight optimization.\")\n    optimal_weights = CONFIG[\"ENSEMBLE_WEIGHTS\"] # Fallback to config weights\nelse:\n    best_val_accuracy = -1\n    optimal_weights = CONFIG[\"ENSEMBLE_WEIGHTS\"] # Default\n\n    # Grid search for best weight for model1 (model2 weight = 1 - w1)\n    print(\"Searching for optimal ensemble weights...\")\n    for w1 in np.arange(0.0, 1.05, 0.05): # Search in 0.05 increments\n        w2 = 1.0 - w1\n        weighted_preds = w1 * val_preds1 + w2 * val_preds2\n        y_val_pred = np.argmax(weighted_preds, axis=1)\n        current_accuracy = accuracy_score(y_val_true, y_val_pred)\n        print(f\"  Weight [M1={w1:.2f}, M2={w2:.2f}] -> Validation Accuracy: {current_accuracy:.4f}\")\n        if current_accuracy > best_val_accuracy:\n            best_val_accuracy = current_accuracy\n            optimal_weights = [w1, w2]\n\nprint(f\"Optimal weights found: {optimal_weights} with Validation Accuracy: {best_val_accuracy:.4f}\")\n# You can override CONFIG or just use optimal_weights in evaluation\n# CONFIG[\"ENSEMBLE_WEIGHTS\"] = optimal_weights # Optional: Update config dictionary\n\n# =============================================================================\n# Build and Save the Ensemble Model Object (Using Average layer for simplicity)\n# NOTE: This saved model uses EQUAL weights. Optimized weights are applied during evaluation below.\n# =============================================================================\nprint(\"\\n--- Building Saveable Ensemble Model (Equal Weights in Saved Object) ---\") # [cite: 44]\nmodel1.trainable = False # [cite: 44]\nmodel2.trainable = False # [cite: 44]\ninput_shape = (CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], 3) # [cite: 45]\nensemble_input = tf.keras.layers.Input(shape=input_shape, name=\"ensemble_input\") # [cite: 45]\noutput1 = model1(ensemble_input) # [cite: 45]\noutput2 = model2(ensemble_input) # [cite: 45]\n# The saved model uses simple average. Optimized weights are applied at evaluation time.\nensemble_output = tf.keras.layers.Average(name=\"ensemble_average\")([output1, output2]) # [cite: 45]\nensemble_model = tf.keras.Model(inputs=ensemble_input, outputs=ensemble_output, name=\"Emotion_Ensemble_B0_B4\") # [cite: 45]\nensemble_model.summary() # [cite: 45]\nensemble_model_path = \"emotion_ensemble_final.keras\" # [cite: 45]\nensemble_model.save(ensemble_model_path) # [cite: 45]\nprint(f\"Ensemble model object saved successfully to {ensemble_model_path}\") # [cite: 45]\n\n# =============================================================================\n# Individual Model Evaluation (Same as V3.2)\n# =============================================================================\nprint(\"\\n=== Evaluating Individual Models ===\") # [cite: 46]\nmetrics_m1_affect = evaluate_model(model1, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 46]\nmetrics_m1_fer = evaluate_model(model1, fer2013_test_ds, class_names, \"FER2013\") # [cite: 46]\nmetrics_m2_affect = evaluate_model(model2, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 46]\nmetrics_m2_fer = evaluate_model(model2, fer2013_test_ds, class_names, \"FER2013\") # [cite: 46]\n\n# =============================================================================\n# Ensemble Prediction and Evaluation (Using OPTIMIZED weights)\n# =============================================================================\nmodels_to_ensemble = [model1, model2] # [cite: 46]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names_list, dataset_name): # [cite: 46]\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    if test_dataset is None: # [cite: 46]\n        print(f\"Skipping ensemble evaluation on {dataset_name}: Dataset not loaded.\") # [cite: 46]\n        return None # [cite: 46]\n\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} using Weights: {weights} ---\") # Modified print # [cite: 47]\n    all_preds_probs = [] # [cite: 47]\n    for i, model in enumerate(models): # [cite: 47]\n        print(f\"Predicting with model {i+1}: {model.name}\") # [cite: 47]\n        preds = model.predict(test_dataset) # [cite: 47]\n        all_preds_probs.append(preds) # [cite: 47]\n\n    # Weighted average of probabilities using the provided (optimized) weights\n    weighted_preds_probs = np.tensordot(weights, all_preds_probs, axes=([0],[0])) # [cite: 47]\n    y_pred = np.argmax(weighted_preds_probs, axis=1) # [cite: 47]\n\n    # Extract true labels\n    y_true = [] # [cite: 47]\n    for _, labels_batch in test_dataset: # [cite: 47]\n         y_true.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 48]\n    y_true = np.array(y_true) # [cite: 48]\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int) # [cite: 48]\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)] # [cite: 48]\n\n    if not present_class_names: # [cite: 48]\n         print(\"Warning: No predictable classes found in ensemble evaluation data.\") # [cite: 48]\n         return {\"accuracy\": 0, \"f1_score\": 0} # [cite: 48]\n\n    accuracy = np.mean(y_true == y_pred) # [cite: 48]\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0) # [cite: 48]\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\") # [cite: 48]\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\") # [cite: 49]\n    print(f\"Ensemble {dataset_name} Classification Report:\") # [cite: 49]\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0)) # [cite: 49]\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data) # [cite: 49]\n    plt.figure(figsize=(10, 8)) # [cite: 49]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=present_class_names, yticklabels=present_class_names) # [cite: 49]\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix') # [cite: 49]\n    plt.ylabel('Actual') # [cite: 49]\n    plt.xlabel('Predicted') # [cite: 49]\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix_w{weights[0]:.2f}_{weights[1]:.2f}.png\" # Add weights to filename\n    plt.savefig(cm_filename) # [cite: 49]\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\") # [cite: 49]\n    plt.close() # [cite: 49]\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1} # [cite: 49]\n\n# --- Evaluate Ensemble ---\nprint(\"\\n=== Evaluating Ensemble (Using Optimized Weights) ===\") # [cite: 49]\n# --- CHANGE: Use optimal_weights found previously ---\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, optimal_weights, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 49]\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, optimal_weights, fer2013_test_ds, class_names, \"FER2013\") # [cite: 50]\n\n# =============================================================================\n# Final Results Printout (Same as V3.2)\n# =============================================================================\nprint(\"\\n=== FINAL RESULTS ===\") # [cite: 50]\nprint(f\"(Using Optimized Ensemble Weights: {optimal_weights})\") # Added note\nprint(\"\\n--- Individual Model Performance ---\") # [cite: 50]\nif metrics_m1_affect: print(f\"{model1.name} AffectNet Test Accuracy: {metrics_m1_affect['accuracy']:.4f}, F1: {metrics_m1_affect['f1_score']:.4f}\") # [cite: 50]\nif metrics_m1_fer: print(f\"{model1.name} FER2013 Test Accuracy: {metrics_m1_fer['accuracy']:.4f}, F1: {metrics_m1_fer['f1_score']:.4f}\") # [cite: 50]\nif metrics_m2_affect: print(f\"{model2.name} AffectNet Test Accuracy: {metrics_m2_affect['accuracy']:.4f}, F1: {metrics_m2_affect['f1_score']:.4f}\") # [cite: 50]\nif metrics_m2_fer: print(f\"{model2.name} FER2013 Test Accuracy: {metrics_m2_fer['accuracy']:.4f}, F1: {metrics_m2_fer['f1_score']:.4f}\") # [cite: 50]\n\nprint(\"\\n--- Ensemble Performance ---\") # [cite: 50]\nif affectnet_metrics_ens: # [cite: 50]\n    print(f\"Ensemble AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\") # [cite: 50]\n    print(f\"Ensemble AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\") # [cite: 50]\nif fer_metrics_ens: # [cite: 50]\n    print(f\"Ensemble FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\") # [cite: 50]\n    print(f\"Ensemble FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\") # [cite: 50]\n\nprint(\"\\nTraining and evaluation complete.\") # [cite: 50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:51:26.583500Z","iopub.execute_input":"2025-04-08T06:51:26.583854Z","iopub.status.idle":"2025-04-08T12:32:03.330938Z","shell.execute_reply.started":"2025-04-08T06:51:26.583789Z","shell.execute_reply":"2025-04-08T12:32:03.329936Z"}},"outputs":[{"name":"stdout","text":"Found 1 GPUs: Memory growth enabled\nMixed precision enabled ('mixed_float16')\nLoading training/validation data from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Train\nFound 57744 files belonging to 8 classes.\nUsing 46196 files for training.\nFound 57744 files belonging to 8 classes.\nUsing 11548 files for validation.\nDataset loaded with classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nLoading test data for 'affectnet' from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test\nFound emotion folders: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\n  Found 4039 images for emotion 'surprise' in 'affectnet'.\n  Found 3176 images for emotion 'fear' in 'affectnet'.\n  Found 5126 images for emotion 'neutral' in 'affectnet'.\n  Found 3091 images for emotion 'sad' in 'affectnet'.\n  Found 2477 images for emotion 'disgust' in 'affectnet'.\n  Found 2871 images for emotion 'contempt' in 'affectnet'.\n  Found 5044 images for emotion 'happy' in 'affectnet'.\n  Found 3218 images for emotion 'anger' in 'affectnet'.\nTotal images found for 'affectnet' test set: 29042\nLoading test data for 'fer2013' from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test\nFound emotion folders: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\n  Found 831 images for emotion 'surprise' in 'fer2013'.\n  Found 1024 images for emotion 'fear' in 'fer2013'.\n  Found 1233 images for emotion 'neutral' in 'fer2013'.\n  Found 1247 images for emotion 'sad' in 'fer2013'.\n  Found 111 images for emotion 'disgust' in 'fer2013'.\n  Found 54 images for emotion 'contempt' in 'fer2013'.\n  Found 1774 images for emotion 'happy' in 'fer2013'.\n  Found 958 images for emotion 'anger' in 'fer2013'.\nTotal images found for 'fer2013' test set: 7232\nCalculating class weights...\nApproximate number of batches in training dataset: 1444\nUnique labels found for weight calculation: [0 1 2 3 4 5 6 7] with counts [5744 2362 2296 5804 9773 8078 6381 5758]\nClass weights calculated: {0: 1.0053098885793872, 1: 2.4447502116850126, 2: 2.515026132404181, 3: 0.9949172984148863, 4: 0.5908625805791466, 5: 0.7148427828670463, 6: 0.90495220184924, 7: 1.0028655783258076}\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEfficientNetB0 model built successfully.\nWarning: Building EfficientNetB4 with image size 160.\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\nEfficientNetB4 model built successfully.\n\nClass names being used: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nNumber of classes for models: 8\n\n=== Phase 1: Training Head of Model 1 ===\nSetting up head training for EfficientNetB0.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB0\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (\u001b[38;5;33mCast\u001b[0m)                                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │             \u001b[38;5;34m4,049,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m10,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">10,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,059,819\u001b[0m (15.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,059,819</span> (15.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,248\u001b[0m (40.03 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,248</span> (40.03 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Head Training for EfficientNetB0 (Epochs 0 to 50) ---\nEpoch 1/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2568 - loss: 1.9461\nEpoch 1: val_accuracy improved from -inf to 0.50113, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 44ms/step - accuracy: 0.2568 - loss: 1.9461 - val_accuracy: 0.5011 - val_loss: 1.7010\nEpoch 2/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3393 - loss: 1.8144\nEpoch 2: val_accuracy did not improve from 0.50113\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3393 - loss: 1.8144 - val_accuracy: 0.4832 - val_loss: 1.6891\nEpoch 3/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3475 - loss: 1.7988\nEpoch 3: val_accuracy improved from 0.50113 to 0.52174, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 39ms/step - accuracy: 0.3475 - loss: 1.7988 - val_accuracy: 0.5217 - val_loss: 1.6601\nEpoch 4/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3485 - loss: 1.7958\nEpoch 4: val_accuracy did not improve from 0.52174\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3485 - loss: 1.7958 - val_accuracy: 0.5110 - val_loss: 1.7159\nEpoch 5/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3527 - loss: 1.7933\nEpoch 5: val_accuracy did not improve from 0.52174\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 39ms/step - accuracy: 0.3527 - loss: 1.7933 - val_accuracy: 0.5179 - val_loss: 1.6427\nEpoch 6/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3528 - loss: 1.7926\nEpoch 6: val_accuracy did not improve from 0.52174\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3528 - loss: 1.7926 - val_accuracy: 0.4841 - val_loss: 1.6980\nEpoch 7/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3517 - loss: 1.7919\nEpoch 7: val_accuracy did not improve from 0.52174\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3517 - loss: 1.7919 - val_accuracy: 0.5119 - val_loss: 1.6460\nEpoch 8/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3516 - loss: 1.7903\nEpoch 8: val_accuracy did not improve from 0.52174\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3516 - loss: 1.7903 - val_accuracy: 0.4907 - val_loss: 1.6904\nEpoch 9/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3484 - loss: 1.7907\nEpoch 9: val_accuracy improved from 0.52174 to 0.57317, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3484 - loss: 1.7907 - val_accuracy: 0.5732 - val_loss: 1.5739\nEpoch 10/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3553 - loss: 1.7844\nEpoch 10: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3553 - loss: 1.7844 - val_accuracy: 0.5242 - val_loss: 1.6619\nEpoch 11/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3560 - loss: 1.7880\nEpoch 11: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3560 - loss: 1.7880 - val_accuracy: 0.5048 - val_loss: 1.6865\nEpoch 12/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3507 - loss: 1.7897\nEpoch 12: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3507 - loss: 1.7897 - val_accuracy: 0.4687 - val_loss: 1.7174\nEpoch 13/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3570 - loss: 1.7837\nEpoch 13: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 38ms/step - accuracy: 0.3570 - loss: 1.7837 - val_accuracy: 0.5195 - val_loss: 1.6716\nEpoch 14/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3495 - loss: 1.7855\nEpoch 14: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 38ms/step - accuracy: 0.3495 - loss: 1.7854 - val_accuracy: 0.4809 - val_loss: 1.6859\nEpoch 15/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3564 - loss: 1.7832\nEpoch 15: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3564 - loss: 1.7832 - val_accuracy: 0.4887 - val_loss: 1.6858\nEpoch 16/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3556 - loss: 1.7834\nEpoch 16: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 38ms/step - accuracy: 0.3556 - loss: 1.7834 - val_accuracy: 0.4637 - val_loss: 1.7178\nEpoch 17/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3552 - loss: 1.7766\nEpoch 17: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - accuracy: 0.3552 - loss: 1.7766 - val_accuracy: 0.5087 - val_loss: 1.6753\nEpoch 18/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3571 - loss: 1.7811\nEpoch 18: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - accuracy: 0.3571 - loss: 1.7811 - val_accuracy: 0.5159 - val_loss: 1.6645\nEpoch 19/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3565 - loss: 1.7789\nEpoch 19: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - accuracy: 0.3565 - loss: 1.7789 - val_accuracy: 0.4635 - val_loss: 1.7534\nEpoch 20/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3578 - loss: 1.7707\nEpoch 20: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - accuracy: 0.3578 - loss: 1.7707 - val_accuracy: 0.4620 - val_loss: 1.7064\nEpoch 21/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3625 - loss: 1.7683\nEpoch 21: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - accuracy: 0.3625 - loss: 1.7683 - val_accuracy: 0.5048 - val_loss: 1.6533\nEpoch 22/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3601 - loss: 1.7730\nEpoch 22: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3601 - loss: 1.7730 - val_accuracy: 0.5246 - val_loss: 1.6394\nEpoch 23/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3594 - loss: 1.7671\nEpoch 23: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3594 - loss: 1.7671 - val_accuracy: 0.5063 - val_loss: 1.6418\nEpoch 24/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3612 - loss: 1.7723\nEpoch 24: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3612 - loss: 1.7723 - val_accuracy: 0.4808 - val_loss: 1.6813\nEpoch 25/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3587 - loss: 1.7698\nEpoch 25: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3587 - loss: 1.7698 - val_accuracy: 0.4818 - val_loss: 1.6809\nEpoch 26/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3642 - loss: 1.7638\nEpoch 26: val_accuracy did not improve from 0.57317\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 39ms/step - accuracy: 0.3642 - loss: 1.7638 - val_accuracy: 0.4958 - val_loss: 1.6757\nEpoch 27/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3561 - loss: 1.8029\nEpoch 2: val_accuracy improved from 0.48788 to 0.49844, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 90ms/step - accuracy: 0.3561 - loss: 1.8028 - val_accuracy: 0.4984 - val_loss: 1.6266\nEpoch 3/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3624 - loss: 1.7879\nEpoch 3: val_accuracy improved from 0.49844 to 0.52208, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 90ms/step - accuracy: 0.3624 - loss: 1.7879 - val_accuracy: 0.5221 - val_loss: 1.5921\nEpoch 4/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3638 - loss: 1.7882\nEpoch 4: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 89ms/step - accuracy: 0.3638 - loss: 1.7882 - val_accuracy: 0.5083 - val_loss: 1.6095\nEpoch 5/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3669 - loss: 1.7870\nEpoch 5: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 89ms/step - accuracy: 0.3669 - loss: 1.7870 - val_accuracy: 0.4732 - val_loss: 1.6669\nEpoch 6/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3604 - loss: 1.7885\nEpoch 6: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3604 - loss: 1.7885 - val_accuracy: 0.5164 - val_loss: 1.6149\nEpoch 7/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3672 - loss: 1.7827\nEpoch 7: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3672 - loss: 1.7827 - val_accuracy: 0.4619 - val_loss: 1.6873\nEpoch 8/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3698 - loss: 1.7779\nEpoch 8: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3698 - loss: 1.7779 - val_accuracy: 0.4760 - val_loss: 1.6684\nEpoch 9/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3700 - loss: 1.7811\nEpoch 9: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3700 - loss: 1.7811 - val_accuracy: 0.5099 - val_loss: 1.6000\nEpoch 10/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3699 - loss: 1.7823\nEpoch 10: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3699 - loss: 1.7823 - val_accuracy: 0.5175 - val_loss: 1.5937\nEpoch 11/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3710 - loss: 1.7786\nEpoch 11: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3710 - loss: 1.7786 - val_accuracy: 0.5194 - val_loss: 1.5894\nEpoch 12/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3657 - loss: 1.7825\nEpoch 12: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 90ms/step - accuracy: 0.3657 - loss: 1.7825 - val_accuracy: 0.5214 - val_loss: 1.5693\nEpoch 13/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3682 - loss: 1.7805\nEpoch 13: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 87ms/step - accuracy: 0.3682 - loss: 1.7805 - val_accuracy: 0.5080 - val_loss: 1.6007\nEpoch 14/50\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3751 - loss: 1.7757\nEpoch 14: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 88ms/step - accuracy: 0.3751 - loss: 1.7757 - val_accuracy: 0.5053 - val_loss: 1.6179\nEpoch 15/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3725 - loss: 1.7754\nEpoch 15: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 88ms/step - accuracy: 0.3725 - loss: 1.7753 - val_accuracy: 0.5210 - val_loss: 1.5866\nEpoch 16/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3740 - loss: 1.7714\nEpoch 16: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 89ms/step - accuracy: 0.3740 - loss: 1.7714 - val_accuracy: 0.4907 - val_loss: 1.6449\nEpoch 17/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3754 - loss: 1.7620\nEpoch 17: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3754 - loss: 1.7620 - val_accuracy: 0.4781 - val_loss: 1.6477\nEpoch 18/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3705 - loss: 1.7723\nEpoch 18: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3705 - loss: 1.7723 - val_accuracy: 0.4719 - val_loss: 1.6554\nEpoch 19/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3742 - loss: 1.7624\nEpoch 19: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 89ms/step - accuracy: 0.3742 - loss: 1.7624 - val_accuracy: 0.5115 - val_loss: 1.6035\nEpoch 20/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3744 - loss: 1.7620\nEpoch 20: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 89ms/step - accuracy: 0.3744 - loss: 1.7620 - val_accuracy: 0.4678 - val_loss: 1.6605\nEpoch 21/50\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3767 - loss: 1.7649\nEpoch 21: val_accuracy did not improve from 0.52208\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 89ms/step - accuracy: 0.3767 - loss: 1.7649 - val_accuracy: 0.4632 - val_loss: 1.6622\nEpoch 21: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\n--- Loading best weights after head training ---\nLoaded best head weights for EfficientNetB0\nLoaded best head weights for EfficientNetB4\n\n=== Phase 2: Fine-tuning Model 1 ===\nSetting up fine-tuning for EfficientNetB0...\nUnfreezing top 15 layers of efficientnetb0 (out of 239). Freezing up to layer 224.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB0\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (\u001b[38;5;33mCast\u001b[0m)                                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │             \u001b[38;5;34m4,049,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m10,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">10,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,059,819\u001b[0m (15.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,059,819</span> (15.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m910,648\u001b[0m (3.47 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">910,648</span> (3.47 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,149,171\u001b[0m (12.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,149,171</span> (12.01 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Fine-tuning for EfficientNetB0 (Epochs 0 to 100) ---\nEpoch 1/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3696 - loss: 1.7508\nEpoch 1: val_accuracy improved from -inf to 0.52676, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 44ms/step - accuracy: 0.3697 - loss: 1.7507 - val_accuracy: 0.5268 - val_loss: 1.5768 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4394 - loss: 1.6245\nEpoch 2: val_accuracy improved from 0.52676 to 0.52780, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.4394 - loss: 1.6245 - val_accuracy: 0.5278 - val_loss: 1.5369 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4738 - loss: 1.5650\nEpoch 3: val_accuracy improved from 0.52780 to 0.53819, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.4738 - loss: 1.5650 - val_accuracy: 0.5382 - val_loss: 1.5332 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4882 - loss: 1.5255\nEpoch 4: val_accuracy improved from 0.53819 to 0.56070, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.4882 - loss: 1.5255 - val_accuracy: 0.5607 - val_loss: 1.4892 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5100 - loss: 1.4906\nEpoch 5: val_accuracy improved from 0.56070 to 0.57352, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.5100 - loss: 1.4906 - val_accuracy: 0.5735 - val_loss: 1.4618 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5224 - loss: 1.4685\nEpoch 6: val_accuracy did not improve from 0.57352\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.5224 - loss: 1.4685 - val_accuracy: 0.5267 - val_loss: 1.5284 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5355 - loss: 1.4487\nEpoch 7: val_accuracy improved from 0.57352 to 0.58919, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.5355 - loss: 1.4487 - val_accuracy: 0.5892 - val_loss: 1.4059 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5470 - loss: 1.4250\nEpoch 8: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5470 - loss: 1.4250 - val_accuracy: 0.5688 - val_loss: 1.4462 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5510 - loss: 1.4097\nEpoch 9: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5511 - loss: 1.4097 - val_accuracy: 0.5774 - val_loss: 1.4258 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5569 - loss: 1.3900\nEpoch 10: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5569 - loss: 1.3900 - val_accuracy: 0.5803 - val_loss: 1.4087 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5683 - loss: 1.3787\nEpoch 11: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5683 - loss: 1.3787 - val_accuracy: 0.5725 - val_loss: 1.3983 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5782 - loss: 1.3602\nEpoch 12: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5782 - loss: 1.3602 - val_accuracy: 0.5869 - val_loss: 1.3932 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5834 - loss: 1.3494\nEpoch 13: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5834 - loss: 1.3494 - val_accuracy: 0.5543 - val_loss: 1.4301 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5910 - loss: 1.3320\nEpoch 14: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5910 - loss: 1.3320 - val_accuracy: 0.5496 - val_loss: 1.4410 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5906 - loss: 1.3288\nEpoch 15: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5906 - loss: 1.3288 - val_accuracy: 0.5604 - val_loss: 1.4091 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5995 - loss: 1.3139\nEpoch 16: val_accuracy did not improve from 0.58919\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.5995 - loss: 1.3139 - val_accuracy: 0.5467 - val_loss: 1.4466 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6063 - loss: 1.3003\nEpoch 17: val_accuracy improved from 0.58919 to 0.59274, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6063 - loss: 1.3003 - val_accuracy: 0.5927 - val_loss: 1.3757 - learning_rate: 1.0000e-04\nEpoch 18/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6106 - loss: 1.2951\nEpoch 18: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.6106 - loss: 1.2951 - val_accuracy: 0.5578 - val_loss: 1.4284 - learning_rate: 1.0000e-04\nEpoch 19/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6119 - loss: 1.2890\nEpoch 19: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6119 - loss: 1.2890 - val_accuracy: 0.5572 - val_loss: 1.4155 - learning_rate: 1.0000e-04\nEpoch 20/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6205 - loss: 1.2731\nEpoch 20: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6205 - loss: 1.2731 - val_accuracy: 0.5731 - val_loss: 1.3904 - learning_rate: 1.0000e-04\nEpoch 21/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6194 - loss: 1.2681\nEpoch 21: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6194 - loss: 1.2681 - val_accuracy: 0.5826 - val_loss: 1.3809 - learning_rate: 1.0000e-04\nEpoch 22/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6283 - loss: 1.2550\nEpoch 22: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6283 - loss: 1.2550 - val_accuracy: 0.5620 - val_loss: 1.4050 - learning_rate: 1.0000e-04\nEpoch 23/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6316 - loss: 1.2431\nEpoch 23: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.6316 - loss: 1.2431 - val_accuracy: 0.5856 - val_loss: 1.3742 - learning_rate: 1.0000e-04\nEpoch 24/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6330 - loss: 1.2430\nEpoch 24: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.6330 - loss: 1.2430 - val_accuracy: 0.5653 - val_loss: 1.3995 - learning_rate: 1.0000e-04\nEpoch 25/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6363 - loss: 1.2309\nEpoch 25: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.6363 - loss: 1.2309 - val_accuracy: 0.5571 - val_loss: 1.4124 - learning_rate: 1.0000e-04\nEpoch 26/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6401 - loss: 1.2266\nEpoch 26: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6401 - loss: 1.2266 - val_accuracy: 0.5749 - val_loss: 1.3947 - learning_rate: 1.0000e-04\nEpoch 27/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6466 - loss: 1.2132\nEpoch 27: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6466 - loss: 1.2132 - val_accuracy: 0.5850 - val_loss: 1.3690 - learning_rate: 1.0000e-04\nEpoch 28/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6542 - loss: 1.2065\nEpoch 28: val_accuracy did not improve from 0.59274\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6542 - loss: 1.2065 - val_accuracy: 0.5719 - val_loss: 1.3765 - learning_rate: 1.0000e-04\nEpoch 29/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6569 - loss: 1.1982\nEpoch 29: val_accuracy improved from 0.59274 to 0.60608, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.6569 - loss: 1.1982 - val_accuracy: 0.6061 - val_loss: 1.3209 - learning_rate: 1.0000e-04\nEpoch 30/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6562 - loss: 1.1990\nEpoch 30: val_accuracy improved from 0.60608 to 0.60885, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.6562 - loss: 1.1990 - val_accuracy: 0.6089 - val_loss: 1.3283 - learning_rate: 1.0000e-04\nEpoch 31/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6620 - loss: 1.1840\nEpoch 31: val_accuracy did not improve from 0.60885\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6620 - loss: 1.1840 - val_accuracy: 0.5950 - val_loss: 1.3542 - learning_rate: 1.0000e-04\nEpoch 32/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6605 - loss: 1.1847\nEpoch 32: val_accuracy did not improve from 0.60885\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6605 - loss: 1.1847 - val_accuracy: 0.6054 - val_loss: 1.3184 - learning_rate: 1.0000e-04\nEpoch 33/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6678 - loss: 1.1783\nEpoch 33: val_accuracy improved from 0.60885 to 0.61560, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.6678 - loss: 1.1783 - val_accuracy: 0.6156 - val_loss: 1.3113 - learning_rate: 1.0000e-04\nEpoch 34/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6694 - loss: 1.1690\nEpoch 34: val_accuracy did not improve from 0.61560\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6694 - loss: 1.1690 - val_accuracy: 0.6063 - val_loss: 1.3288 - learning_rate: 1.0000e-04\nEpoch 35/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6713 - loss: 1.1616\nEpoch 35: val_accuracy improved from 0.61560 to 0.63569, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.6713 - loss: 1.1616 - val_accuracy: 0.6357 - val_loss: 1.2850 - learning_rate: 1.0000e-04\nEpoch 36/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6747 - loss: 1.1578\nEpoch 36: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6747 - loss: 1.1578 - val_accuracy: 0.6036 - val_loss: 1.3204 - learning_rate: 1.0000e-04\nEpoch 37/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6735 - loss: 1.1549\nEpoch 37: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6735 - loss: 1.1549 - val_accuracy: 0.6247 - val_loss: 1.2879 - learning_rate: 1.0000e-04\nEpoch 38/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6819 - loss: 1.1416\nEpoch 38: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6819 - loss: 1.1416 - val_accuracy: 0.6034 - val_loss: 1.3187 - learning_rate: 1.0000e-04\nEpoch 39/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6850 - loss: 1.1359\nEpoch 39: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6850 - loss: 1.1359 - val_accuracy: 0.6102 - val_loss: 1.3100 - learning_rate: 1.0000e-04\nEpoch 40/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6848 - loss: 1.1329\nEpoch 40: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6848 - loss: 1.1329 - val_accuracy: 0.5912 - val_loss: 1.3502 - learning_rate: 1.0000e-04\nEpoch 41/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6897 - loss: 1.1283\nEpoch 41: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6897 - loss: 1.1283 - val_accuracy: 0.6217 - val_loss: 1.2972 - learning_rate: 1.0000e-04\nEpoch 42/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6939 - loss: 1.1230\nEpoch 42: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6939 - loss: 1.1230 - val_accuracy: 0.6300 - val_loss: 1.2862 - learning_rate: 1.0000e-04\nEpoch 43/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6962 - loss: 1.1198\nEpoch 43: val_accuracy did not improve from 0.63569\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6962 - loss: 1.1198 - val_accuracy: 0.6051 - val_loss: 1.3207 - learning_rate: 1.0000e-04\nEpoch 44/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6976 - loss: 1.1113\nEpoch 44: val_accuracy did not improve from 0.63569\n\nEpoch 44: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.6976 - loss: 1.1113 - val_accuracy: 0.6283 - val_loss: 1.3055 - learning_rate: 1.0000e-04\nEpoch 45/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7024 - loss: 1.1005\nEpoch 45: val_accuracy improved from 0.63569 to 0.64704, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 42ms/step - accuracy: 0.7024 - loss: 1.1005 - val_accuracy: 0.6470 - val_loss: 1.2656 - learning_rate: 2.0000e-05\nEpoch 46/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7094 - loss: 1.0923\nEpoch 46: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.7094 - loss: 1.0923 - val_accuracy: 0.6429 - val_loss: 1.2633 - learning_rate: 2.0000e-05\nEpoch 47/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7126 - loss: 1.0864\nEpoch 47: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7126 - loss: 1.0864 - val_accuracy: 0.6400 - val_loss: 1.2680 - learning_rate: 2.0000e-05\nEpoch 48/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7055 - loss: 1.0883\nEpoch 48: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7055 - loss: 1.0883 - val_accuracy: 0.6354 - val_loss: 1.2756 - learning_rate: 2.0000e-05\nEpoch 49/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7118 - loss: 1.0794\nEpoch 49: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7118 - loss: 1.0794 - val_accuracy: 0.6399 - val_loss: 1.2729 - learning_rate: 2.0000e-05\nEpoch 50/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7148 - loss: 1.0825\nEpoch 50: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7148 - loss: 1.0825 - val_accuracy: 0.6420 - val_loss: 1.2660 - learning_rate: 2.0000e-05\nEpoch 51/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7169 - loss: 1.0786\nEpoch 51: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7169 - loss: 1.0786 - val_accuracy: 0.6345 - val_loss: 1.2794 - learning_rate: 2.0000e-05\nEpoch 52/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7164 - loss: 1.0724\nEpoch 52: val_accuracy did not improve from 0.64704\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7164 - loss: 1.0724 - val_accuracy: 0.6412 - val_loss: 1.2658 - learning_rate: 2.0000e-05\nEpoch 53/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7186 - loss: 1.0725\nEpoch 53: val_accuracy improved from 0.64704 to 0.64721, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.7186 - loss: 1.0725 - val_accuracy: 0.6472 - val_loss: 1.2534 - learning_rate: 2.0000e-05\nEpoch 54/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7179 - loss: 1.0751\nEpoch 54: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7179 - loss: 1.0751 - val_accuracy: 0.6408 - val_loss: 1.2695 - learning_rate: 2.0000e-05\nEpoch 55/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7208 - loss: 1.0738\nEpoch 55: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7208 - loss: 1.0738 - val_accuracy: 0.6414 - val_loss: 1.2632 - learning_rate: 2.0000e-05\nEpoch 56/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7180 - loss: 1.0657\nEpoch 56: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7180 - loss: 1.0657 - val_accuracy: 0.6455 - val_loss: 1.2495 - learning_rate: 2.0000e-05\nEpoch 57/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7200 - loss: 1.0674\nEpoch 57: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7200 - loss: 1.0674 - val_accuracy: 0.6316 - val_loss: 1.2767 - learning_rate: 2.0000e-05\nEpoch 58/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7174 - loss: 1.0690\nEpoch 58: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.7174 - loss: 1.0690 - val_accuracy: 0.6340 - val_loss: 1.2806 - learning_rate: 2.0000e-05\nEpoch 59/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7255 - loss: 1.0611\nEpoch 59: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7255 - loss: 1.0611 - val_accuracy: 0.6431 - val_loss: 1.2557 - learning_rate: 2.0000e-05\nEpoch 60/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7195 - loss: 1.0658\nEpoch 60: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7195 - loss: 1.0658 - val_accuracy: 0.6414 - val_loss: 1.2621 - learning_rate: 2.0000e-05\nEpoch 61/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7227 - loss: 1.0628\nEpoch 61: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7227 - loss: 1.0628 - val_accuracy: 0.6379 - val_loss: 1.2647 - learning_rate: 2.0000e-05\nEpoch 62/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7230 - loss: 1.0619\nEpoch 62: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7230 - loss: 1.0618 - val_accuracy: 0.6257 - val_loss: 1.2841 - learning_rate: 2.0000e-05\nEpoch 63/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7219 - loss: 1.0626\nEpoch 63: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7219 - loss: 1.0626 - val_accuracy: 0.6363 - val_loss: 1.2695 - learning_rate: 2.0000e-05\nEpoch 64/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7240 - loss: 1.0576\nEpoch 64: val_accuracy did not improve from 0.64721\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7240 - loss: 1.0576 - val_accuracy: 0.6299 - val_loss: 1.2762 - learning_rate: 2.0000e-05\nEpoch 65/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7252 - loss: 1.0562\nEpoch 65: val_accuracy improved from 0.64721 to 0.64834, saving model to best_EfficientNetB0.keras\n\nEpoch 65: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.7252 - loss: 1.0562 - val_accuracy: 0.6483 - val_loss: 1.2530 - learning_rate: 2.0000e-05\nEpoch 66/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7258 - loss: 1.0558\nEpoch 66: val_accuracy did not improve from 0.64834\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7258 - loss: 1.0558 - val_accuracy: 0.6450 - val_loss: 1.2590 - learning_rate: 4.0000e-06\nEpoch 67/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7279 - loss: 1.0525\nEpoch 67: val_accuracy did not improve from 0.64834\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7279 - loss: 1.0525 - val_accuracy: 0.6403 - val_loss: 1.2655 - learning_rate: 4.0000e-06\nEpoch 68/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7287 - loss: 1.0558\nEpoch 68: val_accuracy did not improve from 0.64834\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7287 - loss: 1.0558 - val_accuracy: 0.6460 - val_loss: 1.2561 - learning_rate: 4.0000e-06\nEpoch 69/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7277 - loss: 1.0549\nEpoch 69: val_accuracy improved from 0.64834 to 0.64894, saving model to best_EfficientNetB0.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.7277 - loss: 1.0548 - val_accuracy: 0.6489 - val_loss: 1.2511 - learning_rate: 4.0000e-06\nEpoch 70/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7328 - loss: 1.0480\nEpoch 70: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7328 - loss: 1.0480 - val_accuracy: 0.6473 - val_loss: 1.2528 - learning_rate: 4.0000e-06\nEpoch 71/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7264 - loss: 1.0497\nEpoch 71: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7264 - loss: 1.0497 - val_accuracy: 0.6465 - val_loss: 1.2498 - learning_rate: 4.0000e-06\nEpoch 72/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7277 - loss: 1.0518\nEpoch 72: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7277 - loss: 1.0518 - val_accuracy: 0.6394 - val_loss: 1.2643 - learning_rate: 4.0000e-06\nEpoch 73/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7269 - loss: 1.0517\nEpoch 73: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7269 - loss: 1.0517 - val_accuracy: 0.6458 - val_loss: 1.2538 - learning_rate: 4.0000e-06\nEpoch 74/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7305 - loss: 1.0543\nEpoch 74: val_accuracy did not improve from 0.64894\n\nEpoch 74: ReduceLROnPlateau reducing learning rate to 1e-06.\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7305 - loss: 1.0543 - val_accuracy: 0.6470 - val_loss: 1.2516 - learning_rate: 4.0000e-06\nEpoch 75/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7262 - loss: 1.0506\nEpoch 75: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7262 - loss: 1.0506 - val_accuracy: 0.6463 - val_loss: 1.2530 - learning_rate: 1.0000e-06\nEpoch 76/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7314 - loss: 1.0502\nEpoch 76: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7314 - loss: 1.0502 - val_accuracy: 0.6444 - val_loss: 1.2556 - learning_rate: 1.0000e-06\nEpoch 77/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7274 - loss: 1.0553\nEpoch 77: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7274 - loss: 1.0553 - val_accuracy: 0.6438 - val_loss: 1.2558 - learning_rate: 1.0000e-06\nEpoch 78/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7296 - loss: 1.0504\nEpoch 78: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7296 - loss: 1.0504 - val_accuracy: 0.6439 - val_loss: 1.2549 - learning_rate: 1.0000e-06\nEpoch 79/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7263 - loss: 1.0544\nEpoch 79: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7263 - loss: 1.0544 - val_accuracy: 0.6424 - val_loss: 1.2568 - learning_rate: 1.0000e-06\nEpoch 80/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7261 - loss: 1.0535\nEpoch 80: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7262 - loss: 1.0535 - val_accuracy: 0.6431 - val_loss: 1.2575 - learning_rate: 1.0000e-06\nEpoch 81/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7298 - loss: 1.0491\nEpoch 81: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7298 - loss: 1.0491 - val_accuracy: 0.6430 - val_loss: 1.2574 - learning_rate: 1.0000e-06\nEpoch 82/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7324 - loss: 1.0477\nEpoch 82: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7324 - loss: 1.0477 - val_accuracy: 0.6439 - val_loss: 1.2559 - learning_rate: 1.0000e-06\nEpoch 83/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7333 - loss: 1.0470\nEpoch 83: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7333 - loss: 1.0470 - val_accuracy: 0.6435 - val_loss: 1.2582 - learning_rate: 1.0000e-06\nEpoch 84/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7292 - loss: 1.0537\nEpoch 84: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7292 - loss: 1.0537 - val_accuracy: 0.6434 - val_loss: 1.2568 - learning_rate: 1.0000e-06\nEpoch 85/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7308 - loss: 1.0481\nEpoch 85: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7308 - loss: 1.0481 - val_accuracy: 0.6442 - val_loss: 1.2546 - learning_rate: 1.0000e-06\nEpoch 86/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7309 - loss: 1.0460\nEpoch 86: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.7309 - loss: 1.0460 - val_accuracy: 0.6449 - val_loss: 1.2543 - learning_rate: 1.0000e-06\nEpoch 87/100\n\u001b[1m1443/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7296 - loss: 1.0498\nEpoch 87: val_accuracy did not improve from 0.64894\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.7296 - loss: 1.0498 - val_accuracy: 0.6437 - val_loss: 1.2572 - learning_rate: 1.0000e-06\nEpoch 87: early stopping\nRestoring model weights from the end of the best epoch: 69.\n\n=== Phase 2: Fine-tuning Model 2 ===\nSetting up fine-tuning for EfficientNetB4...\nUnfreezing top 20 layers of efficientnetb4 (out of 476). Freezing up to layer 456.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │            \u001b[38;5;34m17,673,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m14,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">14,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,688,167\u001b[0m (67.48 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,688,167</span> (67.48 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,854,712\u001b[0m (14.70 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,854,712</span> (14.70 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m13,833,455\u001b[0m (52.77 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,833,455</span> (52.77 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Fine-tuning for EfficientNetB4 (Epochs 0 to 100) ---\nEpoch 1/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3881 - loss: 1.7362\nEpoch 1: val_accuracy improved from -inf to 0.47480, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 122ms/step - accuracy: 0.3881 - loss: 1.7362 - val_accuracy: 0.4748 - val_loss: 1.6231 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.4474 - loss: 1.6304\nEpoch 2: val_accuracy improved from 0.47480 to 0.53022, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 117ms/step - accuracy: 0.4474 - loss: 1.6304 - val_accuracy: 0.5302 - val_loss: 1.5128 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.4739 - loss: 1.5725\nEpoch 3: val_accuracy improved from 0.53022 to 0.56824, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.4739 - loss: 1.5725 - val_accuracy: 0.5682 - val_loss: 1.4583 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.4923 - loss: 1.5337\nEpoch 4: val_accuracy did not improve from 0.56824\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.4923 - loss: 1.5337 - val_accuracy: 0.5145 - val_loss: 1.5178 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5086 - loss: 1.4969\nEpoch 5: val_accuracy did not improve from 0.56824\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.5086 - loss: 1.4969 - val_accuracy: 0.5479 - val_loss: 1.4651 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5221 - loss: 1.4732\nEpoch 6: val_accuracy did not improve from 0.56824\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.5221 - loss: 1.4731 - val_accuracy: 0.5494 - val_loss: 1.4709 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5403 - loss: 1.4374\nEpoch 7: val_accuracy improved from 0.56824 to 0.59404, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.5403 - loss: 1.4374 - val_accuracy: 0.5940 - val_loss: 1.3780 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.5485 - loss: 1.4147\nEpoch 8: val_accuracy did not improve from 0.59404\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.5485 - loss: 1.4147 - val_accuracy: 0.5794 - val_loss: 1.4144 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5609 - loss: 1.4003\nEpoch 9: val_accuracy improved from 0.59404 to 0.60175, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.5609 - loss: 1.4003 - val_accuracy: 0.6017 - val_loss: 1.3631 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.5707 - loss: 1.3707\nEpoch 10: val_accuracy improved from 0.60175 to 0.61301, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 117ms/step - accuracy: 0.5707 - loss: 1.3707 - val_accuracy: 0.6130 - val_loss: 1.3422 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.5786 - loss: 1.3514\nEpoch 11: val_accuracy did not improve from 0.61301\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.5786 - loss: 1.3514 - val_accuracy: 0.6047 - val_loss: 1.3374 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5922 - loss: 1.3242\nEpoch 12: val_accuracy did not improve from 0.61301\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 114ms/step - accuracy: 0.5922 - loss: 1.3242 - val_accuracy: 0.6019 - val_loss: 1.3526 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6017 - loss: 1.3126\nEpoch 13: val_accuracy improved from 0.61301 to 0.62695, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 117ms/step - accuracy: 0.6017 - loss: 1.3126 - val_accuracy: 0.6269 - val_loss: 1.3074 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6084 - loss: 1.2967\nEpoch 14: val_accuracy improved from 0.62695 to 0.63751, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 116ms/step - accuracy: 0.6084 - loss: 1.2967 - val_accuracy: 0.6375 - val_loss: 1.2752 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.6225 - loss: 1.2691\nEpoch 15: val_accuracy improved from 0.63751 to 0.64669, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.6225 - loss: 1.2691 - val_accuracy: 0.6467 - val_loss: 1.2826 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6293 - loss: 1.2567\nEpoch 16: val_accuracy did not improve from 0.64669\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6293 - loss: 1.2567 - val_accuracy: 0.6277 - val_loss: 1.3106 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6354 - loss: 1.2462\nEpoch 17: val_accuracy did not improve from 0.64669\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.6354 - loss: 1.2462 - val_accuracy: 0.6368 - val_loss: 1.2817 - learning_rate: 1.0000e-04\nEpoch 18/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.6436 - loss: 1.2262\nEpoch 18: val_accuracy did not improve from 0.64669\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.6436 - loss: 1.2262 - val_accuracy: 0.6256 - val_loss: 1.3108 - learning_rate: 1.0000e-04\nEpoch 19/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6495 - loss: 1.2132\nEpoch 19: val_accuracy improved from 0.64669 to 0.65942, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 117ms/step - accuracy: 0.6495 - loss: 1.2132 - val_accuracy: 0.6594 - val_loss: 1.2543 - learning_rate: 1.0000e-04\nEpoch 20/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6488 - loss: 1.2049\nEpoch 20: val_accuracy improved from 0.65942 to 0.66159, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 117ms/step - accuracy: 0.6488 - loss: 1.2049 - val_accuracy: 0.6616 - val_loss: 1.2319 - learning_rate: 1.0000e-04\nEpoch 21/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6645 - loss: 1.1893\nEpoch 21: val_accuracy improved from 0.66159 to 0.68142, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.6645 - loss: 1.1893 - val_accuracy: 0.6814 - val_loss: 1.2046 - learning_rate: 1.0000e-04\nEpoch 22/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6696 - loss: 1.1766\nEpoch 22: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.6696 - loss: 1.1766 - val_accuracy: 0.6440 - val_loss: 1.2461 - learning_rate: 1.0000e-04\nEpoch 23/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6771 - loss: 1.1600\nEpoch 23: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6771 - loss: 1.1600 - val_accuracy: 0.6740 - val_loss: 1.2096 - learning_rate: 1.0000e-04\nEpoch 24/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6790 - loss: 1.1520\nEpoch 24: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6790 - loss: 1.1520 - val_accuracy: 0.6807 - val_loss: 1.2023 - learning_rate: 1.0000e-04\nEpoch 25/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6846 - loss: 1.1435\nEpoch 25: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6846 - loss: 1.1435 - val_accuracy: 0.6811 - val_loss: 1.2028 - learning_rate: 1.0000e-04\nEpoch 26/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6924 - loss: 1.1324\nEpoch 26: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.6924 - loss: 1.1324 - val_accuracy: 0.6714 - val_loss: 1.2004 - learning_rate: 1.0000e-04\nEpoch 27/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6892 - loss: 1.1288\nEpoch 27: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6892 - loss: 1.1288 - val_accuracy: 0.6660 - val_loss: 1.2174 - learning_rate: 1.0000e-04\nEpoch 28/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6969 - loss: 1.1198\nEpoch 28: val_accuracy did not improve from 0.68142\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.6969 - loss: 1.1198 - val_accuracy: 0.6764 - val_loss: 1.1911 - learning_rate: 1.0000e-04\nEpoch 29/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7030 - loss: 1.1025\nEpoch 29: val_accuracy improved from 0.68142 to 0.68696, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.7030 - loss: 1.1025 - val_accuracy: 0.6870 - val_loss: 1.1864 - learning_rate: 1.0000e-04\nEpoch 30/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7128 - loss: 1.0909\nEpoch 30: val_accuracy improved from 0.68696 to 0.68973, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 117ms/step - accuracy: 0.7128 - loss: 1.0909 - val_accuracy: 0.6897 - val_loss: 1.1695 - learning_rate: 1.0000e-04\nEpoch 31/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7167 - loss: 1.0825\nEpoch 31: val_accuracy did not improve from 0.68973\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7167 - loss: 1.0825 - val_accuracy: 0.6838 - val_loss: 1.1665 - learning_rate: 1.0000e-04\nEpoch 32/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7184 - loss: 1.0836\nEpoch 32: val_accuracy improved from 0.68973 to 0.71372, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 117ms/step - accuracy: 0.7184 - loss: 1.0836 - val_accuracy: 0.7137 - val_loss: 1.1408 - learning_rate: 1.0000e-04\nEpoch 33/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7219 - loss: 1.0739\nEpoch 33: val_accuracy did not improve from 0.71372\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7219 - loss: 1.0739 - val_accuracy: 0.7012 - val_loss: 1.1461 - learning_rate: 1.0000e-04\nEpoch 34/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7290 - loss: 1.0633\nEpoch 34: val_accuracy did not improve from 0.71372\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7290 - loss: 1.0633 - val_accuracy: 0.7041 - val_loss: 1.1405 - learning_rate: 1.0000e-04\nEpoch 35/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7355 - loss: 1.0540\nEpoch 35: val_accuracy improved from 0.71372 to 0.71476, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 116ms/step - accuracy: 0.7355 - loss: 1.0540 - val_accuracy: 0.7148 - val_loss: 1.1292 - learning_rate: 1.0000e-04\nEpoch 36/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7376 - loss: 1.0468\nEpoch 36: val_accuracy improved from 0.71476 to 0.72506, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 116ms/step - accuracy: 0.7376 - loss: 1.0468 - val_accuracy: 0.7251 - val_loss: 1.0976 - learning_rate: 1.0000e-04\nEpoch 37/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7410 - loss: 1.0410\nEpoch 37: val_accuracy did not improve from 0.72506\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7410 - loss: 1.0410 - val_accuracy: 0.7062 - val_loss: 1.1464 - learning_rate: 1.0000e-04\nEpoch 38/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7442 - loss: 1.0376\nEpoch 38: val_accuracy improved from 0.72506 to 0.75468, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.7442 - loss: 1.0376 - val_accuracy: 0.7547 - val_loss: 1.0563 - learning_rate: 1.0000e-04\nEpoch 39/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7453 - loss: 1.0317\nEpoch 39: val_accuracy did not improve from 0.75468\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.7453 - loss: 1.0317 - val_accuracy: 0.7025 - val_loss: 1.1414 - learning_rate: 1.0000e-04\nEpoch 40/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7451 - loss: 1.0258\nEpoch 40: val_accuracy did not improve from 0.75468\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7451 - loss: 1.0258 - val_accuracy: 0.7149 - val_loss: 1.1217 - learning_rate: 1.0000e-04\nEpoch 41/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7530 - loss: 1.0216\nEpoch 41: val_accuracy did not improve from 0.75468\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 114ms/step - accuracy: 0.7530 - loss: 1.0216 - val_accuracy: 0.7352 - val_loss: 1.0882 - learning_rate: 1.0000e-04\nEpoch 42/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7575 - loss: 1.0120\nEpoch 42: val_accuracy did not improve from 0.75468\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7575 - loss: 1.0120 - val_accuracy: 0.7314 - val_loss: 1.0805 - learning_rate: 1.0000e-04\nEpoch 43/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7571 - loss: 1.0114\nEpoch 43: val_accuracy improved from 0.75468 to 0.77442, saving model to best_EfficientNetB4.keras\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.7571 - loss: 1.0114 - val_accuracy: 0.7744 - val_loss: 1.0111 - learning_rate: 1.0000e-04\nEpoch 44/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7653 - loss: 0.9962\nEpoch 44: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7653 - loss: 0.9962 - val_accuracy: 0.7477 - val_loss: 1.0490 - learning_rate: 1.0000e-04\nEpoch 45/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7689 - loss: 0.9916\nEpoch 45: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7689 - loss: 0.9916 - val_accuracy: 0.7325 - val_loss: 1.0909 - learning_rate: 1.0000e-04\nEpoch 46/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7718 - loss: 0.9866\nEpoch 46: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 116ms/step - accuracy: 0.7718 - loss: 0.9866 - val_accuracy: 0.7192 - val_loss: 1.1013 - learning_rate: 1.0000e-04\nEpoch 47/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7668 - loss: 0.9900\nEpoch 47: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 116ms/step - accuracy: 0.7668 - loss: 0.9900 - val_accuracy: 0.7419 - val_loss: 1.0602 - learning_rate: 1.0000e-04\nEpoch 48/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7707 - loss: 0.9826\nEpoch 48: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 114ms/step - accuracy: 0.7707 - loss: 0.9826 - val_accuracy: 0.7144 - val_loss: 1.1057 - learning_rate: 1.0000e-04\nEpoch 49/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7731 - loss: 0.9816\nEpoch 49: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 114ms/step - accuracy: 0.7731 - loss: 0.9816 - val_accuracy: 0.7366 - val_loss: 1.0791 - learning_rate: 1.0000e-04\nEpoch 50/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7719 - loss: 0.9825\nEpoch 50: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 114ms/step - accuracy: 0.7719 - loss: 0.9825 - val_accuracy: 0.7474 - val_loss: 1.0661 - learning_rate: 1.0000e-04\nEpoch 51/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7782 - loss: 0.9683\nEpoch 51: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 115ms/step - accuracy: 0.7782 - loss: 0.9683 - val_accuracy: 0.7600 - val_loss: 1.0261 - learning_rate: 1.0000e-04\nEpoch 52/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7808 - loss: 0.9658\nEpoch 52: val_accuracy did not improve from 0.77442\n\nEpoch 52: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 117ms/step - accuracy: 0.7808 - loss: 0.9658 - val_accuracy: 0.7561 - val_loss: 1.0452 - learning_rate: 1.0000e-04\nEpoch 53/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7929 - loss: 0.9443\nEpoch 53: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.7929 - loss: 0.9443 - val_accuracy: 0.7435 - val_loss: 1.0663 - learning_rate: 2.0000e-05\nEpoch 54/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8017 - loss: 0.9235\nEpoch 54: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8017 - loss: 0.9235 - val_accuracy: 0.7378 - val_loss: 1.0743 - learning_rate: 2.0000e-05\nEpoch 55/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8051 - loss: 0.9224\nEpoch 55: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8051 - loss: 0.9224 - val_accuracy: 0.7447 - val_loss: 1.0580 - learning_rate: 2.0000e-05\nEpoch 56/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8071 - loss: 0.9146\nEpoch 56: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8071 - loss: 0.9146 - val_accuracy: 0.7687 - val_loss: 1.0179 - learning_rate: 2.0000e-05\nEpoch 57/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8082 - loss: 0.9142\nEpoch 57: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8082 - loss: 0.9142 - val_accuracy: 0.7612 - val_loss: 1.0333 - learning_rate: 2.0000e-05\nEpoch 58/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8129 - loss: 0.9029\nEpoch 58: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8129 - loss: 0.9029 - val_accuracy: 0.7646 - val_loss: 1.0212 - learning_rate: 2.0000e-05\nEpoch 59/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8133 - loss: 0.8978\nEpoch 59: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8133 - loss: 0.8978 - val_accuracy: 0.7603 - val_loss: 1.0306 - learning_rate: 2.0000e-05\nEpoch 60/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8186 - loss: 0.8949\nEpoch 60: val_accuracy did not improve from 0.77442\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 118ms/step - accuracy: 0.8186 - loss: 0.8949 - val_accuracy: 0.7584 - val_loss: 1.0338 - learning_rate: 2.0000e-05\nEpoch 61/100\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8157 - loss: 0.8932\nEpoch 61: val_accuracy did not improve from 0.77442\n\nEpoch 61: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 118ms/step - accuracy: 0.8157 - loss: 0.8932 - val_accuracy: 0.7574 - val_loss: 1.0411 - learning_rate: 2.0000e-05\nEpoch 61: early stopping\nRestoring model weights from the end of the best epoch: 43.\n\n--- Loading potentially best fine-tuned weights ---\nLoaded best weights for EfficientNetB0\nLoaded best weights for EfficientNetB4\n\n--- Optimizing Ensemble Weights on Validation Set ---\nPredicting on validation set with Model 1...\n\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step\nPredicting on validation set with Model 2...\n\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 70ms/step\nExtracting true labels from validation set...\nSearching for optimal ensemble weights...\n  Weight [M1=0.00, M2=1.00] -> Validation Accuracy: 0.7744\n  Weight [M1=0.05, M2=0.95] -> Validation Accuracy: 0.7768\n  Weight [M1=0.10, M2=0.90] -> Validation Accuracy: 0.7796\n  Weight [M1=0.15, M2=0.85] -> Validation Accuracy: 0.7811\n  Weight [M1=0.20, M2=0.80] -> Validation Accuracy: 0.7824\n  Weight [M1=0.25, M2=0.75] -> Validation Accuracy: 0.7852\n  Weight [M1=0.30, M2=0.70] -> Validation Accuracy: 0.7847\n  Weight [M1=0.35, M2=0.65] -> Validation Accuracy: 0.7857\n  Weight [M1=0.40, M2=0.60] -> Validation Accuracy: 0.7813\n  Weight [M1=0.45, M2=0.55] -> Validation Accuracy: 0.7762\n  Weight [M1=0.50, M2=0.50] -> Validation Accuracy: 0.7722\n  Weight [M1=0.55, M2=0.45] -> Validation Accuracy: 0.7647\n  Weight [M1=0.60, M2=0.40] -> Validation Accuracy: 0.7562\n  Weight [M1=0.65, M2=0.35] -> Validation Accuracy: 0.7456\n  Weight [M1=0.70, M2=0.30] -> Validation Accuracy: 0.7322\n  Weight [M1=0.75, M2=0.25] -> Validation Accuracy: 0.7209\n  Weight [M1=0.80, M2=0.20] -> Validation Accuracy: 0.7067\n  Weight [M1=0.85, M2=0.15] -> Validation Accuracy: 0.6914\n  Weight [M1=0.90, M2=0.10] -> Validation Accuracy: 0.6783\n  Weight [M1=0.95, M2=0.05] -> Validation Accuracy: 0.6638\n  Weight [M1=1.00, M2=0.00] -> Validation Accuracy: 0.6489\nOptimal weights found: [0.35000000000000003, 0.6499999999999999] with Validation Accuracy: 0.7857\n\n--- Building Saveable Ensemble Model (Equal Weights in Saved Object) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"Emotion_Ensemble_B0_B4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Emotion_Ensemble_B0_B4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ ensemble_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ EfficientNetB0            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │      \u001b[38;5;34m4,059,819\u001b[0m │ ensemble_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mFunctional\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ EfficientNetB4            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │     \u001b[38;5;34m17,688,167\u001b[0m │ ensemble_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mFunctional\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cast_6 (\u001b[38;5;33mCast\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ EfficientNetB0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cast_7 (\u001b[38;5;33mCast\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ EfficientNetB4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ ensemble_average          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ cast_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n│ (\u001b[38;5;33mAverage\u001b[0m)                 │                        │                │ cast_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ ensemble_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ EfficientNetB0            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,059,819</span> │ ensemble_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ EfficientNetB4            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">17,688,167</span> │ ensemble_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cast_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ EfficientNetB0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cast_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ EfficientNetB4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ ensemble_average          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cast_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Average</span>)                 │                        │                │ cast_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,747,986\u001b[0m (82.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,747,986</span> (82.96 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m21,747,986\u001b[0m (82.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,747,986</span> (82.96 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Ensemble model object saved successfully to emotion_ensemble_final.keras\n\n=== Evaluating Individual Models ===\n\n--- Evaluating EfficientNetB0 on AffectNet ---\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 75ms/step - accuracy: 0.6584 - loss: 1.2212\nEfficientNetB0 AffectNet Test Loss: 1.1213\nEfficientNetB0 AffectNet Test Accuracy: 0.7124\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step\nEfficientNetB0 AffectNet F1 Score (Weighted): 0.7089\nEfficientNetB0 AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.70      0.55      0.62      3218\n    contempt       0.56      0.88      0.69      2871\n     disgust       0.55      0.78      0.65      2477\n        fear       0.72      0.52      0.60      3176\n       happy       0.87      0.87      0.87      5044\n     neutral       0.82      0.85      0.84      5126\n         sad       0.66      0.52      0.58      3091\n    surprise       0.70      0.60      0.65      4039\n\n    accuracy                           0.71     29042\n   macro avg       0.70      0.70      0.69     29042\nweighted avg       0.72      0.71      0.71     29042\n\nSaved confusion matrix to EfficientNetB0_AffectNet_confusion_matrix.png\n\n--- Evaluating EfficientNetB0 on FER2013 ---\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 81ms/step - accuracy: 0.5691 - loss: 1.3781\nEfficientNetB0 FER2013 Test Loss: 1.3767\nEfficientNetB0 FER2013 Test Accuracy: 0.5722\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step\nEfficientNetB0 FER2013 F1 Score (Weighted): 0.5702\nEfficientNetB0 FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.44      0.50      0.47       958\n    contempt       1.00      0.52      0.68        54\n     disgust       0.53      0.34      0.42       111\n        fear       0.46      0.32      0.38      1024\n       happy       0.79      0.77      0.78      1774\n     neutral       0.55      0.52      0.53      1233\n         sad       0.44      0.53      0.48      1247\n    surprise       0.66      0.73      0.69       831\n\n    accuracy                           0.57      7232\n   macro avg       0.61      0.53      0.55      7232\nweighted avg       0.58      0.57      0.57      7232\n\nSaved confusion matrix to EfficientNetB0_FER2013_confusion_matrix.png\n\n--- Evaluating EfficientNetB4 on AffectNet ---\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 73ms/step - accuracy: 0.7729 - loss: 1.0183\nEfficientNetB4 AffectNet Test Loss: 0.9838\nEfficientNetB4 AffectNet Test Accuracy: 0.7912\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 57ms/step\nEfficientNetB4 AffectNet F1 Score (Weighted): 0.7912\nEfficientNetB4 AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.79      0.69      0.74      3218\n    contempt       0.68      0.85      0.75      2871\n     disgust       0.68      0.80      0.73      2477\n        fear       0.84      0.63      0.72      3176\n       happy       0.94      0.88      0.91      5044\n     neutral       0.87      0.90      0.88      5126\n         sad       0.72      0.70      0.71      3091\n    surprise       0.73      0.77      0.75      4039\n\n    accuracy                           0.79     29042\n   macro avg       0.78      0.78      0.77     29042\nweighted avg       0.80      0.79      0.79     29042\n\nSaved confusion matrix to EfficientNetB4_AffectNet_confusion_matrix.png\n\n--- Evaluating EfficientNetB4 on FER2013 ---\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 72ms/step - accuracy: 0.5541 - loss: 1.3965\nEfficientNetB4 FER2013 Test Loss: 1.4406\nEfficientNetB4 FER2013 Test Accuracy: 0.5400\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 57ms/step\nEfficientNetB4 FER2013 F1 Score (Weighted): 0.5400\nEfficientNetB4 FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.47      0.40      0.43       958\n    contempt       1.00      0.52      0.68        54\n     disgust       0.45      0.33      0.38       111\n        fear       0.42      0.30      0.35      1024\n       happy       0.80      0.71      0.75      1774\n     neutral       0.53      0.44      0.48      1233\n         sad       0.37      0.58      0.45      1247\n    surprise       0.62      0.74      0.68       831\n\n    accuracy                           0.54      7232\n   macro avg       0.58      0.50      0.53      7232\nweighted avg       0.56      0.54      0.54      7232\n\nSaved confusion matrix to EfficientNetB4_FER2013_confusion_matrix.png\n\n=== Evaluating Ensemble (Using Optimized Weights) ===\n\n--- Evaluating Ensemble on AffectNet using Weights: [0.35000000000000003, 0.6499999999999999] ---\nPredicting with model 1: EfficientNetB0\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step\nPredicting with model 2: EfficientNetB4\n\u001b[1m908/908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 57ms/step\nEnsemble AffectNet Test Accuracy: 0.8090\nEnsemble AffectNet F1 Score (Weighted): 0.8083\nEnsemble AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.82      0.70      0.75      3218\n    contempt       0.68      0.89      0.77      2871\n     disgust       0.68      0.84      0.75      2477\n        fear       0.86      0.63      0.73      3176\n       happy       0.95      0.91      0.93      5044\n     neutral       0.88      0.92      0.90      5126\n         sad       0.77      0.70      0.73      3091\n    surprise       0.76      0.78      0.77      4039\n\n    accuracy                           0.81     29042\n   macro avg       0.80      0.80      0.79     29042\nweighted avg       0.82      0.81      0.81     29042\n\nSaved ensemble confusion matrix to ensemble_AffectNet_confusion_matrix_w0.35_0.65.png\n\n--- Evaluating Ensemble on FER2013 using Weights: [0.35000000000000003, 0.6499999999999999] ---\nPredicting with model 1: EfficientNetB0\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step\nPredicting with model 2: EfficientNetB4\n\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 57ms/step\nEnsemble FER2013 Test Accuracy: 0.5784\nEnsemble FER2013 F1 Score (Weighted): 0.5761\nEnsemble FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.49      0.45      0.47       958\n    contempt       1.00      0.56      0.71        54\n     disgust       0.57      0.34      0.43       111\n        fear       0.47      0.31      0.38      1024\n       happy       0.81      0.77      0.79      1774\n     neutral       0.56      0.49      0.52      1233\n         sad       0.41      0.61      0.49      1247\n    surprise       0.66      0.76      0.71       831\n\n    accuracy                           0.58      7232\n   macro avg       0.62      0.54      0.56      7232\nweighted avg       0.59      0.58      0.58      7232\n\nSaved ensemble confusion matrix to ensemble_FER2013_confusion_matrix_w0.35_0.65.png\n\n=== FINAL RESULTS ===\n(Using Optimized Ensemble Weights: [0.35000000000000003, 0.6499999999999999])\n\n--- Individual Model Performance ---\nEfficientNetB0 AffectNet Test Accuracy: 0.7124, F1: 0.7089\nEfficientNetB0 FER2013 Test Accuracy: 0.5722, F1: 0.5702\nEfficientNetB4 AffectNet Test Accuracy: 0.7912, F1: 0.7912\nEfficientNetB4 FER2013 Test Accuracy: 0.5400, F1: 0.5400\n\n--- Ensemble Performance ---\nEnsemble AffectNet Test Accuracy: 0.8090\nEnsemble AffectNet F1 Score: 0.8083\nEnsemble FER2013 Test Accuracy: 0.5784\nEnsemble FER2013 F1 Score: 0.5761\n\nTraining and evaluation complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.2 (Handling specific Test directory structure)\n#=== FINAL RESULTS ===\n\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.6136, F1: 0.6056\n#EfficientNetB0 FER2013 Test Accuracy: 0.5346, F1: 0.5247\n#EfficientNetB4 AffectNet Test Accuracy: 0.8172, F1: 0.8172\n#EfficientNetB4 FER2013 Test Accuracy: 0.5524, F1: 0.5543\n\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.7515\n#Ensemble AffectNet F1 Score: 0.7509\n#Ensemble FER2013 Test Accuracy: 0.5780\n#Ensemble FER2013 F1 Score: 0.5721\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\nimport math\nimport glob # Needed for finding files\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path\n    \"TRAIN_DIR\": \"Train\", # Combined train dir name\n    \"TEST_DIR\": \"Test\",   # Base test dir name\n    \"IMG_SIZE\": 112, # 112= accuracy 0.56 / 224= accuracy 0.49 / 380=x / 336=x (160px, 192px, or 256px)\n    \"BATCH_SIZE\": 32, # 64+ / 128x / 32\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE,\n    \"EPOCHS_HEAD\": 50, # 15\n    \"EPOCHS_FINE_TUNE\": 100, # 30 60+\n    \"LR_HEAD\": 1e-3,\n    \"LR_FINE_TUNE_START\": 1e-4,\n    \"DROPOUT_RATE\": 0.4,\n    \"NUM_CLASSES\": 8,\n    \"MODEL_ARCH_1\": \"EfficientNetB0\",\n    \"MODEL_ARCH_2\": \"EfficientNetB4\",\n    \"ENSEMBLE_WEIGHTS\": [0.3, 0.7], # 50/50 80/20 -- 70/30\n    \"FINE_TUNE_LAYERS_B0\": 15, # 10\n    \"FINE_TUNE_LAYERS_B4\": 20, # 15\n    \"LOG_DIR_BASE\": \"logs/fit/\"\n}\n\n# Set Seed\ntf.random.set_seed(CONFIG[\"SEED\"])\nnp.random.seed(CONFIG[\"SEED\"])\n\n# =============================================================================\n# GPU Configuration & Mixed Precision (Same as before)\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\nelse:\n    print(\"No GPU detected. Running on CPU.\")\n\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\nprint(\"Mixed precision enabled ('mixed_float16')\")\n\n# =============================================================================\n# Data Loading (Train/Validation - Standard; Test - Custom)\n# =============================================================================\n\ndef create_train_val_tf_dataset(directory, image_size, batch_size, validation_split=0.2):\n    \"\"\"Creates tf.data.Dataset for training and validation using image_dataset_from_directory.\"\"\"\n    print(f\"Loading training/validation data from: {directory}\")\n\n    # Create the training dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=True,\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"training\",\n    )\n\n    # Create the validation dataset\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=False, # No need to shuffle validation\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"validation\",\n    )\n\n    # Get class names from the training dataset BEFORE optimization\n    class_names = train_ds.class_names\n    print(f\"Dataset loaded with classes: {class_names}\")\n\n    # Configure performance for both datasets\n    train_ds = train_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n    val_ds = val_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return train_ds, val_ds, class_names\n\n# --- NEW: Function to load test data from the specific structure ---\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size):\n    \"\"\"\n    Creates a tf.data.Dataset for testing by manually finding files in the Test/<emotion>/<target_dataset> structure.\n    \"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\")\n    all_image_paths = []\n    all_labels = []\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    if not emotion_dirs:\n         print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\")\n         return None\n\n    print(f\"Found emotion folders: {emotion_dirs}\")\n\n    for emotion in emotion_dirs:\n        if emotion not in class_names_map:\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\")\n            continue\n\n        label_index = class_names_map[emotion] # Get the integer label\n        target_path = os.path.join(base_test_dir, emotion, target_dataset)\n\n        if not tf.io.gfile.exists(target_path):\n             print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\")\n             continue # Skip if the specific dataset subdir doesn't exist for this emotion\n\n        # Find all image files (adjust extensions if needed)\n        image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpeg'))\n\n        if not image_files:\n             print(f\"Warning: No image files found in '{target_path}'.\")\n             continue\n\n        all_image_paths.extend(image_files)\n        all_labels.extend([label_index] * len(image_files))\n        print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\")\n\n    if not all_image_paths:\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\")\n        return None\n\n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\")\n\n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=CONFIG[\"NUM_CLASSES\"])) # Convert labels to one-hot\n    image_ds = path_ds.map(lambda x: load_and_preprocess_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Combine images and labels\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n\n    # Batch and prefetch\n    test_ds = image_label_ds.batch(batch_size).prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return test_ds\n\n# --- NEW: Helper function to load and preprocess images from paths ---\ndef load_and_preprocess_image(path, image_size):\n    \"\"\"Loads and preprocesses a single image file.\"\"\"\n    image = tf.io.read_file(path)\n    image = tf.image.decode_image(image, channels=3, expand_animations=False) # Decode any format\n    image = tf.image.resize(image, [image_size, image_size], method='nearest')\n    image.set_shape((image_size, image_size, 3))\n    # Rescaling: EfficientNet generally handles this internally, but if needed:\n    # image = image / 255.0\n    return image\n\n# --- Create Datasets ---\n# Training and Validation Data\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"])\ntrain_ds, val_ds, class_names = create_train_val_tf_dataset(\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n)\n\n# Create a mapping from class name to integer index for test set loading\nclass_names_map = {name: i for i, name in enumerate(class_names)}\n\n# Test Datasets (using the new custom function)\nbase_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"])\naffectnet_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\nfer2013_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\n\n\n# =============================================================================\n# Class Weights Calculation (Same as V3.1, check if needs adjustment for 8 classes)\n# =============================================================================\ndef get_class_weights(dataset, class_names_list):\n    print(\"Calculating class weights...\")\n    all_labels = []\n    num_batches = tf.data.experimental.cardinality(dataset)\n    print(f\"Approximate number of batches in training dataset: {num_batches}\")\n\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY:\n         print(\"Warning: Cannot determine dataset cardinality accurately. Iterating...\")\n         for _, labels_batch in dataset: # Iterate batches\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n         if not all_labels:\n             print(\"Error: Could not extract labels. Using uniform weights.\")\n             return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    else:\n        for _, labels_batch in dataset:\n            all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n\n    unique_classes, counts = np.unique(all_labels, return_counts=True)\n    print(f\"Unique labels found for weight calculation: {unique_classes} with counts {counts}\")\n\n    if len(unique_classes) == 0:\n        print(\"Error: No labels found. Using uniform weights.\")\n        return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=unique_classes,\n        y=all_labels\n    )\n\n    class_weights_dict = {i: 0.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    for i, cls_label in enumerate(unique_classes):\n        if cls_label < CONFIG[\"NUM_CLASSES\"]:\n             class_weights_dict[cls_label] = class_weights[i]\n        else:\n             print(f\"Warning: Label {cls_label} >= NUM_CLASSES ({CONFIG['NUM_CLASSES']}). Ignoring.\")\n\n    for i in range(CONFIG[\"NUM_CLASSES\"]):\n        if class_weights_dict[i] == 0.0 and i in unique_classes: # Check if it was actually missing or just had 0 weight\n            print(f\"Warning: Class {i} ({class_names_list[i]}) had 0 weight initially. Assigning 1.0.\")\n            class_weights_dict[i] = 1.0 # Avoid 0 weight if class exists but wasn't found / calculated\n        elif class_weights_dict[i] == 0.0:\n             print(f\"Info: Class {i} ({class_names_list[i]}) not found in iterated samples. Assigning weight 1.0.\")\n             class_weights_dict[i] = 1.0\n\n\n    print(\"Class weights calculated:\", class_weights_dict)\n    return class_weights_dict\n\nclass_weights = get_class_weights(train_ds, class_names)\n\n# =============================================================================\n# Data Augmentation Layer (Same as before)\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]),\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building (Same as before - ensure NUM_CLASSES is correct)\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate):\n    input_shape = (img_size, img_size, 3)\n    inputs = Input(shape=input_shape, name=\"input_layer\")\n    x = data_augmentation(inputs) # Apply augmentation first\n\n    if model_arch == \"EfficientNetB0\":\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # Let EffNet handle input scaling\n        # Apply augmentation *after* potential base model preprocessing if needed, but usually before is fine.\n        x_processed = base_model(x, training=False) # Pass augmented data to base model\n    elif model_arch == \"EfficientNetB4\":\n         print(f\"Warning: Building {model_arch} with image size {img_size}.\")\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg')\n         x_processed = base_model(x, training=False) # Pass augmented data to base model\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\")\n\n    base_model.trainable = False # Freeze base model\n\n    # Add classification head\n    output = Dropout(dropout_rate, name=\"top_dropout\")(x_processed) # Use output from base model\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(output)\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch)\n    print(f\"{model_arch} model built successfully.\")\n    return model\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\n\n# =============================================================================\n# Training Functions (Mostly same as V3.1, adjust fine-tune layer freezing logic)\n# =============================================================================\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0):\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name\n    checkpoint_path = f\"best_{model.name}.keras\"\n\n    # --- Callbacks ---\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=1)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=9, min_lr=1e-6, verbose=1) # patience=3\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=18, restore_best_weights=True, verbose=1) # patience=7\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback]\n    optimizer_choice = tf.keras.optimizers.Adam\n\n    # Find the base model layer to control its trainability\n    base_model_layer = None\n    for layer in model.layers:\n        if layer.name.startswith(\"efficientnet\"): # Find the actual base model layer by name\n            base_model_layer = layer\n            break\n    if base_model_layer is None and fine_tune:\n        print(\"Warning: Could not automatically find base model layer for fine-tuning.\")\n\n    # --- Compile Step ---\n    if fine_tune:\n        print(f\"Setting up fine-tuning for {model.name}...\")\n        if base_model_layer:\n            base_model_layer.trainable = True # Unfreeze the base model layer\n            # Fine-tune only the top 'fine_tune_layers' layers within the base model\n            num_base_layers = len(base_model_layer.layers)\n            freeze_until = num_base_layers - fine_tune_layers\n            print(f\"Unfreezing top {fine_tune_layers} layers of {base_model_layer.name} (out of {num_base_layers}). Freezing up to layer {freeze_until}.\")\n            if freeze_until < 0: freeze_until = 0\n\n            for layer in base_model_layer.layers[:freeze_until]:\n                 # Keep Batch Norm layers frozen, as is often recommended during fine-tuning\n                 # to prevent destabilization from small batch statistics.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                      layer.trainable = False\n                 else:\n                      # You might choose to freeze all layers up to this point\n                      layer.trainable = False\n                      pass # Or set layer.trainable = False if you want to be strict\n\n            for layer in base_model_layer.layers[freeze_until:]:\n                 # Keep Batch Norm frozen here too for consistency? Experiment needed.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                     layer.trainable = False\n                 else:\n                     layer.trainable = True # Unfreeze the top layers\n        else: # Fallback if base model layer not found\n             model.trainable = True # Unfreeze everything if base layer not identified\n\n        learning_rate_schedule = learning_rate # Use starting LR for fine-tune\n        callbacks.append(reduce_lr)\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n    else: # Head training\n        print(f\"Setting up head training for {model.name}.\")\n        if base_model_layer:\n            base_model_layer.trainable = False # Ensure base is frozen\n        else:\n             print(\"Warning: Could not find base model layer to freeze for head training.\")\n\n        # Cosine Decay for head training\n        train_cardinality = tf.data.experimental.cardinality(train_dataset)\n        if train_cardinality == tf.data.experimental.UNKNOWN_CARDINALITY:\n            print(\"Warning: Unknown training steps for CosineDecay. Estimating.\")\n            try: # Estimate steps\n                 steps_per_epoch = len(train_ds.list_files(os.path.join(train_dir,'*/*'))) // CONFIG[\"BATCH_SIZE\"]\n            except: steps_per_epoch = 1000 # Fallback\n            total_steps = steps_per_epoch * epochs\n        else:\n            total_steps = train_cardinality.numpy() * epochs\n\n        warmup_steps = int(total_steps * 0.1)\n        learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay(\n             initial_learning_rate=learning_rate, decay_steps=max(1, total_steps - warmup_steps), alpha=0.0\n        )\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n        # callbacks.append(reduce_lr) # Optionally add ReduceLR here too\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    print(\"\\n--- Model Summary After Compilation ---\")\n    model.summary(line_length=120) # Print summary after compilation and setting trainability\n\n    print(f\"\\n--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} (Epochs {initial_epoch} to {initial_epoch+epochs}) ---\")\n    history = model.fit(\n        train_dataset,\n        epochs=initial_epoch + epochs, # End epoch\n        validation_data=validation_dataset,\n        class_weight=class_weights_dict,\n        callbacks=callbacks,\n        initial_epoch=initial_epoch, # Start epoch\n        verbose=1\n    )\n    return history, model\n\n\n# =============================================================================\n# Evaluation Function (Adapting to potentially missing classes in test sets)\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating {model.name} on {dataset_name} ---\")\n    results = model.evaluate(test_dataset, verbose=1)\n    print(f\"{model.name} {dataset_name} Test Loss: {results[0]:.4f}\")\n    print(f\"{model.name} {dataset_name} Test Accuracy: {results[1]:.4f}\")\n\n    y_pred_probs = model.predict(test_dataset)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in evaluation data.\")\n         return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": 0}\n\n\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"{model.name} {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"{model.name} {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'{model.name} {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"{model.name}_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1}\n\n# =============================================================================\n# Main Training Pipeline (Adjusted epoch handling)\n# =============================================================================\n\nprint(f\"\\nClass names being used: {class_names}\")\nprint(f\"Number of classes for models: {CONFIG['NUM_CLASSES']}\")\nif len(class_names) != CONFIG['NUM_CLASSES']:\n     print(f\"Warning: Number of classes found ({len(class_names)}) differs from CONFIG['NUM_CLASSES'] ({CONFIG['NUM_CLASSES']})\")\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\")\nhistory1_head, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\")\nhistory2_head, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\n# Load best weights from head training before fine-tuning\nprint(\"\\n--- Loading best weights after head training ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best head weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model1.name} - {e}.\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best head weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model2.name} - {e}.\")\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\")\n# Start fine-tuning epochs from 0 for this phase, but total epochs define the duration\nhistory1_ft, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\")\nhistory2_ft, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\n# Load best weights saved during the entire process (should be the fine-tuned ones if they improved)\nprint(\"\\n--- Loading potentially best fine-tuned weights ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model1.name} - {e}\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model2.name} - {e}\")\n\n# =============================================================================\n# Build and Save the Ensemble Model Object\n# =============================================================================\nprint(\"\\n--- Building Saveable Ensemble Model ---\")\n\n# Ensure individual models are loaded with best weights (already done previously)\n# model1 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_1']}.keras\", compile=False) # Example if re-loading needed\n# model2 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_2']}.keras\", compile=False)\n\n# Ensure base models within the loaded models are not trainable for the ensemble definition\n# (This prevents accidental further training if the ensemble object were compiled later)\n# Set the loaded models themselves as non-trainable before combining\nmodel1.trainable = False\nmodel2.trainable = False\n\n# Define the input layer (must match the input shape of individual models)\ninput_shape = (CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], 3)\nensemble_input = tf.keras.layers.Input(shape=input_shape, name=\"ensemble_input\")\n\n# Get the outputs from the individual models for the shared input\noutput1 = model1(ensemble_input)\noutput2 = model2(ensemble_input)\n\n# Combine the outputs using Average (simple average)\n# If you want weighted average, you might need a custom layer or use tf.tensordot outside the model definition\n# Or, if using TF >= 2.11 (approx), check for tf.keras.layers.WeightedAverage\nensemble_output = tf.keras.layers.Average(name=\"ensemble_average\")([output1, output2])\n\n# Create the ensemble model\nensemble_model = tf.keras.Model(inputs=ensemble_input, outputs=ensemble_output, name=\"Emotion_Ensemble_B0_B4\")\n\nensemble_model.summary()\n\n# Save the ensemble model\nensemble_model_path = \"emotion_ensemble_final.keras\"\nensemble_model.save(ensemble_model_path)\nprint(f\"Ensemble model object saved successfully to {ensemble_model_path}\")\n\n# Optional: You can now use 'ensemble_model' directly for predictions if needed immediately\n# Example: ensemble_model.predict(some_test_data)\n\n# =============================================================================\n# Individual Model Evaluation (Optional but recommended)\n# =============================================================================\nprint(\"\\n=== Evaluating Individual Models ===\")\nmetrics_m1_affect = evaluate_model(model1, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m1_fer = evaluate_model(model1, fer2013_test_ds, class_names, \"FER2013\")\nmetrics_m2_affect = evaluate_model(model2, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m2_fer = evaluate_model(model2, fer2013_test_ds, class_names, \"FER2013\")\n\n# =============================================================================\n# Ensemble Prediction and Evaluation (Same as V3.1, ensure test datasets loaded)\n# =============================================================================\nmodels_to_ensemble = [model1, model2]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping ensemble evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} ---\")\n    all_preds_probs = []\n    for i, model in enumerate(models):\n        print(f\"Predicting with model {i+1}: {model.name}\")\n        preds = model.predict(test_dataset)\n        all_preds_probs.append(preds)\n\n    # Weighted average of probabilities\n    weighted_preds_probs = np.tensordot(weights, all_preds_probs, axes=([0],[0]))\n    y_pred = np.argmax(weighted_preds_probs, axis=1)\n\n    # Extract true labels\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in ensemble evaluation data.\")\n         return {\"accuracy\": 0, \"f1_score\": 0}\n\n    accuracy = np.mean(y_true == y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\")\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"Ensemble {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1}\n\n# --- Evaluate Ensemble ---\nprint(\"\\n=== Evaluating Ensemble ===\")\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], affectnet_test_ds, class_names, \"AffectNet\")\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], fer2013_test_ds, class_names, \"FER2013\")\n\nprint(\"\\n=== FINAL RESULTS ===\")\nprint(\"\\n--- Individual Model Performance ---\")\nif metrics_m1_affect: print(f\"{model1.name} AffectNet Test Accuracy: {metrics_m1_affect['accuracy']:.4f}, F1: {metrics_m1_affect['f1_score']:.4f}\")\nif metrics_m1_fer: print(f\"{model1.name} FER2013 Test Accuracy: {metrics_m1_fer['accuracy']:.4f}, F1: {metrics_m1_fer['f1_score']:.4f}\")\nif metrics_m2_affect: print(f\"{model2.name} AffectNet Test Accuracy: {metrics_m2_affect['accuracy']:.4f}, F1: {metrics_m2_affect['f1_score']:.4f}\")\nif metrics_m2_fer: print(f\"{model2.name} FER2013 Test Accuracy: {metrics_m2_fer['accuracy']:.4f}, F1: {metrics_m2_fer['f1_score']:.4f}\")\n\nprint(\"\\n--- Ensemble Performance ---\")\nif affectnet_metrics_ens:\n    print(f\"Ensemble AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\")\nif fer_metrics_ens:\n    print(f\"Ensemble FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\")\n\nprint(\"\\nTraining and evaluation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:02:58.286865Z","iopub.execute_input":"2025-04-05T14:02:58.287181Z","iopub.status.idle":"2025-04-05T18:23:25.138544Z","shell.execute_reply.started":"2025-04-05T14:02:58.287156Z","shell.execute_reply":"2025-04-05T18:23:25.137442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My code 2.5 (deepcloud) best one yet\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters (Adjusted for memory optimization and stability)\n# =============================================================================\nBATCH_SIZE = 64  # Maintain balance between memory and throughput\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./weights\")\n\n# Extend problematic classes for more targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust', 'anger', 'fear']\n\n# =============================================================================\n# Enhanced Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Enhanced Image Preprocessing with Stronger Augmentation for Hard Classes\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with stronger augmentation for difficult classes.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply enhanced augmentation during training\n    if training:\n        # Basic augmentations for all images\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced brightness and contrast adjustment\n        img = tf.image.random_brightness(img, 0.3)  # Increased from 0.2\n        img = tf.image.random_contrast(img, 0.7, 1.3)  # Wider range\n        \n        # Add random rotation (±15 degrees)\n        angle = tf.random.uniform([], -0.261799, 0.261799)  # ±15 degrees in radians\n        img = tf.image.rot90(img, k=tf.cast(angle / (np.pi/2) * 4, tf.int32))\n        \n        # Random zoom\n        zoom_factor = tf.random.uniform([], 0.8, 1.0)\n        h, w = tf.shape(img)[0], tf.shape(img)[1]\n        crop_size_h = tf.cast(tf.cast(h, tf.float32) * zoom_factor, tf.int32)\n        crop_size_w = tf.cast(tf.cast(w, tf.float32) * zoom_factor, tf.int32)\n        img = tf.image.random_crop(img, [crop_size_h, crop_size_w, 3])\n        img = tf.image.resize(img, [224, 224])\n        \n        # Add more aggressive noise\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.015)  # Increased from 0.01\n        img = img + noise\n        \n        # Ensure valid range\n        img = tf.clip_by_value(img, 0.0, 255.0)\n    \n    # Basic normalization to [0,1] range for consistency\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with proper repeat mechanism\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=False):\n    \"\"\"Memory-optimized dataset creation with proper repeat mechanism\"\"\"\n    if dataset_type:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n    \n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Memory optimization: Only cache validation/test datasets\n    if cache and not is_training:\n        cache_name = f'./cache/{dataset_type}_cache' if dataset_type else './cache/combined_cache'\n        ds = ds.cache(cache_name)\n    \n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Better shuffling with larger buffer\n        buffer_size = min(len(dataframe), 20000)  # Increased from 10000\n        ds = ds.shuffle(buffer_size=buffer_size)\n        # IMPORTANT: Create an infinite dataset by repeating\n        ds = ds.repeat()\n    \n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create more aggressively balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a more aggressively balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        \n        # Base sampling - reduced for non-problematic classes\n        samples_per_class = 350  \n        \n        # Significantly increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            # 100% more samples for problematic classes (from 400 to 700)\n            samples_per_class = 700  \n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with properly working repeat\n    return create_dataset(balanced_df, is_training=is_training, cache=False)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 20  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n       \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n        # Add memory cleanup\n        del images, labels, batch_preds\n        gc.collect()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble with model-specific preprocessing layers.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # ==================== MobileNetV2 Branch ====================\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    # Create MobileNetV2 as standalone model\n    try:\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n    except Exception as e:\n        print(f\"MobileNetV2 imagenet weights failed to load: {e}\")\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights=None,\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n        mobilenet_core.load_weights('weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n    \n    mobilenet_features = mobilenet_core(mobilenet_preprocess)\n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_features)\n\n    # ==================== Xception Branch ====================\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    # Create Xception as standalone model\n    try:\n        xception_core = Xception(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n    except Exception as e:\n        print(f\"Xception imagenet weights failed to load: {e}\")\n        xception_core = Xception(\n            include_top=False,\n            weights=None,\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n        xception_core.load_weights('weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n    \n    xception_features = xception_core(xception_preprocess)\n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_features)\n\n    # ==================== EfficientNetB0 Branch ====================\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    # Create EfficientNetB0 as standalone model\n    try:\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n    except Exception as e:\n        print(f\"EfficientNetB0 imagenet weights failed to load: {e}\")\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights=None,\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n        try:\n            efficientnet_core.load_weights('/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5')\n        except Exception as e:\n            print(f\"Failed to load local EfficientNetB0 weights: {e}\")\n            print(\"Continuing with random initialization for EfficientNetB0\")\n    \n    efficientnet_features = efficientnet_core(efficientnet_preprocess)\n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_features)\n\n    # ==================== Feature Processing ====================\n    def create_projection_head(inputs, name):\n        x = Dense(128, name=f'{name}_projection')(inputs)\n        x = BatchNormalization()(x)\n        return Activation('relu', dtype='float32')(x)\n    \n    mobilenet_features = create_projection_head(mobilenet_features, 'mobilenet')\n    xception_features = create_projection_head(xception_features, 'xception')\n    efficientnet_features = create_projection_head(efficientnet_features, 'efficientnet')\n\n    # ==================== Feature Fusion ====================\n    merged_features = Concatenate(name='feature_fusion')([\n        mobilenet_features,\n        xception_features,\n        efficientnet_features\n    ])\n\n    # ==================== Classification Head ====================\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n\n    # ==================== Model Assembly ====================\n    model = keras.Model(\n        inputs=inputs,\n        outputs=outputs,\n        name='emotion_ensemble'\n    )\n    \n    # Freeze base models if requested\n    if freeze_base:\n        mobilenet_core.trainable = False\n        xception_core.trainable = False\n        efficientnet_core.trainable = False\n\n    # ==================== Compilation ====================\n    # Use a gentler initial learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n        keras.optimizers.Adam(\n            tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=5e-4,  # Reduced from 1e-3\n                decay_steps=15000\n            )\n        )\n    )\n    \n    # Add gradient clipping to avoid instability\n    optimizer.clipnorm = 1.0\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),  # Increased from 2.0 for better focus on hard examples\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# FIXED: Progressive Training Strategy for Ensemble with Corrected Layer Names\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (15% of total epochs)\n    stage1_epochs = max(5, int(total_epochs * 0.15))  # Increased from 10% to 15%\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    mobilenet_core = model.get_layer('mobilenet_core')\n    xception_core = model.get_layer('xception_core')\n    efficientnet_core = model.get_layer('efficientnet_core')\n    \n    # Explicitly freeze all base models\n    mobilenet_core.trainable = False\n    xception_core.trainable = False\n    efficientnet_core.trainable = False\n    \n    # Recompile with stable initial learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (35% of total epochs)\n    stage2_epochs = max(7, int(total_epochs * 0.35))  # Increased from 30% to 35%\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Get reference to actual base models (FIXED: using correct layer names)\n    mobilenet = model.get_layer('mobilenet_core')\n    efficientnet = model.get_layer('efficientnet_core')\n    \n    # Partially unfreeze base models (last 30 layers of each)\n    for base_model in [mobilenet, efficientnet]:\n        base_model.trainable = True\n        # Freeze early layers, unfreeze later layers\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 30)\n    \n    # Keep Xception frozen\n    xception_core.trainable = False\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=8e-5)  # Gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze Xception (FIXED: using correct layer name)\n    xception = model.get_layer('xception_core')\n    xception.trainable = True\n    \n    # Partially unfreeze layers (last 50 layers of each model)\n    for base_model in [mobilenet, efficientnet, xception]:\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 50)\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-5)  # Even gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n\n    K.clear_session()\n    gc.collect()\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # Create datasets with memory optimizations\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True\n    )\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True\n    )\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n\n    # Add periodic garbage collection during evaluation\n    def evaluate_with_gc(model, test_ds, steps, class_names, log_dir, dataset_name):\n        metrics = evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name)\n        K.clear_session()\n        gc.collect()\n        return metrics\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with more aggressive adjustments\n    print(\"\\n4. Computing class weights with more aggressive adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes more aggressively\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 50% (up from 20%)\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.5\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.5\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping with more patience\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,  # Increased from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate monitoring\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        ),\n        # Learning rate reduction on plateau\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            verbose=1,\n            min_lr=1e-7\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.h5\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.h5'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_with_gc(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_with_gc(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_with_gc(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.h5\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.h5'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Add memory monitoring\n    print(\"Initial RAM usage:\", psutil.virtual_memory().percent)\n    model, metrics = train_emotion_ensemble(data_dir)\n    print(\"Final RAM usage:\", psutil.virtual_memory().percent)\n      \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fer_dirs = glob.glob(os.path.join(train_dir, \"*\", \"fer2013\"))\naff_dirs = glob.glob(os.path.join(train_dir, \"*\", \"affectnet\"))\nprint(\"FER directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"fer2013\")))\nprint(\"AffectNet directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"affectnet\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
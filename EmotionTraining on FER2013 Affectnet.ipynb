{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":10537282,"sourceType":"datasetVersion","datasetId":6519896},{"sourceId":10634054,"sourceType":"datasetVersion","datasetId":6583945}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ResNet18 + MobileNetV2 + EfficientNetB0\n# folcalloss is good for fer2013\n\n!pip install tensorflow\n!pip install numpy\n!pip install matplotlib\nimport tensorflow as tf # print(tf.__version__) 2.17.1\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:55:40.279387Z","iopub.execute_input":"2025-03-11T10:55:40.279681Z","iopub.status.idle":"2025-03-11T10:56:12.360576Z","shell.execute_reply.started":"2025-03-11T10:55:40.279647Z","shell.execute_reply":"2025-03-11T10:56:12.359424Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nMulti-Task Facial Emotion Recognition with Implicit Stress Mapping\n--------------------------------------------------------------------\nThis system employs multi-task learning to enrich emotion recognition by simultaneously\npredicting emotion category, intensity, valence, arousal, and emotion group. This\nmulti-dimensional representation is intended to provide a robust foundation for later\nstress assessment without needing explicit stress-level labels.\n\nKey improvements:\n1. A shared base (MobileNetV2 with alpha=0.75) extracts features.\n2. Five task-specific branches are added.\n3. Advanced data augmentation is applied using both manual and Keras preprocessing layers.\n4. Progressive unfreezing training strategy with staged learning rate reduction.\n5. Custom loss functions (label smoothing for classification, uncertainty-aware MSE for regression).\n6. Enhanced monitoring callback for multi-task outputs.\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import MobileNetV2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nrotation_layer = tf.keras.layers.RandomRotation(factor=0.1)\n\n\n# ---------------------------------------------------------------------------------\n# Configuration & Directories\n# ---------------------------------------------------------------------------------\nIMG_SIZE = 112\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    \"\"\"Ensure a directory exists.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create log directories\nfor sub in [LOG_DIR, os.path.join(LOG_DIR, 'affectnet'), os.path.join(LOG_DIR, 'fer2013'),\n            os.path.join(LOG_DIR, 'combined'), \"./model_checkpoints\"]:\n    ensure_dir(sub)\n\n# ---------------------------------------------------------------------------------\n# Emotion Metadata (Psychological & Group Mapping)\n# ---------------------------------------------------------------------------------\nEMOTION_VA_MAPPING = {\n    'angry': (-0.7, 0.7),    # Negative valence, high arousal\n    'disgust': (-0.6, 0.2),  \n    'fear': (-0.6, 0.6),     \n    'happy': (0.8, 0.4),     \n    'neutral': (0.0, 0.0),   \n    'sad': (-0.6, -0.3),     \n    'surprise': (0.4, 0.7),  \n    'contempt': (-0.4, 0.0)\n}\n\nEMOTION_INTENSITY_MAPPING = {\n    'angry': 0.8,\n    'disgust': 0.7,\n    'fear': 0.8,\n    'happy': 0.7,\n    'neutral': 0.1,\n    'sad': 0.5,\n    'surprise': 0.9,\n    'contempt': 0.5\n}\n\nEMOTION_GROUP_MAPPING = {\n    'angry': 0,      # Negative\n    'disgust': 0,    \n    'fear': 0,       \n    'happy': 1,      # Positive\n    'neutral': 2,    # Neutral\n    'sad': 0,        \n    'surprise': 1,   \n    'contempt': 0    \n}\nNUM_EMOTION_GROUPS = 3\n\n# Emphasize problematic classes for augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# ---------------------------------------------------------------------------------\n# Custom Loss Functions\n# ---------------------------------------------------------------------------------\ndef label_smoothing_loss(epsilon=0.1):\n    \"\"\"Categorical crossentropy loss with label smoothing.\"\"\"\n    def loss_fn(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        y_true = y_true * (1.0 - epsilon) + (epsilon / num_classes)\n        return -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7), axis=-1)\n    return loss_fn\n\ndef mse_with_uncertainty(y_true, y_pred):\n    \"\"\"Uncertainty-aware mean squared error loss.\"\"\"\n    squared_diff = tf.square(y_true - y_pred)\n    return tf.reduce_mean(squared_diff + 0.05 * tf.exp(-10.0 * squared_diff))\n\n# Create weighted categorical crossentropy loss for emotion classification\ndef create_weighted_categorical_crossentropy(class_distribution, class_indices):\n    \"\"\"Creates a weighted categorical crossentropy loss function.\"\"\"\n    class_distribution = np.array(class_distribution)\n    weights = 1.0 / (class_distribution / np.sum(class_distribution))\n    weights = weights / np.sum(weights) * len(weights)\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            weights[class_idx] *= 1.5  # Increase weight by 50%\n    def loss(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        y_true = y_true * (1.0 - 0.1) + (0.1 / num_classes)\n        loss_value = -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7) * tf.constant(weights, dtype=tf.float32), axis=-1)\n        return loss_value\n    return loss\n\n# ---------------------------------------------------------------------------------\n# Data Pipeline & Preprocessing\n# ---------------------------------------------------------------------------------\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"Build DataFrame with image paths and multi-task labels.\"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                for img_file in img_files:\n                    valence, arousal = EMOTION_VA_MAPPING.get(emotion, (0.0, 0.0))\n                    intensity = EMOTION_INTENSITY_MAPPING.get(emotion, 0.5)\n                    group = EMOTION_GROUP_MAPPING.get(emotion, 2)\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub,\n                        \"valence\": valence,\n                        \"arousal\": arousal,\n                        \"intensity\": intensity,\n                        \"group\": group\n                    })\n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\ndef preprocess_image(file_path, label, source, valence, arousal, intensity, group, training=True):\n    # Read file and decode image\n    img = tf.io.read_file(file_path)\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            return tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    img = decode_image()\n    \n    # Resize and normalize\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    img = tf.cast(img, tf.float32) / 127.5 - 1.0\n    \n    if training:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        # Use the global rotation layer instead of instantiating a new one\n        img = rotation_layer(tf.expand_dims(img, 0))[0]\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        img = tf.image.random_saturation(img, 0.8, 1.2)\n        img = tf.image.random_hue(img, 0.05)\n        img = tf.clip_by_value(img, -1.0, 1.0)\n    \n    emotion_label = tf.one_hot(label, depth=8)\n    group_label = tf.one_hot(group, depth=NUM_EMOTION_GROUPS)\n    valence_label = tf.cast(valence, dtype=tf.float32)\n    arousal_label = tf.cast(arousal, dtype=tf.float32)\n    intensity_label = tf.cast(intensity, dtype=tf.float32)\n    \n    return img, {\n        'emotion_output': emotion_label,\n        'intensity_output': intensity_label,\n        'valence_output': valence_label, \n        'arousal_output': arousal_label,\n        'group_output': group_label\n    }\n\ndef create_dataset(dataframe, is_training=True, dataset_type=None):\n    \"\"\"Creates a tf.data.Dataset with multi-task labels.\"\"\"\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values,\n        dataframe[\"valence\"].values,\n        dataframe[\"arousal\"].values,\n        dataframe[\"intensity\"].values,\n        dataframe[\"group\"].values\n    ))\n    training_flag = tf.constant(is_training)\n    ds = ds.map(lambda path, label, source, val, aro, inten, grp:\n                preprocess_image(path, label, source, val, aro, inten, grp, training=training_flag),\n                num_parallel_calls=AUTOTUNE)\n    if is_training:\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n    ds = ds.repeat().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return ds, class_indices\n\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"Creates a balanced dataset with oversampling for problematic classes.\"\"\"\n    balanced_data = []\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400\n        if class_name in emphasis_classes:\n            samples_per_class = 800\n        if len(class_df) < samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n        balanced_data.append(sampled)\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (emphasis: {emphasis_classes})\")\n    return create_dataset(balanced_df, is_training=is_training)\n\n# ---------------------------------------------------------------------------------\n# Multi-Task Monitoring Callback\n# ---------------------------------------------------------------------------------\nclass MultiTaskMonitorCallback(tf.keras.callbacks.Callback):\n    \"\"\"Enhanced callback to track multi-task metrics during training.\"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(MultiTaskMonitorCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()\n        self.class_metrics_history = {cls: [] for cls in class_names}\n        self.regression_metrics_history = {'intensity_mae': [], 'valence_mae': [], 'arousal_mae': []}\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        val_steps = 30\n        y_true_emotion, y_pred_emotion = [], []\n        intensity_errors, valence_errors, arousal_errors = [], [], []\n        for i, (images, labels_dict) in enumerate(self.validation_data):\n            if i >= val_steps: break\n            predictions = self.model.predict(images, verbose=0)\n            if isinstance(predictions, list):\n                emotion_preds = predictions[0]\n                intensity_preds = predictions[1]\n                valence_preds = predictions[2]\n                arousal_preds = predictions[3]\n            else:\n                emotion_preds = predictions['emotion_output']\n                intensity_preds = predictions['intensity_output']\n                valence_preds = predictions['valence_output']\n                arousal_preds = predictions['arousal_output']\n            y_pred_emotion.append(np.argmax(emotion_preds, axis=1))\n            y_true_emotion.append(np.argmax(labels_dict['emotion_output'].numpy(), axis=1))\n            intensity_errors.append(np.abs(intensity_preds - labels_dict['intensity_output'].numpy()))\n            valence_errors.append(np.abs(valence_preds - labels_dict['valence_output'].numpy()))\n            arousal_errors.append(np.abs(arousal_preds - labels_dict['arousal_output'].numpy()))\n        y_true_emotion = np.concatenate(y_true_emotion)\n        y_pred_emotion = np.concatenate(y_pred_emotion)\n        intensity_errors = np.concatenate([e.flatten() for e in intensity_errors])\n        valence_errors = np.concatenate([e.flatten() for e in valence_errors])\n        arousal_errors = np.concatenate([e.flatten() for e in arousal_errors])\n        cm = confusion_matrix(y_true_emotion, y_pred_emotion)\n        class_accuracies = np.array([cm[i, i] / np.sum(y_true_emotion == i) if np.sum(y_true_emotion == i) > 0 else 0\n                                     for i in range(len(self.class_names))])\n        for i, cls in enumerate(self.class_names):\n            self.class_metrics_history[cls].append(class_accuracies[i])\n        intensity_mae = np.mean(intensity_errors)\n        valence_mae = np.mean(valence_errors)\n        arousal_mae = np.mean(arousal_errors)\n        self.regression_metrics_history['intensity_mae'].append(intensity_mae)\n        self.regression_metrics_history['valence_mae'].append(valence_mae)\n        self.regression_metrics_history['arousal_mae'].append(arousal_mae)\n        zero_pred_classes = [self.class_names[i] for i in range(len(self.class_names)) if np.sum(cm[:, i]) == 0]\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        if (epoch + 1) % self.freq == 0:\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            print(\"\\nEmotion Classification Accuracies:\")\n            for i, cls in enumerate(self.class_names):\n                print(f\"{cls}: {class_accuracies[i]:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0: print()\n            print(\"\\nRegression Metrics:\")\n            print(f\"Intensity MAE: {intensity_mae:.4f}\")\n            print(f\"Valence MAE: {valence_mae:.4f}\")\n            print(f\"Arousal MAE: {arousal_mae:.4f}\\n\")\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=self.class_names, yticklabels=self.class_names)\n            plt.xlabel('Predicted'); plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save plot: {e}\")\n            plt.close()\n\n# ---------------------------------------------------------------------------------\n# Multi-Task Model Architecture\n# ---------------------------------------------------------------------------------\ndef create_multitask_emotion_model(num_classes):\n    \"\"\"Builds and compiles the multi-task emotion recognition model.\"\"\"\n    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n    inputs = keras.layers.Input(shape=input_shape)\n    base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=inputs, alpha=0.75)\n    print(\"Using MobileNetV2 base model with alpha=0.75\")\n    for layer in base_model.layers:\n        layer.trainable = False\n    x = base_model.output\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(512)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    shared_features = keras.layers.Dropout(0.4)(x)\n    # Emotion Classification Branch\n    emotion_features = keras.layers.Dense(256)(shared_features)\n    emotion_features = keras.layers.BatchNormalization()(emotion_features)\n    emotion_features = keras.layers.Activation('relu')(emotion_features)\n    emotion_features = keras.layers.Dropout(0.3)(emotion_features)\n    emotion_features = keras.layers.Dense(128)(emotion_features)\n    emotion_features = keras.layers.BatchNormalization()(emotion_features)\n    emotion_features = keras.layers.Activation('relu')(emotion_features)\n    emotion_output = keras.layers.Dense(num_classes, activation='softmax', name='emotion_output')(emotion_features)\n    # Intensity Regression Branch\n    intensity_features = keras.layers.Dense(128)(shared_features)\n    intensity_features = keras.layers.BatchNormalization()(intensity_features)\n    intensity_features = keras.layers.Activation('relu')(intensity_features)\n    intensity_features = keras.layers.Dropout(0.3)(intensity_features)\n    intensity_output = keras.layers.Dense(1, activation='sigmoid', name='intensity_output')(intensity_features)\n    # Valence-Arousal Regression Branch\n    va_features = keras.layers.Dense(128)(shared_features)\n    va_features = keras.layers.BatchNormalization()(va_features)\n    va_features = keras.layers.Activation('relu')(va_features)\n    va_features = keras.layers.Dropout(0.3)(va_features)\n    valence_output = keras.layers.Dense(1, activation='tanh', name='valence_output')(va_features)\n    arousal_output = keras.layers.Dense(1, activation='tanh', name='arousal_output')(va_features)\n    # Emotion Group Classification Branch\n    group_features = keras.layers.Dense(64)(shared_features)\n    group_features = keras.layers.BatchNormalization()(group_features)\n    group_features = keras.layers.Activation('relu')(group_features)\n    group_output = keras.layers.Dense(NUM_EMOTION_GROUPS, activation='softmax', name='group_output')(group_features)\n    model = keras.Model(inputs=inputs, outputs=[emotion_output, intensity_output, valence_output, arousal_output, group_output])\n    loss_weights = {\n        'emotion_output': 1.0,\n        'intensity_output': 0.5,\n        'valence_output': 0.3,\n        'arousal_output': 0.3,\n        'group_output': 0.4\n    }\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss={\n            'emotion_output': label_smoothing_loss(epsilon=0.1),\n            'intensity_output': mse_with_uncertainty,\n            'valence_output': mse_with_uncertainty,\n            'arousal_output': mse_with_uncertainty,\n            'group_output': 'categorical_crossentropy'\n        },\n        loss_weights=loss_weights,\n        metrics={\n            'emotion_output': 'accuracy',\n            'intensity_output': 'mae',\n            'valence_output': 'mae',\n            'arousal_output': 'mae',\n            'group_output': 'accuracy'\n        }\n    )\n    return model, base_model\n\n# ---------------------------------------------------------------------------------\n# Evaluation Function\n# ---------------------------------------------------------------------------------\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"Evaluate multi-task model and produce confusion matrices and reports.\"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    all_true_emotions, all_pred_emotions = [], []\n    all_true_groups, all_pred_groups = [], []\n    intensity_errors, valence_errors, arousal_errors = [], [], []\n    for i, (images, labels_dict) in enumerate(test_ds):\n        if i >= steps: break\n        predictions = model.predict(images, verbose=0)\n        if isinstance(predictions, list):\n            emotion_preds = predictions[0]\n            intensity_preds = predictions[1]\n            valence_preds = predictions[2]\n            arousal_preds = predictions[3]\n            group_preds = predictions[4]\n        else:\n            emotion_preds = predictions['emotion_output']\n            intensity_preds = predictions['intensity_output']\n            valence_preds = predictions['valence_output']\n            arousal_preds = predictions['arousal_output']\n            group_preds = predictions['group_output']\n        all_pred_emotions.append(np.argmax(emotion_preds, axis=1))\n        all_true_emotions.append(np.argmax(labels_dict['emotion_output'].numpy(), axis=1))\n        all_pred_groups.append(np.argmax(group_preds, axis=1))\n        all_true_groups.append(np.argmax(labels_dict['group_output'].numpy(), axis=1))\n        intensity_errors.append(np.abs(intensity_preds - labels_dict['intensity_output'].numpy()))\n        valence_errors.append(np.abs(valence_preds - labels_dict['valence_output'].numpy()))\n        arousal_errors.append(np.abs(arousal_preds - labels_dict['arousal_output'].numpy()))\n    all_true_emotions = np.concatenate(all_true_emotions)\n    all_pred_emotions = np.concatenate(all_pred_emotions)\n    all_true_groups = np.concatenate(all_true_groups)\n    all_pred_groups = np.concatenate(all_pred_groups)\n    intensity_errors = np.concatenate([e.flatten() for e in intensity_errors])\n    valence_errors = np.concatenate([e.flatten() for e in valence_errors])\n    arousal_errors = np.concatenate([e.flatten() for e in arousal_errors])\n    emotion_accuracy = np.mean(all_pred_emotions == all_true_emotions)\n    emotion_f1 = f1_score(all_true_emotions, all_pred_emotions, average='weighted')\n    group_accuracy = np.mean(all_pred_groups == all_true_groups)\n    intensity_mae = np.mean(intensity_errors)\n    valence_mae = np.mean(valence_errors)\n    arousal_mae = np.mean(arousal_errors)\n    print(f\"\\n=== {dataset_name} Evaluation Results ===\")\n    print(f\"Emotion Accuracy: {emotion_accuracy:.4f}\")\n    print(f\"Emotion F1 Score: {emotion_f1:.4f}\")\n    print(f\"Group Accuracy: {group_accuracy:.4f}\")\n    print(f\"Intensity MAE: {intensity_mae:.4f}\")\n    print(f\"Valence MAE: {valence_mae:.4f}\")\n    print(f\"Arousal MAE: {arousal_mae:.4f}\")\n    emotion_cm = confusion_matrix(all_true_emotions, all_pred_emotions)\n    group_cm = confusion_matrix(all_true_groups, all_pred_groups)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(emotion_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted'); plt.ylabel('True')\n    plt.title(f'Emotion Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    try:\n        plt.savefig(f'{log_dir}/emotion_confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save plot: {e}\")\n    plt.close()\n    group_names = ['Negative', 'Positive', 'Neutral']\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(group_cm, annot=True, fmt='d', cmap='Blues', xticklabels=group_names, yticklabels=group_names)\n    plt.xlabel('Predicted'); plt.ylabel('True')\n    plt.title(f'Group Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    try:\n        plt.savefig(f'{log_dir}/group_confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save plot: {e}\")\n    plt.close()\n    print(f\"\\n{dataset_name} Emotion Classification Report:\")\n    emotion_report = classification_report(all_true_emotions, all_pred_emotions, target_names=class_names, zero_division=0)\n    print(emotion_report)\n    print(f\"\\n{dataset_name} Group Classification Report:\")\n    group_report = classification_report(all_true_groups, all_pred_groups, target_names=group_names, zero_division=0)\n    print(group_report)\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(\"EMOTION REPORT:\\n\" + emotion_report + \"\\n\\n\")\n        f.write(\"GROUP REPORT:\\n\" + group_report + \"\\n\\n\")\n        f.write(\"REGRESSION METRICS:\\n\")\n        f.write(f\"Intensity MAE: {intensity_mae:.4f}\\n\")\n        f.write(f\"Valence MAE: {valence_mae:.4f}\\n\")\n        f.write(f\"Arousal MAE: {arousal_mae:.4f}\\n\")\n    return {\n        'emotion_accuracy': emotion_accuracy,\n        'emotion_f1': emotion_f1,\n        'group_accuracy': group_accuracy,\n        'intensity_mae': intensity_mae,\n        'valence_mae': valence_mae,\n        'arousal_mae': arousal_mae,\n        'emotion_cm': emotion_cm,\n        'group_cm': group_cm\n    }\n\n# ---------------------------------------------------------------------------------\n# Progressive Unfreezing Training for Multi-Task Learning\n# ---------------------------------------------------------------------------------\ndef train_with_progressive_unfreezing(model, base_model, train_ds, val_ds, \n                                      steps_per_epoch, val_steps, \n                                      epochs_head=10, epochs_finetune=20,\n                                      callbacks=None, class_weights=None):\n    print(f\"\\nStage 1: Training task-specific heads for {epochs_head} epochs\")\n    for layer in base_model.layers:\n        layer.trainable = False\n    loss_weights = {\n        'emotion_output': 1.0,\n        'intensity_output': 0.5,\n        'valence_output': 0.3,\n        'arousal_output': 0.3,\n        'group_output': 0.4\n    }\n    # Here we use our custom loss for emotion output (determined later based on dataset)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss={\n            'emotion_output': label_smoothing_loss(epsilon=0.1),\n            'intensity_output': mse_with_uncertainty,\n            'valence_output': mse_with_uncertainty,\n            'arousal_output': mse_with_uncertainty,\n            'group_output': 'categorical_crossentropy'\n        },\n        loss_weights=loss_weights,\n        metrics={\n            'emotion_output': 'accuracy',\n            'intensity_output': 'mae',\n            'valence_output': 'mae',\n            'arousal_output': 'mae',\n            'group_output': 'accuracy'\n        }\n    )\n    history_head = model.fit(\n        train_ds,\n        epochs=epochs_head,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        verbose=1\n    )\n    print(f\"\\nStage 2: Progressive unfreezing for {epochs_finetune} epochs\")\n    fine_tuning_history = []\n    layer_groups = [\n        base_model.layers[-15:],\n        base_model.layers[-30:-15],\n        base_model.layers[-50:-30]\n    ]\n    for i, group in enumerate(layer_groups):\n        print(f\"\\nUnfreezing group {i+1}/{len(layer_groups)} ({len(group)} layers)\")\n        for layer in group:\n            layer.trainable = True\n        lr = 1e-4 / (i + 1)\n        fine_tune_loss_weights = {\n            'emotion_output': 1.0,\n            'intensity_output': 0.4/(i+1),\n            'valence_output': 0.2/(i+1),\n            'arousal_output': 0.2/(i+1),\n            'group_output': 0.3/(i+1)\n        }\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=lr),\n            loss={\n                'emotion_output': label_smoothing_loss(epsilon=0.1),\n                'intensity_output': mse_with_uncertainty,\n                'valence_output': mse_with_uncertainty,\n                'arousal_output': mse_with_uncertainty,\n                'group_output': 'categorical_crossentropy'\n            },\n            loss_weights=fine_tune_loss_weights,\n            metrics={\n                'emotion_output': 'accuracy',\n                'intensity_output': 'mae',\n                'valence_output': 'mae',\n                'arousal_output': 'mae',\n                'group_output': 'accuracy'\n            }\n        )\n        epochs_per_group = max(5, epochs_finetune // len(layer_groups))\n        history = model.fit(\n            train_ds,\n            epochs=epochs_per_group,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            verbose=1\n        )\n        fine_tuning_history.append(history)\n    return history_head, fine_tuning_history\n\n# ---------------------------------------------------------------------------------\n# Multi-Task Training Pipeline\n# ---------------------------------------------------------------------------------\ndef train_multitask_emotion_model(data_dir):\n    print(\"Starting multi-task emotion recognition training\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    # Create validation splits\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, test_size=0.15, stratify=affectnet_train_df[\"label\"], random_state=42)\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, test_size=0.15, stratify=fer_train_df[\"label\"], random_state=42)\n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    print(\"\\nCreating enhanced data pipelines\")\n    affectnet_train_ds, class_indices = create_emphasis_dataset(affectnet_train_df, is_training=True)\n    affectnet_val_ds, _ = create_dataset(affectnet_val_df, is_training=False, dataset_type='affectnet')\n    affectnet_test_ds, _ = create_dataset(test_affectnet_df, is_training=False, dataset_type='affectnet')\n    fer_train_ds, _ = create_emphasis_dataset(fer_train_df, is_training=True)\n    fer_val_ds, _ = create_dataset(fer_val_df, is_training=False, dataset_type='fer2013')\n    fer_test_ds, _ = create_dataset(test_fer_df, is_training=False, dataset_type='fer2013')\n    combined_test_ds, _ = create_dataset(test_df, is_training=False)\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    combined_test_steps = len(test_df) // BATCH_SIZE\n    print(\"\\nCreating multi-task emotion model\")\n    model, base_model = create_multitask_emotion_model(num_classes)\n    print(\"\\nComputing class weights for emotion classification task\")\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"]),\n        y=affectnet_train_df[\"label\"]\n    )\n    affectnet_class_weights = {\n        'emotion_output': {class_indices[label]: weight for label, weight in \n                           zip(np.unique(affectnet_train_df[\"label\"]), affectnet_weights)}\n    }\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"]),\n        y=fer_train_df[\"label\"]\n    )\n    fer_class_weights = {\n        'emotion_output': {class_indices[label]: weight for label, weight in \n                           zip(np.unique(fer_train_df[\"label\"]), fer_weights)}\n    }\n    # Create weighted loss functions (example shown here; adapt as needed)\n    # Assume affectnet_class_dist and fer_class_dist are computed distributions from the DataFrame.\n    affectnet_class_dist = affectnet_train_df['label'].value_counts().sort_index()\n    fer_class_dist = fer_train_df['label'].value_counts().sort_index()\n    affectnet_emotion_loss = create_weighted_categorical_crossentropy(affectnet_class_dist.values, class_indices)\n    fer_emotion_loss = create_weighted_categorical_crossentropy(fer_class_dist.values, class_indices)\n    print(\"Created weighted loss functions for emotion classification\")\n    print(\"\\nSetting up enhanced callbacks\")\n    base_callbacks = [\n        tf.keras.callbacks.EarlyStopping(monitor='val_emotion_output_accuracy', patience=8, restore_best_weights=True, verbose=1),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1),\n        tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1, update_freq='epoch')\n    ]\n    affectnet_callbacks = base_callbacks + [\n        tf.keras.callbacks.ModelCheckpoint('model_checkpoints/affectnet_multitask_best.weights.h5', monitor='val_emotion_output_accuracy', save_best_only=True, save_weights_only=True, verbose=1),\n        MultiTaskMonitorCallback(affectnet_val_ds, classes, log_dir=os.path.join(LOG_DIR, 'affectnet'), model_name=\"AffectNet-MultiTask\", freq=3),\n        tf.keras.callbacks.CSVLogger('affectnet_multitask_training_log.csv', append=True)\n    ]\n    fer_callbacks = base_callbacks + [\n        tf.keras.callbacks.ModelCheckpoint('model_checkpoints/fer2013_multitask_best.weights.h5', monitor='val_emotion_output_accuracy', save_best_only=True, save_weights_only=True, verbose=1),\n        MultiTaskMonitorCallback(fer_val_ds, classes, log_dir=os.path.join(LOG_DIR, 'fer2013'), model_name=\"FER2013-MultiTask\", freq=3),\n        tf.keras.callbacks.CSVLogger('fer2013_multitask_training_log.csv', append=True)\n    ]\n    print(\"\\nStage 1: Training on AffectNet with progressive unfreezing\")\n    history_affectnet_head, history_affectnet_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        epochs_head=12, epochs_finetune=18,\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    model.save(\"affectnet_multitask_model.keras\")\n    print(\"AffectNet multi-task model saved to 'affectnet_multitask_model.keras'\")\n    affectnet_metrics = evaluate_model(model, affectnet_test_ds, affectnet_test_steps, classes, LOG_DIR, dataset_name=\"AffectNet\")\n    print(\"\\nStage 2: Fine-tuning on FER2013 with progressive unfreezing\")\n    history_fer_head, history_fer_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        epochs_head=10, epochs_finetune=15,\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    fer_metrics = evaluate_model(model, fer_test_ds, fer_test_steps, classes, LOG_DIR, dataset_name=\"FER2013\")\n    combined_metrics = evaluate_model(model, combined_test_ds, combined_test_steps, classes, LOG_DIR, dataset_name=\"Combined\")\n    model.save(\"final_multitask_emotion_model.keras\")\n    print(\"Final multi-task model saved to 'final_multitask_emotion_model.keras'\")\n    return model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# ---------------------------------------------------------------------------------\n# Main Entry Point\n# ---------------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    model, metrics = train_multitask_emotion_model(data_dir)\n    print(\"\\n=== FINAL MULTI-TASK RESULTS ===\")\n    print(f\"AffectNet Emotion Accuracy: {metrics['affectnet']['emotion_accuracy']:.4f}\")\n    print(f\"AffectNet Emotion F1 Score: {metrics['affectnet']['emotion_f1']:.4f}\")\n    print(f\"AffectNet Group Accuracy: {metrics['affectnet']['group_accuracy']:.4f}\")\n    print(f\"AffectNet Intensity MAE: {metrics['affectnet']['intensity_mae']:.4f}\")\n    print(f\"FER2013 Emotion Accuracy: {metrics['fer2013']['emotion_accuracy']:.4f}\")\n    print(f\"FER2013 Emotion F1 Score: {metrics['fer2013']['emotion_f1']:.4f}\")\n    print(f\"FER2013 Group Accuracy: {metrics['fer2013']['group_accuracy']:.4f}\")\n    print(f\"FER2013 Intensity MAE: {metrics['fer2013']['intensity_mae']:.4f}\")\n    print(f\"Combined Emotion Accuracy: {metrics['combined']['emotion_accuracy']:.4f}\")\n    print(f\"Combined Emotion F1 Score: {metrics['combined']['emotion_f1']:.4f}\")\n    print(f\"Combined Group Accuracy: {metrics['combined']['group_accuracy']:.4f}\")\n    print(f\"Combined Intensity MAE: {metrics['combined']['intensity_mae']:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6o current accuracy\n#=== FINAL RESULTS === 96x96\n#AffectNet Test Accuracy: 0.4461\n#AffectNet F1 Score: 0.4446\n#FER2013 Test Accuracy: 0.2203\n#FER2013 F1 Score: 0.1875\n#Combined Test Accuracy: 0.3168\n#Combined F1 Score: 0.3050\n\n#=== FINAL RESULTS === 224x224 try 112\n#AffectNet Test Accuracy: 0.4669\n#AffectNet F1 Score: 0.4799\n#FER2013 Test Accuracy: 0.2953\n#FER2013 F1 Score: 0.2474\n#Combined Test Accuracy: 0.3050\n#Combined F1 Score: 0.2910\n\n#=== FINAL RESULTS === 112x112 64\n#AffectNet Test Accuracy: 0.4738\n#AffectNet F1 Score: 0.4748\n#FER2013 Test Accuracy: 0.3175\n#FER2013 F1 Score: 0.3021\n#Combined Test Accuracy: 0.3431\n#Combined F1 Score: 0.3168\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 112  # Keep at 96x96 as specified\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Custom Label Smoothing Loss\n# =============================================================================\ndef label_smoothing_loss(epsilon=0.1):\n    \"\"\"\n    Cross entropy loss with label smoothing to prevent model from being too confident.\n    \n    Args:\n        epsilon: Smoothing factor (0 = no smoothing, 1 = complete smoothing)\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        \n        # Apply label smoothing\n        y_true = y_true * (1.0 - epsilon) + (epsilon / num_classes)\n        \n        # Calculate cross entropy with extra small epsilon to prevent log(0)\n        return -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7), axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# Fixed graph-compatible preprocessing function\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Fixed graph-compatible preprocessing with class-specific augmentation.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize pixel values using MobileNet standard preprocessing\n    img = tf.cast(img, tf.float32)\n    img = img / 127.5 - 1.0  # Scale to [-1, 1]\n    \n    # Apply basic augmentation during training\n    if training:\n        # Random flip - works in graph mode\n        img = tf.image.random_flip_left_right(img)\n        \n        # Basic brightness and contrast\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n            \n        # Add random noise to improve robustness\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        \n        # Ensure values stay in valid range\n        img = tf.clip_by_value(img, -1.0, 1.0)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None):\n    \"\"\"\n    Creates a tf.data.Dataset with fixed preprocessing.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Repeat dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset\n    return create_dataset(balanced_df, is_training=is_training)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends instead of plotting them\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization (still useful)\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# Create emotion recognition model with additional MLP head\n# =============================================================================\ndef create_emotion_model(num_classes):\n    \"\"\"\n    Create a facial emotion recognition model with enhanced classification head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model and base model\n    \"\"\"\n    # Input shape\n    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n    \n    # Create input layer\n    inputs = keras.layers.Input(shape=input_shape)\n    \n    # Use MobileNetV2 as base\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs,\n        alpha=1.0  # Controls model width\n    )\n    print(\"Using MobileNetV2 base model\")\n    \n    # Freeze base model layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom head with dropout and batch normalization\n    x = base_model.output\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # First dense block\n    x = keras.layers.Dense(256)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.4)(x)\n    \n    # Second dense block\n    x = keras.layers.Dense(128)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    \n    # Output layer with label smoothing\n    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with label smoothing loss\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.1),\n        metrics=['accuracy']\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Two-Stage Fine-Tuning with Progressive Unfreezing\n# =============================================================================\ndef train_with_progressive_unfreezing(model, base_model, train_ds, val_ds, \n                                    steps_per_epoch, val_steps, \n                                    epochs_head=10, epochs_finetune=20,\n                                    callbacks=None, class_weights=None):\n    \"\"\"\n    Two-stage training approach: first train only the head, then progressively unfreeze layers.\n    \n    Args:\n        model: The model to train\n        base_model: The base model part (for unfreezing)\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        epochs_head: Epochs for head-only training\n        epochs_finetune: Epochs for fine-tuning\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    print(f\"\\nStage 1: Training only the classification head ({epochs_head} epochs)\")\n    \n    # Ensure base model is frozen\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Compile with higher learning rate for head training\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.1),\n        metrics=['accuracy']\n    )\n    \n    # Train head only\n    history_head = model.fit(\n        train_ds,\n        epochs=epochs_head,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    print(f\"\\nStage 2: Fine-tuning with progressive unfreezing ({epochs_finetune} epochs)\")\n    \n    # Progressively unfreeze layers in groups\n    fine_tuning_history = []\n    \n    # Groups of layers to unfreeze (from last to first)\n    layer_groups = [\n        # Unfreeze last layers first (deeper = more specific features)\n        base_model.layers[-15:],  # Last block\n        base_model.layers[-30:-15],  # Second-to-last block\n        base_model.layers[-50:-30]   # Third-to-last block\n    ]\n    \n    for i, group in enumerate(layer_groups):\n        print(f\"\\nUnfreezing group {i+1}/{len(layer_groups)} ({len(group)} layers)\")\n        \n        # Unfreeze current group\n        for layer in group:\n            layer.trainable = True\n            \n        # Recompile with lower learning rate as we go deeper\n        lr = 1e-4 / (i + 1)  # Decrease learning rate for deeper layers\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=lr),\n            loss=label_smoothing_loss(epsilon=0.1),\n            metrics=['accuracy']\n        )\n        \n        # Train for a few epochs\n        epochs_per_group = max(5, epochs_finetune // len(layer_groups))\n        \n        history = model.fit(\n            train_ds,\n            epochs=epochs_per_group,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        \n        fine_tuning_history.append(history)\n    \n    # Return combined history\n    return history_head, fine_tuning_history\n\n# =============================================================================\n# Sequential Training Pipeline\n# =============================================================================\ndef train_enhanced_emotion_model(data_dir):\n    \"\"\"\n    Enhanced sequential training with all improvements.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced sequential emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes\n    print(\"\\n2. Creating enhanced data pipelines\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet')\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet')\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013')\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013')\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False)\n    \n    # 5. Calculate steps\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create enhanced model\n    print(\"\\n3. Creating enhanced model\")\n    model, base_model = create_emotion_model(num_classes)\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"]),\n        y=affectnet_train_df[\"label\"]\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"]), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"]),\n        y=fer_train_df[\"label\"]\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"]), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR,\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train on AffectNet using progressive unfreezing\n    print(\"\\n6. STAGE 1: Training on AffectNet with progressive unfreezing\")\n    \n    history_affectnet_head, history_affectnet_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        epochs_head=10, epochs_finetune=15,\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    model.save(\"affectnet_model.keras\")\n    print(\"AffectNet model saved to 'affectnet_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive unfreezing\n    print(\"\\n7. STAGE 2: Fine-tuning on FER2013 with progressive unfreezing\")\n    \n    history_fer_head, history_fer_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        epochs_head=8, epochs_finetune=12,\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    model.save(\"final_enhanced_emotion_model.keras\")\n    print(\"Final model saved to 'final_enhanced_emotion_model.keras'\")\n    \n    # Return models and metrics\n    return model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model with all improvements\n    model, metrics = train_enhanced_emotion_model(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:39:03.133178Z","iopub.execute_input":"2025-03-13T10:39:03.133664Z","iopub.status.idle":"2025-03-13T11:01:30.058078Z","shell.execute_reply.started":"2025-03-13T10:39:03.133622Z","shell.execute_reply":"2025-03-13T11:01:30.057400Z"}},"outputs":[{"name":"stdout","text":"Found 1 GPUs: Memory growth enabled\nMixed precision enabled\nStarting enhanced sequential emotion recognition training\n\n1. Loading datasets\nFound 8 emotion categories: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\nFound 3161 images in surprise/fer2013\nFound 4039 images in surprise/affectnet\nFound 4092 images in fear/fer2013\nFound 3176 images in fear/affectnet\nFound 4951 images in neutral/fer2013\nFound 5126 images in neutral/affectnet\nFound 4823 images in sad/fer2013\nFound 3091 images in sad/affectnet\nFound 435 images in disgust/fer2013\nFound 2477 images in disgust/affectnet\nFound 54 images in contempt/fer2013\nFound 2871 images in contempt/affectnet\nFound 7199 images in happy/fer2013\nFound 5044 images in happy/affectnet\nFound 3987 images in anger/fer2013\nFound 3218 images in anger/affectnet\nTotal images: 57744\nFound 8 emotion categories: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\nFound 831 images in surprise/fer2013\nFound 4039 images in surprise/affectnet\nFound 1024 images in fear/fer2013\nFound 3176 images in fear/affectnet\nFound 1233 images in neutral/fer2013\nFound 5126 images in neutral/affectnet\nFound 1247 images in sad/fer2013\nFound 3091 images in sad/affectnet\nFound 111 images in disgust/fer2013\nFound 2477 images in disgust/affectnet\nFound 54 images in contempt/fer2013\nFound 2871 images in contempt/affectnet\nFound 1774 images in happy/fer2013\nFound 5044 images in happy/affectnet\nFound 958 images in anger/fer2013\nFound 3218 images in anger/affectnet\nTotal images: 36274\n\nAffectNet training distribution:\nlabel\nneutral     5126\nhappy       5044\nsurprise    4039\nanger       3218\nfear        3176\nsad         3091\ncontempt    2871\ndisgust     2477\nName: count, dtype: int64\n\nFER2013 training distribution:\nlabel\nhappy       7199\nneutral     4951\nsad         4823\nfear        4092\nanger       3987\nsurprise    3161\ndisgust      435\ncontempt      54\nName: count, dtype: int64\n\nTest sets: AffectNet=29042, FER2013=7232\nClasses: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nAffectNet: 24685 train, 4357 validation\nFER2013: 24396 train, 4306 validation\n\n2. Creating enhanced data pipelines\nCreated balanced dataset with 3800 samples (with emphasis on ['surprise', 'sad', 'disgust'])\nFiltered to 4357 affectnet images\nFiltered to 29042 affectnet images\nCreated balanced dataset with 3800 samples (with emphasis on ['surprise', 'sad', 'disgust'])\nFiltered to 4306 fer2013 images\nFiltered to 7232 fer2013 images\n\n3. Creating enhanced model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-a066aad07bb2>:410: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  base_model = MobileNetV2(\n","output_type":"stream"},{"name":"stdout","text":"Using MobileNetV2 base model\n\n4. Computing class weights with adjustments for problematic classes\nEnhanced AffectNet class weights: {0: 1.128199268738574, 1: 1.2646004098360655, 2: 1.758190883190883, 3: 1.142824074074074, 4: 0.7197632376953581, 5: 0.7081994491622676, 6: 1.4094975256947089, 7: 1.0785755898630933}\nEnhanced FER2013 class weights: {0: 0.899822956624373, 1: 66.29347826086956, 2: 9.890270270270271, 3: 0.8767970097757332, 4: 0.4983657460369341, 5: 0.7246910646387833, 6: 0.8927543303244693, 7: 1.3618905842947524}\n\n5. Setting up enhanced callbacks\n\n6. STAGE 1: Training on AffectNet with progressive unfreezing\n\nStage 1: Training only the classification head (10 epochs)\nEpoch 1/10\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2655 - loss: 2.3965\nEpoch 1: val_accuracy improved from -inf to 0.32054, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 79ms/step - accuracy: 0.2657 - loss: 2.3959 - val_accuracy: 0.3205 - val_loss: 1.8420 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4795 - loss: 1.8107\nEpoch 2: val_accuracy improved from 0.32054 to 0.33548, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.4796 - loss: 1.8105 - val_accuracy: 0.3355 - val_loss: 1.8383 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6030 - loss: 1.5413\nEpoch 3: val_accuracy improved from 0.33548 to 0.33594, saving model to model_checkpoints/affectnet_best.weights.h5\n\nConfusion Matrix:\n[[ 59  24  51  16   2   2  34  24]\n [ 28  66  53   8   3   3  27  13]\n [ 25  15  53  16   2   4  30  22]\n [ 23  20  31  54   0   0  48  33]\n [  9  12  29   5 154  55  19  44]\n [ 19  16  22  10  65 116  20  69]\n [ 29  32  32  22   0   1  59  28]\n [ 19  18  34  31  12  18  41  91]]\nanger: 0.2783  contempt: 0.3284  disgust: 0.3174  fear: 0.2584  \nhappy: 0.4709  neutral: 0.3442  sad: 0.2906  surprise: 0.3447  \n\n\nClass Accuracy Trends:\nanger: [0.1745, 0.2311, 0.2783]\ncontempt: [0.3333, 0.2537, 0.3284]\ndisgust: [0.5808, 0.4970, 0.3174]\nfear: [0.1818, 0.2536, 0.2584]\nhappy: [0.4832, 0.4771, 0.4709]\nneutral: [0.2938, 0.4036, 0.3442]\nsad: [0.1921, 0.2463, 0.2906]\nsurprise: [0.3902, 0.2803, 0.3447]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6031 - loss: 1.5410 - val_accuracy: 0.3359 - val_loss: 1.8983 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6798 - loss: 1.3627\nEpoch 4: val_accuracy improved from 0.33594 to 0.34237, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.6799 - loss: 1.3624 - val_accuracy: 0.3424 - val_loss: 1.9575 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7539 - loss: 1.1998\nEpoch 5: val_accuracy did not improve from 0.34237\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.7539 - loss: 1.1998 - val_accuracy: 0.3316 - val_loss: 2.0244 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8061 - loss: 1.0855\nEpoch 6: val_accuracy did not improve from 0.34237\n\nConfusion Matrix:\n[[ 35  25  49  22   3   3  46  29]\n [ 21  57  36   7   6   5  39  30]\n [ 16  17  49  25   1   5  30  24]\n [ 15  14  27  55   2   1  55  40]\n [ 10  18  15   7 152  80  13  32]\n [ 13  17  14  17  67 142  17  50]\n [ 12  36  40  27   1   0  53  34]\n [ 18  19  28  35  13  19  42  90]]\nanger: 0.1651  contempt: 0.2836  disgust: 0.2934  fear: 0.2632  \nhappy: 0.4648  neutral: 0.4214  sad: 0.2611  surprise: 0.3409  \n\n\nClass Accuracy Trends:\nanger: [0.2311, 0.2783, 0.2358, 0.2358, 0.1651]\ncontempt: [0.2537, 0.3284, 0.3532, 0.2040, 0.2836]\ndisgust: [0.4970, 0.3174, 0.3533, 0.3473, 0.2934]\nfear: [0.2536, 0.2584, 0.2249, 0.2344, 0.2632]\nhappy: [0.4771, 0.4709, 0.5321, 0.5076, 0.4648]\nneutral: [0.4036, 0.3442, 0.3353, 0.3175, 0.4214]\nsad: [0.2463, 0.2906, 0.2365, 0.3054, 0.2611]\nsurprise: [0.2803, 0.3447, 0.3106, 0.3371, 0.3409]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.8061 - loss: 1.0854 - val_accuracy: 0.3410 - val_loss: 1.9855 - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8395 - loss: 1.0061\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 7: val_accuracy did not improve from 0.34237\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.8396 - loss: 1.0061 - val_accuracy: 0.3272 - val_loss: 2.1046 - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8895 - loss: 0.9105\nEpoch 8: val_accuracy improved from 0.34237 to 0.34697, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.8896 - loss: 0.9104 - val_accuracy: 0.3470 - val_loss: 2.0310 - learning_rate: 5.0000e-04\nEpoch 9/10\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9192 - loss: 0.8543\nEpoch 9: val_accuracy did not improve from 0.34697\n\nConfusion Matrix:\n[[ 30  20  77  15   2   5  42  21]\n [ 18  44  67  11   5   5  36  15]\n [ 15   9  66  14   1   7  34  21]\n [ 11  14  45  50   3   3  53  30]\n [  5   9  32   5 158  85  15  18]\n [ 15  10  33  11  66 149  24  29]\n [ 19  21  56  21   2   3  56  25]\n [ 17  12  50  38  14  26  51  56]]\nanger: 0.1415  contempt: 0.2189  disgust: 0.3952  fear: 0.2392  \nhappy: 0.4832  neutral: 0.4421  sad: 0.2759  surprise: 0.2121  \n\n\nClass Accuracy Trends:\nanger: [0.2358, 0.1651, 0.1745, 0.2736, 0.1415]\ncontempt: [0.2040, 0.2836, 0.1741, 0.3781, 0.2189]\ndisgust: [0.3473, 0.2934, 0.2036, 0.2635, 0.3952]\nfear: [0.2344, 0.2632, 0.2249, 0.2536, 0.2392]\nhappy: [0.5076, 0.4648, 0.4159, 0.4893, 0.4832]\nneutral: [0.3175, 0.4214, 0.3709, 0.4421, 0.4421]\nsad: [0.3054, 0.2611, 0.3498, 0.2365, 0.2759]\nsurprise: [0.3371, 0.3409, 0.4621, 0.2841, 0.2121]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.9193 - loss: 0.8543 - val_accuracy: 0.3288 - val_loss: 2.0750 - learning_rate: 5.0000e-04\nEpoch 10/10\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9273 - loss: 0.8358\nEpoch 10: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 47ms/step - accuracy: 0.9274 - loss: 0.8357 - val_accuracy: 0.3385 - val_loss: 2.0581 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 8.\n\nStage 2: Fine-tuning with progressive unfreezing (15 epochs)\n\nUnfreezing group 1/3 (15 layers)\nEpoch 1/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5402 - loss: 1.8444\nEpoch 1: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.5408 - loss: 1.8425 - val_accuracy: 0.2500 - val_loss: 2.8479 - learning_rate: 1.0000e-04\nEpoch 2/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8505 - loss: 0.9892\nEpoch 2: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - accuracy: 0.8506 - loss: 0.9889 - val_accuracy: 0.2661 - val_loss: 2.7332 - learning_rate: 1.0000e-04\nEpoch 3/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9341 - loss: 0.8212\nEpoch 3: val_accuracy did not improve from 0.34697\n\nConfusion Matrix:\n[[ 77   9  30  19   1   0  10  66]\n [ 39  31  23   7   2   0  10  89]\n [ 36  17  26  27   1   1   4  55]\n [ 24   5  16  61   0   0   6  97]\n [ 19   5  22  12 110  37   7 115]\n [ 29   4  32  11  45  72   3 141]\n [ 48  14  19  23   0   0  21  78]\n [ 28   4  19  27   3   9   7 167]]\nanger: 0.3632  contempt: 0.1542  disgust: 0.1557  fear: 0.2919  \nhappy: 0.3364  neutral: 0.2136  sad: 0.1034  surprise: 0.6326  \n\n\nClass Accuracy Trends:\nanger: [0.1415, 0.3113, 0.2453, 0.2453, 0.3632]\ncontempt: [0.2189, 0.2687, 0.2438, 0.1642, 0.1542]\ndisgust: [0.3952, 0.2874, 0.1916, 0.1856, 0.1557]\nfear: [0.2392, 0.3110, 0.1483, 0.1866, 0.2919]\nhappy: [0.4832, 0.4893, 0.2324, 0.3058, 0.3364]\nneutral: [0.4421, 0.3561, 0.0742, 0.1068, 0.2136]\nsad: [0.2759, 0.2562, 0.0985, 0.0690, 0.1034]\nsurprise: [0.2121, 0.2689, 0.6932, 0.7424, 0.6326]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 61ms/step - accuracy: 0.9342 - loss: 0.8211 - val_accuracy: 0.2964 - val_loss: 2.4350 - learning_rate: 1.0000e-04\nEpoch 4/5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9601 - loss: 0.7637\nEpoch 4: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.9601 - loss: 0.7637 - val_accuracy: 0.3104 - val_loss: 2.2469 - learning_rate: 1.0000e-04\nEpoch 5/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9779 - loss: 0.7245\nEpoch 5: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - accuracy: 0.9779 - loss: 0.7245 - val_accuracy: 0.2845 - val_loss: 2.2968 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 1.\n\nUnfreezing group 2/3 (15 layers)\nEpoch 1/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4964 - loss: 2.0311\nEpoch 1: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 70ms/step - accuracy: 0.4969 - loss: 2.0293 - val_accuracy: 0.2291 - val_loss: 2.6455 - learning_rate: 5.0000e-05\nEpoch 2/5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7797 - loss: 1.1445\nEpoch 2: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: 0.7798 - loss: 1.1443 - val_accuracy: 0.2505 - val_loss: 2.5072 - learning_rate: 5.0000e-05\nEpoch 3/5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8853 - loss: 0.9157\nEpoch 3: val_accuracy did not improve from 0.34697\n\nConfusion Matrix:\n[[ 64  23  35  20   1   0  30  39]\n [ 22  54  29  14   1   3  26  52]\n [ 21  11  37  22   0   2  33  41]\n [ 13  12  21  62   0   0  41  60]\n [  9  26  38  15  97  48  28  66]\n [ 29  21  41  25  36  73  24  88]\n [ 32  19  19  20   0   0  64  49]\n [ 27  17  13  42   4   8  34 119]]\nanger: 0.3019  contempt: 0.2687  disgust: 0.2216  fear: 0.2967  \nhappy: 0.2966  neutral: 0.2166  sad: 0.3153  surprise: 0.4508  \n\n\nClass Accuracy Trends:\nanger: [0.4245, 0.4198, 0.2075, 0.2547, 0.3019]\ncontempt: [0.1940, 0.1393, 0.2338, 0.2537, 0.2687]\ndisgust: [0.2515, 0.2994, 0.2635, 0.2575, 0.2216]\nfear: [0.3254, 0.3062, 0.2440, 0.2632, 0.2967]\nhappy: [0.4312, 0.3211, 0.0979, 0.1743, 0.2966]\nneutral: [0.1751, 0.1721, 0.0742, 0.1128, 0.2166]\nsad: [0.1478, 0.1133, 0.2118, 0.2266, 0.3153]\nsurprise: [0.4432, 0.4735, 0.5947, 0.4848, 0.4508]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.8853 - loss: 0.9156 - val_accuracy: 0.2957 - val_loss: 2.2968 - learning_rate: 5.0000e-05\nEpoch 4/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9348 - loss: 0.8144\nEpoch 4: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.9348 - loss: 0.8144 - val_accuracy: 0.3277 - val_loss: 2.1851 - learning_rate: 5.0000e-05\nEpoch 5/5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9603 - loss: 0.7634\nEpoch 5: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.9603 - loss: 0.7634 - val_accuracy: 0.3435 - val_loss: 2.1214 - learning_rate: 5.0000e-05\nRestoring model weights from the end of the best epoch: 1.\n\nUnfreezing group 3/3 (20 layers)\nEpoch 1/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5391 - loss: 1.7844\nEpoch 1: val_accuracy did not improve from 0.34697\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 70ms/step - accuracy: 0.5395 - loss: 1.7832 - val_accuracy: 0.3415 - val_loss: 2.1129 - learning_rate: 3.3333e-05\nEpoch 2/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7695 - loss: 1.1674\nEpoch 2: val_accuracy improved from 0.34697 to 0.37937, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.7697 - loss: 1.1670 - val_accuracy: 0.3794 - val_loss: 2.0302 - learning_rate: 3.3333e-05\nEpoch 3/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8724 - loss: 0.9444\nEpoch 3: val_accuracy did not improve from 0.37937\n\nConfusion Matrix:\n[[ 47  29  52  22   0   0  36  26]\n [  9  72  37   8   4   5  26  40]\n [ 13  24  56  21   1   3  24  25]\n [  6  16  30  67   3   1  42  44]\n [  1  17  18   2 198  59  10  22]\n [ 13  17  22   5 109 106  15  50]\n [ 14  38  41  16   2   2  59  31]\n [ 19  27  20  38  20  14  40  86]]\nanger: 0.2217  contempt: 0.3582  disgust: 0.3353  fear: 0.3206  \nhappy: 0.6055  neutral: 0.3145  sad: 0.2906  surprise: 0.3258  \n\n\nClass Accuracy Trends:\nanger: [0.2170, 0.2925, 0.2311, 0.1698, 0.2217]\ncontempt: [0.2985, 0.2537, 0.3980, 0.3781, 0.3582]\ndisgust: [0.3114, 0.3713, 0.2275, 0.2934, 0.3353]\nfear: [0.2344, 0.2679, 0.3110, 0.3397, 0.3206]\nhappy: [0.4037, 0.4098, 0.5015, 0.6636, 0.6055]\nneutral: [0.2819, 0.3294, 0.2522, 0.3145, 0.3145]\nsad: [0.3251, 0.3005, 0.2562, 0.3399, 0.2906]\nsurprise: [0.4621, 0.4167, 0.4470, 0.3598, 0.3258]\n\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.8724 - loss: 0.9442 - val_accuracy: 0.3794 - val_loss: 2.0202 - learning_rate: 3.3333e-05\nEpoch 4/5\n\u001b[1m384/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9216 - loss: 0.8434\nEpoch 4: val_accuracy improved from 0.37937 to 0.39200, saving model to model_checkpoints/affectnet_best.weights.h5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.9216 - loss: 0.8433 - val_accuracy: 0.3920 - val_loss: 2.0055 - learning_rate: 3.3333e-05\nEpoch 5/5\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9518 - loss: 0.7794\nEpoch 5: val_accuracy did not improve from 0.39200\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.9518 - loss: 0.7794 - val_accuracy: 0.3863 - val_loss: 1.9990 - learning_rate: 3.3333e-05\nRestoring model weights from the end of the best epoch: 4.\nAffectNet model saved to 'affectnet_model.keras'\n\nEvaluating model on AffectNet test set\nAffectNet Test Accuracy: 0.4738\nAffectNet Weighted F1-Score: 0.4748\n\nAffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.48      0.27      0.35      3168\n    contempt       0.43      0.47      0.45      2871\n     disgust       0.32      0.53      0.39      2477\n        fear       0.48      0.36      0.41      3176\n       happy       0.65      0.68      0.67      5044\n     neutral       0.68      0.43      0.53      5126\n         sad       0.38      0.45      0.41      3091\n    surprise       0.38      0.51      0.44      4039\n\n    accuracy                           0.47     28992\n   macro avg       0.48      0.46      0.46     28992\nweighted avg       0.50      0.47      0.47     28992\n\n\n7. STAGE 2: Fine-tuning on FER2013 with progressive unfreezing\n\nStage 1: Training only the classification head (8 epochs)\nEpoch 1/8\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3554 - loss: 8.3291\nEpoch 1: val_accuracy improved from -inf to 0.16371, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.3556 - loss: 8.3204 - val_accuracy: 0.1637 - val_loss: 2.3403 - learning_rate: 0.0010\nEpoch 2/8\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4536 - loss: 5.6305\nEpoch 2: val_accuracy improved from 0.16371 to 0.21362, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.4537 - loss: 5.6303 - val_accuracy: 0.2136 - val_loss: 2.1180 - learning_rate: 0.0010\nEpoch 3/8\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5257 - loss: 5.3153\nEpoch 3: val_accuracy improved from 0.21362 to 0.23554, saving model to model_checkpoints/fer2013_best.weights.h5\n\nConfusion Matrix:\n[[ 59   3  83  10   1   4  64  61]\n [  0   4   0   0   0   0   0   0]\n [  1   1  20   1   0   0   3   2]\n [ 19   2  51  33   2   1  59 114]\n [ 43  12 156  20  30   8  79 131]\n [ 25  10  65  14   4  12  83 104]\n [ 24   2  81  13   0   8 114  75]\n [  5   2  16   5   0   2   9 170]]\nanger: 0.2070  contempt: 1.0000  disgust: 0.7143  fear: 0.1174  \nhappy: 0.0626  neutral: 0.0379  sad: 0.3596  surprise: 0.8134  \n\n\nClass Accuracy Trends:\nanger: [0.1158, 0.1649, 0.2070]\ncontempt: [1.0000, 1.0000, 1.0000]\ndisgust: [0.7857, 0.7143, 0.7143]\nfear: [0.0605, 0.0712, 0.1174]\nhappy: [0.0125, 0.0167, 0.0626]\nneutral: [0.0063, 0.0252, 0.0379]\nsad: [0.1609, 0.3438, 0.3596]\nsurprise: [0.7608, 0.8230, 0.8134]\n\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - accuracy: 0.5257 - loss: 5.3152 - val_accuracy: 0.2355 - val_loss: 2.0748 - learning_rate: 0.0010\nEpoch 4/8\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5806 - loss: 5.1799\nEpoch 4: val_accuracy improved from 0.23554 to 0.26516, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.5807 - loss: 5.1799 - val_accuracy: 0.2652 - val_loss: 2.0225 - learning_rate: 0.0010\nEpoch 5/8\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6287 - loss: 5.0412\nEpoch 5: val_accuracy did not improve from 0.26516\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6288 - loss: 5.0412 - val_accuracy: 0.2533 - val_loss: 2.1397 - learning_rate: 0.0010\nEpoch 6/8\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6878 - loss: 4.9546\nEpoch 6: val_accuracy improved from 0.26516 to 0.29851, saving model to model_checkpoints/fer2013_best.weights.h5\n\nConfusion Matrix:\n[[ 90   0  57  16   7  15  51  49]\n [  0   4   0   0   0   0   0   0]\n [  4   1  16   2   0   0   2   3]\n [ 35   1  38  34  10  11  53  99]\n [ 70   4  92  27 101  12  69 104]\n [ 39   4  41  15  22  49  62  85]\n [ 42   1  55  17   9  19 107  67]\n [  8   1  10   6   2   6   7 169]]\nanger: 0.3158  contempt: 1.0000  disgust: 0.5714  fear: 0.1210  \nhappy: 0.2109  neutral: 0.1546  sad: 0.3375  surprise: 0.8086  \n\n\nClass Accuracy Trends:\nanger: [0.1649, 0.2070, 0.2140, 0.2982, 0.3158]\ncontempt: [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]\ndisgust: [0.7143, 0.7143, 0.6786, 0.6786, 0.5714]\nfear: [0.0712, 0.1174, 0.1246, 0.0819, 0.1210]\nhappy: [0.0167, 0.0626, 0.0981, 0.1065, 0.2109]\nneutral: [0.0252, 0.0379, 0.1104, 0.1136, 0.1546]\nsad: [0.3438, 0.3596, 0.3975, 0.2808, 0.3375]\nsurprise: [0.8230, 0.8134, 0.7847, 0.8230, 0.8086]\n\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 56ms/step - accuracy: 0.6879 - loss: 4.9547 - val_accuracy: 0.2985 - val_loss: 2.0247 - learning_rate: 0.0010\nEpoch 7/8\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7442 - loss: 4.8497\nEpoch 7: val_accuracy did not improve from 0.29851\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7443 - loss: 4.8497 - val_accuracy: 0.2740 - val_loss: 2.1386 - learning_rate: 0.0010\nEpoch 8/8\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7860 - loss: 4.8492\nEpoch 8: val_accuracy did not improve from 0.29851\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.7860 - loss: 4.8491 - val_accuracy: 0.2929 - val_loss: 2.1288 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\n\nStage 2: Fine-tuning with progressive unfreezing (12 epochs)\n\nUnfreezing group 1/3 (15 layers)\nEpoch 1/5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4452 - loss: 5.7483\nEpoch 1: val_accuracy did not improve from 0.29851\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 67ms/step - accuracy: 0.4453 - loss: 5.7477 - val_accuracy: 0.2029 - val_loss: 2.2157 - learning_rate: 1.0000e-04\nEpoch 2/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6011 - loss: 5.2349\nEpoch 2: val_accuracy improved from 0.29851 to 0.31833, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.6013 - loss: 5.2345 - val_accuracy: 0.3183 - val_loss: 1.9299 - learning_rate: 1.0000e-04\nEpoch 3/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7247 - loss: 4.9059\nEpoch 3: val_accuracy improved from 0.31833 to 0.36147, saving model to model_checkpoints/fer2013_best.weights.h5\n\nConfusion Matrix:\n[[ 93   1  62  19  10  23  56  21]\n [  0   4   0   0   0   0   0   0]\n [  3   0  17   2   1   1   2   2]\n [ 38   1  38  49   8  22  65  60]\n [ 54   0  99  34 164  23  56  49]\n [ 33   2  44  30  19  74  71  44]\n [ 37   0  55  33  12  27 126  27]\n [ 16   0  13  14   4   9  11 142]]\nanger: 0.3263  contempt: 1.0000  disgust: 0.6071  fear: 0.1744  \nhappy: 0.3424  neutral: 0.2334  sad: 0.3975  surprise: 0.6794  \n\n\nClass Accuracy Trends:\nanger: [0.2316, 0.2912, 0.2316, 0.4175, 0.3263]\ncontempt: [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]\ndisgust: [0.6071, 0.5714, 0.8214, 0.6071, 0.6071]\nfear: [0.1779, 0.1637, 0.0605, 0.1423, 0.1744]\nhappy: [0.1775, 0.1900, 0.0376, 0.2192, 0.3424]\nneutral: [0.1420, 0.2019, 0.0189, 0.1703, 0.2334]\nsad: [0.3375, 0.3186, 0.2177, 0.2997, 0.3975]\nsurprise: [0.7799, 0.8038, 0.7847, 0.7799, 0.6794]\n\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.7248 - loss: 4.9060 - val_accuracy: 0.3615 - val_loss: 1.8328 - learning_rate: 1.0000e-04\nEpoch 4/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8322 - loss: 4.7114\nEpoch 4: val_accuracy improved from 0.36147 to 0.38783, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.8323 - loss: 4.7116 - val_accuracy: 0.3878 - val_loss: 1.8397 - learning_rate: 1.0000e-04\nEpoch 5/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9072 - loss: 4.5825\nEpoch 5: val_accuracy improved from 0.38783 to 0.38853, saving model to model_checkpoints/fer2013_best.weights.h5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.9073 - loss: 4.5826 - val_accuracy: 0.3885 - val_loss: 1.8766 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 1.\n\nUnfreezing group 2/3 (15 layers)\nEpoch 1/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5583 - loss: 5.3459\nEpoch 1: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 64ms/step - accuracy: 0.5584 - loss: 5.3453 - val_accuracy: 0.2962 - val_loss: 1.9639 - learning_rate: 5.0000e-05\nEpoch 2/5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6736 - loss: 4.9979\nEpoch 2: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.6737 - loss: 4.9978 - val_accuracy: 0.3146 - val_loss: 1.9333 - learning_rate: 5.0000e-05\nEpoch 3/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7724 - loss: 4.8570\nEpoch 3: val_accuracy did not improve from 0.38853\n\nConfusion Matrix:\n[[ 67   2  89   9   3  18  63  34]\n [  0   4   0   0   0   0   0   0]\n [  1   1  19   1   0   1   2   3]\n [ 19   4  56  27   4  11  73  87]\n [ 23  10 160  20 118  21  53  74]\n [ 18   3  59  15  11  65  90  56]\n [ 20   2  76  17   7  19 144  32]\n [  3   2  14   9   1   4  10 166]]\nanger: 0.2351  contempt: 1.0000  disgust: 0.6786  fear: 0.0961  \nhappy: 0.2463  neutral: 0.2050  sad: 0.4543  surprise: 0.7943  \n\n\nClass Accuracy Trends:\nanger: [0.2491, 0.1263, 0.3825, 0.3649, 0.2351]\ncontempt: [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]\ndisgust: [0.5714, 0.5714, 0.6429, 0.7143, 0.6786]\nfear: [0.2776, 0.2100, 0.1068, 0.0890, 0.0961]\nhappy: [0.4092, 0.4614, 0.2067, 0.2651, 0.2463]\nneutral: [0.2744, 0.2902, 0.1356, 0.1356, 0.2050]\nsad: [0.3375, 0.4385, 0.3502, 0.3849, 0.4543]\nsurprise: [0.7081, 0.7656, 0.7273, 0.7273, 0.7943]\n\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.7725 - loss: 4.8569 - val_accuracy: 0.3274 - val_loss: 1.9360 - learning_rate: 5.0000e-05\nEpoch 4/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8601 - loss: 4.8012\nEpoch 4: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.8602 - loss: 4.8008 - val_accuracy: 0.3638 - val_loss: 1.8679 - learning_rate: 5.0000e-05\nEpoch 5/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9139 - loss: 4.6595\nEpoch 5: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.9139 - loss: 4.6594 - val_accuracy: 0.3752 - val_loss: 1.8673 - learning_rate: 5.0000e-05\nRestoring model weights from the end of the best epoch: 1.\n\nUnfreezing group 3/3 (20 layers)\nEpoch 1/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6357 - loss: 5.1678\nEpoch 1: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.6358 - loss: 5.1674 - val_accuracy: 0.3160 - val_loss: 1.9465 - learning_rate: 3.3333e-05\nEpoch 2/5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7311 - loss: 4.9916\nEpoch 2: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7311 - loss: 4.9914 - val_accuracy: 0.3428 - val_loss: 1.8654 - learning_rate: 3.3333e-05\nEpoch 3/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8033 - loss: 4.8351\nEpoch 3: val_accuracy did not improve from 0.38853\n\nConfusion Matrix:\n[[ 80   2  64  12   4  20  71  32]\n [  0   4   0   0   0   0   0   0]\n [  1   1  20   0   0   1   2   3]\n [ 28   1  47  30   4  17  69  85]\n [ 41   5 122  12 155  23  57  64]\n [ 23   5  44  12   9  73  93  58]\n [ 32   2  70  19   7  29 130  28]\n [  8   0   9   7   0   5  17 163]]\nanger: 0.2807  contempt: 1.0000  disgust: 0.7143  fear: 0.1068  \nhappy: 0.3236  neutral: 0.2303  sad: 0.4101  surprise: 0.7799  \n\n\nClass Accuracy Trends:\nanger: [0.2491, 0.2596, 0.2912, 0.3368, 0.2807]\ncontempt: [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]\ndisgust: [0.7500, 0.7857, 0.6786, 0.7143, 0.7143]\nfear: [0.1601, 0.1957, 0.0961, 0.0961, 0.1068]\nhappy: [0.3215, 0.3486, 0.2150, 0.2965, 0.3236]\nneutral: [0.3060, 0.2934, 0.1672, 0.1798, 0.2303]\nsad: [0.4606, 0.4890, 0.4132, 0.4322, 0.4101]\nsurprise: [0.7321, 0.6459, 0.8421, 0.7847, 0.7799]\n\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.8034 - loss: 4.8350 - val_accuracy: 0.3489 - val_loss: 1.8643 - learning_rate: 3.3333e-05\nEpoch 4/5\n\u001b[1m380/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8708 - loss: 4.7437\nEpoch 4: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - accuracy: 0.8708 - loss: 4.7435 - val_accuracy: 0.3533 - val_loss: 1.8670 - learning_rate: 3.3333e-05\nEpoch 5/5\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9152 - loss: 4.6333\nEpoch 5: val_accuracy did not improve from 0.38853\n\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.9152 - loss: 4.6333 - val_accuracy: 0.3708 - val_loss: 1.8604 - learning_rate: 3.3333e-05\nRestoring model weights from the end of the best epoch: 1.\n\nEvaluating model on FER2013 test set\nFER2013 Test Accuracy: 0.3175\nFER2013 Weighted F1-Score: 0.3021\n\nFER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.31      0.29      0.30       958\n    contempt       0.40      1.00      0.57        54\n     disgust       0.06      0.77      0.12       111\n        fear       0.28      0.09      0.13      1024\n       happy       0.84      0.18      0.30      1774\n     neutral       0.52      0.15      0.23      1233\n         sad       0.34      0.46      0.39      1247\n    surprise       0.34      0.84      0.48       831\n\n    accuracy                           0.32      7232\n   macro avg       0.39      0.47      0.32      7232\nweighted avg       0.48      0.32      0.30      7232\n\n\nEvaluating model on Combined test set\nCombined Test Accuracy: 0.3431\nCombined Weighted F1-Score: 0.3168\n\nCombined Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.34      0.24      0.28      4126\n    contempt       0.39      0.02      0.04      2925\n     disgust       0.21      0.20      0.20      2588\n        fear       0.30      0.13      0.18      4200\n       happy       0.53      0.29      0.37      6818\n     neutral       0.36      0.61      0.46      6359\n         sad       0.25      0.40      0.31      4338\n    surprise       0.35      0.57      0.44      4870\n\n    accuracy                           0.34     36224\n   macro avg       0.34      0.31      0.29     36224\nweighted avg       0.36      0.34      0.32     36224\n\nFinal model saved to 'final_enhanced_emotion_model.keras'\n\n=== FINAL RESULTS ===\nAffectNet Test Accuracy: 0.4738\nAffectNet F1 Score: 0.4748\nFER2013 Test Accuracy: 0.3175\nFER2013 F1 Score: 0.3021\nCombined Test Accuracy: 0.3431\nCombined F1 Score: 0.3168\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Version 6v accuracy 0.2294\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 96  # Keep at 96x96 as specified\nBATCH_SIZE = 128\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Custom Label Smoothing Loss\n# =============================================================================\ndef label_smoothing_loss(epsilon=0.1):\n    \"\"\"\n    Cross entropy loss with label smoothing to prevent model from being too confident.\n    \n    Args:\n        epsilon: Smoothing factor (0 = no smoothing, 1 = complete smoothing)\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        \n        # Apply label smoothing\n        y_true = y_true * (1.0 - epsilon) + (epsilon / num_classes)\n        \n        # Calculate cross entropy with extra small epsilon to prevent log(0)\n        return -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7), axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# Fixed graph-compatible preprocessing function with stronger augmentation\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Fixed graph-compatible preprocessing with stronger augmentation.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize pixel values using MobileNet standard preprocessing\n    img = tf.cast(img, tf.float32)\n    img = img / 127.5 - 1.0  # Scale to [-1, 1]\n    \n    # Apply enhanced augmentation during training\n    if training:\n        # Random flip - works in graph mode\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced brightness and contrast (stronger than before)\n        img = tf.image.random_brightness(img, 0.3)  # Increased from 0.2\n        img = tf.image.random_contrast(img, 0.7, 1.3)  # Increased range\n        \n        # Add saturation variation\n        img = tf.image.random_saturation(img, 0.8, 1.5)\n        \n        # Apply random crop and resize for shape variation\n        # This simulates zoom/scale augmentation\n        crop_size = tf.random.uniform([], 0.8, 1.0, dtype=tf.float32)\n        scaled_size = tf.cast(tf.cast(tf.shape(img)[:2], tf.float32) * crop_size, tf.int32)\n        img = tf.image.random_crop(img, [scaled_size[0], scaled_size[1], 3])\n        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n            \n        # Add random noise to improve robustness\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.02)  # Increased noise\n        img = img + noise\n        \n        # Ensure values stay in valid range\n        img = tf.clip_by_value(img, -1.0, 1.0)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None):\n    \"\"\"\n    Creates a tf.data.Dataset with fixed preprocessing.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Repeat dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset\n    return create_dataset(balanced_df, is_training=is_training)\n\n# =============================================================================\n# Fixed Confusion Matrix Callback without plotting errors\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends instead of plotting them\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization (still useful)\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# Create emotion recognition model with additional MLP head\n# =============================================================================\ndef create_emotion_model(num_classes):\n    \"\"\"\n    Create a facial emotion recognition model with enhanced classification head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model and base model\n    \"\"\"\n    # Input shape\n    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n    \n    # Create input layer\n    inputs = keras.layers.Input(shape=input_shape)\n    \n    # Use MobileNetV2 as base\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs,\n        alpha=1.0  # Controls model width\n    )\n    print(\"Using MobileNetV2 base model\")\n    \n    # Freeze base model layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom head with dropout and batch normalization\n    x = base_model.output\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # First dense block - wider layers for better capacity\n    x = keras.layers.Dense(512)(x)  # Increased from 256\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.4)(x)\n    \n    # Second dense block\n    x = keras.layers.Dense(256)(x)  # Increased from 128\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    \n    # New third dense block for better capacity\n    x = keras.layers.Dense(128)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    \n    # Output layer with label smoothing\n    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with label smoothing loss\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.2),  # Increased from 0.1\n        metrics=['accuracy']\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Two-Stage Fine-Tuning with Progressive Unfreezing\n# =============================================================================\ndef train_with_progressive_unfreezing(model, base_model, train_ds, val_ds, \n                                    steps_per_epoch, val_steps, \n                                    epochs_head=10, epochs_finetune=20,\n                                    callbacks=None, class_weights=None):\n    \"\"\"\n    Two-stage training approach: first train only the head, then progressively unfreeze layers.\n    \n    Args:\n        model: The model to train\n        base_model: The base model part (for unfreezing)\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        epochs_head: Epochs for head-only training\n        epochs_finetune: Epochs for fine-tuning\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    print(f\"\\nStage 1: Training only the classification head ({epochs_head} epochs)\")\n    \n    # Ensure base model is frozen\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Compile with higher learning rate for head training\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.2),\n        metrics=['accuracy']\n    )\n    \n    # Train head only\n    history_head = model.fit(\n        train_ds,\n        epochs=epochs_head,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    print(f\"\\nStage 2: Fine-tuning with progressive unfreezing ({epochs_finetune} epochs)\")\n    \n    # Progressively unfreeze layers in groups\n    fine_tuning_history = []\n    \n    # Groups of layers to unfreeze (from last to first)\n    layer_groups = [\n        # Unfreeze last layers first (deeper = more specific features)\n        base_model.layers[-15:],  # Last block\n        base_model.layers[-30:-15],  # Second-to-last block\n        base_model.layers[-50:-30]   # Third-to-last block\n    ]\n    \n    for i, group in enumerate(layer_groups):\n        print(f\"\\nUnfreezing group {i+1}/{len(layer_groups)} ({len(group)} layers)\")\n        \n        # Unfreeze current group\n        for layer in group:\n            layer.trainable = True\n            \n        # Recompile with lower learning rate as we go deeper\n        lr = 1e-4 / (i + 1)  # Decrease learning rate for deeper layers\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=lr),\n            loss=label_smoothing_loss(epsilon=0.2),  # Keep consistent\n            metrics=['accuracy']\n        )\n        \n        # Train for a few epochs\n        epochs_per_group = max(5, epochs_finetune // len(layer_groups))\n        \n        history = model.fit(\n            train_ds,\n            epochs=epochs_per_group,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        \n        fine_tuning_history.append(history)\n    \n    # Return combined history\n    return history_head, fine_tuning_history\n\n# =============================================================================\n# Sequential Training Pipeline\n# =============================================================================\ndef train_enhanced_emotion_model(data_dir):\n    \"\"\"\n    Enhanced sequential training with all improvements.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced sequential emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes\n    print(\"\\n2. Creating enhanced data pipelines\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet')\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet')\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013')\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013')\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False)\n    \n    # 5. Calculate steps\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create enhanced model\n    print(\"\\n3. Creating enhanced model\")\n    model, base_model = create_emotion_model(num_classes)\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"]),\n        y=affectnet_train_df[\"label\"]\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"]), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"]),\n        y=fer_train_df[\"label\"]\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"]), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR,\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring (fixed version)\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring (fixed version)\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train on AffectNet using progressive unfreezing\n    print(\"\\n6. STAGE 1: Training on AffectNet with progressive unfreezing\")\n    \n    history_affectnet_head, history_affectnet_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        epochs_head=10, epochs_finetune=15,\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    model.save(\"affectnet_model.keras\")\n    print(\"AffectNet model saved to 'affectnet_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive unfreezing\n    print(\"\\n7. STAGE 2: Fine-tuning on FER2013 with progressive unfreezing\")\n    \n    history_fer_head, history_fer_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        epochs_head=8, epochs_finetune=12,\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    model.save(\"final_enhanced_emotion_model.keras\")\n    print(\"Final model saved to 'final_enhanced_emotion_model.keras'\")\n    \n    # Return models and metrics\n    return model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model with all improvements\n    model, metrics = train_enhanced_emotion_model(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:09:26.023941Z","iopub.execute_input":"2025-03-11T13:09:26.024293Z","iopub.status.idle":"2025-03-11T13:30:17.269744Z","shell.execute_reply.started":"2025-03-11T13:09:26.024264Z","shell.execute_reply":"2025-03-11T13:30:17.268766Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6.0z accuracy 0.1882 class balance deficit \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom sklearn.model_selection import train_test_split\n\n# =============================================================================\n# Enable memory growth and mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Enable mixed precision\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 96  # Keep at 96x96 as requested\nBATCH_SIZE = 128  # Moderate batch size\nEPOCHS = 50\nAUTOTUNE = tf.data.AUTOTUNE\n\n# =============================================================================\n# Custom Focal Loss Implementation\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Focal Loss implementation for multi-class classification.\n    Focal Loss is designed to address class imbalance by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter. Higher values mean more focus on hard examples.\n        alpha: Class weight factor. Higher values give more weight to minority classes.\n        \n    Returns:\n        Focal loss function\n    \"\"\"\n    def focal_loss_fn(y_true, y_pred):\n        epsilon = 1e-7\n        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n        \n        # Calculate cross entropy\n        cross_entropy = -y_true * K.log(y_pred)\n        \n        # Apply focal weight\n        weight = alpha * K.pow(1 - y_pred, gamma) * y_true\n        \n        # Sum over classes\n        focal_loss = K.sum(weight * cross_entropy, axis=-1)\n        return K.mean(focal_loss)\n    \n    return focal_loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# Data Augmentation for Minority Classes\n# =============================================================================\ndef augment_minority_classes(df, target_count=5000, minority_classes=None):\n    \"\"\"\n    Augment minority classes by duplicating samples to achieve more balanced class distribution.\n    \n    Args:\n        df: DataFrame with image paths and labels\n        target_count: Target number of samples per class\n        minority_classes: List of specific classes to augment (if None, determined automatically)\n        \n    Returns:\n        Augmented DataFrame\n    \"\"\"\n    print(\"Class distribution before augmentation:\")\n    print(df['label'].value_counts())\n    \n    if minority_classes is None:\n        # Identify classes with fewer than target_count samples\n        class_counts = df['label'].value_counts()\n        minority_classes = class_counts[class_counts < target_count].index.tolist()\n    \n    augmented_data = []\n    for cls in minority_classes:\n        class_df = df[df['label'] == cls]\n        needed = target_count - len(class_df)\n        if needed <= 0:\n            continue\n            \n        # Sample with replacement if needed\n        print(f\"Augmenting class '{cls}': Adding {needed} samples\")\n        samples = class_df.sample(n=needed, replace=True)\n        augmented_data.append(samples)\n    \n    # Combine augmented data with original\n    augmented_df = pd.concat([df] + augmented_data, ignore_index=True)\n    \n    print(\"Class distribution after augmentation:\")\n    print(augmented_df['label'].value_counts())\n    \n    return augmented_df\n\n# =============================================================================\n# Improved preprocessing function\n# =============================================================================\ndef preprocess_image(file_path, label, source):\n    \"\"\"\n    Unified preprocessing function with consistent augmentation for both datasets.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode with better error handling\n    try:\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n    except tf.errors.InvalidArgumentError:\n        try:\n            img = tf.image.decode_image(img, channels=1, expand_animations=False)\n            img = tf.image.grayscale_to_rgb(img)\n        except:\n            # Create a blank image if decoding fails\n            img = tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n            print(f\"Warning: Failed to decode image at {file_path}\")\n    \n    # Ensure the image has the right shape and type\n    img = tf.ensure_shape(img, [None, None, 3])\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize to [0, 1]\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Apply consistent augmentation for both datasets\n    if tf.random.uniform([], 0, 1) > 0.5:\n        # Standard horizontal flipping\n        img = tf.image.random_flip_left_right(img)\n        \n        # Color augmentations\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_saturation(img, 0.8, 1.2)\n        \n        # Slight rotation (maximum 15 degrees)\n        angle = tf.random.uniform([], -0.25, 0.25)  # in radians\n        img = tf.image.rot90(img, k=tf.cast(angle * 2 / 3.14159, tf.int32))\n    \n    # Convert label to one-hot encoding\n    label = tf.one_hot(label, depth=8)  # Assuming 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True):\n    \"\"\"\n    Create an optimized tf.data.Dataset from a DataFrame with fixed repeating.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether to apply augmentations and shuffling\n        \n    Returns:\n        tf.data.Dataset\n    \"\"\"\n    # Convert labels to indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset from file paths, labels, and sources\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing\n    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n    \n    if is_training:\n        # Training pipeline with augmentation\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Important: Always repeat the dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Confusion Matrix Callback\n# =============================================================================\nclass ConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Callback to display confusion matrix during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, freq=5):\n        super(ConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        \n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.freq == 0:\n            # Get a batch of validation data\n            validation_steps = 30  # Limit to prevent too much computation\n            y_true = []\n            y_pred = []\n            \n            # Get predictions for validation data\n            for i, (images, labels) in enumerate(self.validation_data):\n                if i >= validation_steps:\n                    break\n                batch_preds = self.model.predict(images, verbose=0)\n                y_pred.append(np.argmax(batch_preds, axis=1))\n                y_true.append(np.argmax(labels.numpy(), axis=1))\n            \n            # Flatten the lists\n            y_true = np.concatenate(y_true)\n            y_pred = np.concatenate(y_pred)\n            \n            # Calculate confusion matrix\n            cm = confusion_matrix(y_true, y_pred)\n            \n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Calculate per-class accuracy\n            class_acc = cm.diagonal() / cm.sum(axis=1)\n            for i, (name, acc) in enumerate(zip(self.class_names, class_acc)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print()\n\n# =============================================================================\n# Model Creation\n# =============================================================================\ndef create_simplified_model(num_classes):\n    \"\"\"\n    Create a simplified EfficientNetB0 model with a smaller head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    # Input layer\n    inputs = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    # Load pre-trained model with imagenet weights\n    base_model = EfficientNetB0(\n        include_top=False, \n        weights=\"imagenet\", \n        input_tensor=inputs\n    )\n    \n    # Initially freeze all layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom classification head\n    x = base_model.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    \n    # Ensure final layer uses float32 for numerical stability\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    \n    # Create model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with focal loss\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\"accuracy\"]\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Main Training Function\n# =============================================================================\ndef train_emotion_recognition_model(data_dir):\n    \"\"\"\n    Complete training pipeline incorporating all improvements.\n    \n    Args:\n        data_dir: Path to the dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting improved facial emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading dataset\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    print(f\"Original training data: {train_df_full.shape}\")\n    print(f\"Test data: {test_df.shape}\")\n    \n    # 2. Apply class balancing through augmentation\n    print(\"\\n2. Balancing class distribution\")\n    train_df_balanced = augment_minority_classes(train_df_full, target_count=5000)\n    \n    # 3. Split into train and validation sets\n    print(\"\\n3. Creating train/validation split\")\n    train_df, val_df = train_test_split(\n        train_df_balanced, \n        test_size=0.15, \n        stratify=train_df_balanced[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n    \n    # 4. Create fixed tf.data datasets\n    print(\"\\n4. Creating data pipelines\")\n    train_ds, class_indices = create_dataset(train_df, is_training=True)\n    val_ds, _ = create_dataset(val_df, is_training=False)\n    test_ds, _ = create_dataset(test_df, is_training=False)\n    \n    # Get class names in order\n    classes = sorted(train_df[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # Calculate steps\n    steps_per_epoch = len(train_df) // BATCH_SIZE\n    validation_steps = len(val_df) // BATCH_SIZE\n    \n    # 5. Create model\n    print(\"\\n5. Creating simplified model\")\n    model, base_model = create_simplified_model(num_classes)\n    print(\"Model created\")\n    \n    # 6. Calculate proper class weights\n    print(\"\\n6. Computing class weights\")\n    class_weights_array = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(train_df[\"label\"]),\n        y=train_df[\"label\"]\n    )\n    class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(train_df[\"label\"]), class_weights_array)}\n    print(\"Class weights:\", class_weights)\n    \n    # 7. Setup callbacks\n    print(\"\\n7. Setting up training callbacks\")\n    callbacks = [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'best_emotion_model.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=7,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # Logging\n        tf.keras.callbacks.CSVLogger('training_log.csv', append=True),\n        # Confusion matrix\n        ConfusionMatrixCallback(val_ds, classes, freq=3)\n    ]\n    \n    # 8. Progressive training approach\n    print(\"\\n8. Stage 1: Training only the model head\")\n    history_stage1 = model.fit(\n        train_ds,\n        epochs=10,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    # 9. Fine-tune the upper layers\n    print(\"\\n9. Stage 2: Fine-tuning upper layers\")\n    # Unfreeze the top layers of the base model\n    for layer in base_model.layers[-30:]:\n        layer.trainable = True\n        \n    # Recompile with a lower learning rate\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\"accuracy\"]\n    )\n    \n    history_stage2 = model.fit(\n        train_ds,\n        epochs=20,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    # 10. Evaluate on test set\n    print(\"\\n10. Final evaluation\")\n    # Update test steps\n    test_steps = len(test_df) // BATCH_SIZE\n    \n    # Get predictions\n    all_predictions = []\n    all_labels = []\n    \n    # Loop through batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= test_steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        all_predictions.append(np.argmax(batch_preds, axis=1))\n        all_labels.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    all_predictions = np.concatenate(all_predictions)\n    all_labels = np.concatenate(all_labels)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(all_predictions == all_labels)\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    \n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Weighted F1-Score: {f1:.4f}\")\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        all_labels, \n        all_predictions, \n        target_names=classes,\n        zero_division=0\n    ))\n    \n    # Print confusion matrix\n    cm = confusion_matrix(all_labels, all_predictions)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # 11. Save the final model\n    model.save(\"final_improved_emotion_model.keras\")\n    print(\"Model saved to 'final_improved_emotion_model.keras'\")\n    \n    return model, {'accuracy': test_accuracy, 'f1_score': f1}\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model\n    model, metrics = train_emotion_recognition_model(data_dir)\n    \n    print(\"Training completed successfully!\")\n    print(f\"Final Test Accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"Final F1 Score: {metrics['f1_score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:28:45.447003Z","iopub.execute_input":"2025-03-10T23:28:45.447275Z","iopub.status.idle":"2025-03-10T23:39:01.046153Z","shell.execute_reply.started":"2025-03-10T23:28:45.447252Z","shell.execute_reply":"2025-03-10T23:39:01.045323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6.0 accuracy 0.1832 \n\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\nfrom sklearn.model_selection import train_test_split\n\n# Enable memory growth to prevent TF from allocating all GPU memory at once\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Enable mixed precision for faster training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# Memory cleanup callback\nclass MemoryCleanupCallback(keras.callbacks.Callback):\n    \"\"\"Callback to clear TensorFlow session after each epoch to free memory.\"\"\"\n    def on_epoch_end(self, epoch, logs=None):\n        tf.keras.backend.clear_session()\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nIMG_SIZE = 96  # Unified image size for both datasets\nBATCH_SIZE = 32  # Start with a conservative value\nEPOCHS = 30\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Try to determine optimal batch size based on available GPU memory\n# Start with default batch size\noptimal_batch_size = BATCH_SIZE\n\n# Try to detect available GPU memory and adjust batch size\ntry:\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        # Get GPU memory info if possible (works on some systems)\n        gpu_details = tf.config.experimental.get_device_details(gpus[0])\n        if 'memory_limit' in gpu_details:\n            # Memory in bytes, convert to GB\n            gpu_memory_gb = gpu_details['memory_limit'] / (1024**3)\n            \n            # Simple heuristic: 1GB supports batch size of ~16 for this model\n            if gpu_memory_gb > 14:  # High-end GPU (16GB+)\n                optimal_batch_size = 128\n            elif gpu_memory_gb > 7:  # Mid-range GPU (8GB)\n                optimal_batch_size = 64\n            else:  # Lower memory GPU\n                optimal_batch_size = 32\n                \n            print(f\"Detected {gpu_memory_gb:.1f}GB GPU memory, setting batch size to {optimal_batch_size}\")\n        else:\n            # If we can't detect memory, try a reasonable default for Kaggle\n            optimal_batch_size = 64\n            print(f\"Could not detect GPU memory, using default batch size of {optimal_batch_size}\")\nexcept Exception:\n    # If anything fails, stay with the default\n    print(f\"Using default batch size of {optimal_batch_size}\")\n\n# Update batch size to optimal value\nBATCH_SIZE = optimal_batch_size\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure - Keeping your original function\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")\ntest_dir = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# Add data augmentation specifically for underrepresented classes\ndef augment_minority_classes(df, target_count=5000, minority_classes=None):\n    if minority_classes is None:\n        # Identify classes with fewer than target_count samples\n        class_counts = df['label'].value_counts()\n        minority_classes = class_counts[class_counts < target_count].index.tolist()\n    \n    augmented_data = []\n    for cls in minority_classes:\n        class_df = df[df['label'] == cls]\n        needed = target_count - len(class_df)\n        if needed <= 0:\n            continue\n            \n        # Sample with replacement if needed\n        samples = class_df.sample(n=needed, replace=True)\n        augmented_data.append(samples)\n    \n    # Combine augmented data with original\n    return pd.concat([df] + augmented_data, ignore_index=True)\n\n# Use this before train/val split\ntrain_df_full = augment_minority_classes(train_df_full)\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(\n    train_df_full, \n    test_size=0.2, \n    stratify=train_df_full[\"label\"], \n    random_state=42\n)\n\n# Get class names and create mapping\nclasses = sorted(train_df_full[\"label\"].unique())\nclass_indices = {cls: i for i, cls in enumerate(classes)}\nnum_classes = len(classes)\n\n# =============================================================================\n# 4. Create an optimized tf.data pipeline\n# =============================================================================\ndef preprocess_image(file_path, label, source):\n    \"\"\"\n    Unified preprocessing function that handles both FER2013 and AffectNet images.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode with error handling\n    try:\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n    except:\n        img = tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    # Ensure shape and resize\n    img = tf.ensure_shape(img, [None, None, 3])\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Apply consistent preprocessing for both datasets\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Apply the same augmentation regardless of source\n    if tf.random.uniform([], 0, 1) > 0.5:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=num_classes)\n    \n    return img, label\n\ndef create_dataset(dataframe, is_training=True): # , cache=True\n    \"\"\"\n    Create an optimized tf.data.Dataset from a DataFrame.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether to apply augmentations\n        cache: Whether to cache the dataset (disable for very large datasets)\n        \n    Returns:\n        tf.data.Dataset\n    \"\"\"\n    # Convert labels to indices\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing and batching\n    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n    \n    if is_training:\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Make sure to repeat the dataset for multiple epochs\n    ds = ds.repeat()  # This is crucial\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds\n\n# Create datasets\ntrain_ds = create_dataset(train_df, is_training=True)\nval_ds = create_dataset(val_df, is_training=False)\ntest_ds = create_dataset(test_df, is_training=False)\n\n# Calculate steps\nsteps_per_epoch = len(train_df) // BATCH_SIZE\nvalidation_steps = min(len(val_df) // BATCH_SIZE, 100)  # Limit validation steps\n\n# Sanity check - inspect a batch to verify the dataset pipeline\ndef inspect_dataset(dataset, name):\n    \"\"\"Inspect a dataset to verify it's created correctly.\"\"\"\n    print(f\"\\nInspecting {name} dataset:\")\n    \n    try:\n        # Get one batch\n        for images, labels in dataset.take(1):\n            print(f\"  Batch shape: {images.shape}\")\n            print(f\"  Labels shape: {labels.shape}\")\n            print(f\"  Data type: {images.dtype}\")\n            print(f\"  Min/Max values: {tf.reduce_min(images).numpy():.4f}/{tf.reduce_max(images).numpy():.4f}\")\n            \n            # Check for NaNs\n            has_nans = tf.math.reduce_any(tf.math.is_nan(images))\n            print(f\"  Contains NaNs: {has_nans.numpy()}\")\n            \n            # Verify one-hot labels\n            label_sums = tf.reduce_sum(labels, axis=1)\n            all_ones = tf.reduce_all(tf.equal(label_sums, 1))\n            print(f\"  Labels are valid one-hot: {all_ones.numpy()}\")\n            \n            # All checks passed\n            print(f\"  ✅ {name} dataset looks good!\")\n            return True\n    except Exception as e:\n        print(f\"  ❌ Error inspecting {name} dataset: {str(e)}\")\n        return False\n\n# Run sanity checks\ntrain_ok = inspect_dataset(train_ds, \"Training\")\nval_ok = inspect_dataset(val_ds, \"Validation\")\ntest_ok = inspect_dataset(test_ds, \"Test\")\n\n# Abort if datasets are not created correctly\nif not (train_ok and val_ok and test_ok):\n    print(\"\\n⚠️ Dataset sanity check failed! Please check the error messages above.\")\n    print(\"You can continue but training might fail.\")\n\n# =============================================================================\n# 5. Compute Class Weights to handle imbalance\n# =============================================================================\nclass_weights_array = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(train_df[\"label\"]),\n    y=train_df[\"label\"]\n)\nclass_weights = {class_indices[label]: weight for label, weight in \n                 zip(np.unique(train_df[\"label\"]), class_weights_array)}\n\n# =============================================================================\n# 6. Model Architecture with Optimized Transfer Learning\n# =============================================================================\ndef create_model():\n    \"\"\"\n    Create an EfficientNetB0 model with frozen layers and custom top.\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    # Create input layer with the correct shape\n    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    # Load pre-trained model with imagenet weights\n    base_model = EfficientNetB0(\n        include_top=False, \n        weights=\"imagenet\", \n        input_tensor=input_tensor\n    )\n    \n    # Freeze the first 70% of layers\n    freeze_until = int(len(base_model.layers) * 0.7)\n    for layer in base_model.layers[:freeze_until]:\n        layer.trainable = False\n    \n    # Add custom classification head\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n    x = Dropout(0.4)(x)\n    # Ensure final layer uses float32 for numerical stability with softmax\n    output = Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    \n    # Create model\n    model = Model(inputs=input_tensor, outputs=output)\n    \n    # Compile with Adam optimizer\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n# =============================================================================\n# 7. Learning Rate Scheduler and Callbacks\n# =============================================================================\ndef cosine_decay_schedule(epoch, lr):\n    \"\"\"\n    Cosine decay learning rate schedule.\n    \n    Args:\n        epoch: Current epoch\n        lr: Current learning rate\n        \n    Returns:\n        New learning rate\n    \"\"\"\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(math.pi * epoch / EPOCHS)) / 2\n\n# Create callbacks\ncallbacks = [\n    # Save checkpoints (using proper file extension for weights)\n    keras.callbacks.ModelCheckpoint(\n        'best_model.weights.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        save_weights_only=True,  # Save only weights to reduce I/O\n        verbose=1\n    ),\n    # Early stopping\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    # Learning rate scheduling\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    # Log training metrics\n    keras.callbacks.CSVLogger('training_log.csv', append=True),\n    # Memory cleanup after each epoch\n    MemoryCleanupCallback()\n\n]\n\n# =============================================================================\n# 8. Training\n# =============================================================================\ndef train_model(custom_callbacks=None, existing_model=None, quick_test=False):\n    \"\"\"\n    Train the model with the optimized pipeline.\n    \n    Args:\n        custom_callbacks: Additional callbacks to use during training\n        existing_model: Continue training this model if provided\n        quick_test: Whether this is a quick test run\n        \n    Returns:\n        Trained model and history\n    \"\"\"\n    # Create new model or use existing one\n    if existing_model is None:\n        model = create_model()\n        # Print model summary\n        model.summary()\n    else:\n        model = existing_model\n        print(\"Continuing training with existing model\")\n    \n    # Use custom callbacks if provided, otherwise use default callbacks\n    training_callbacks = custom_callbacks if custom_callbacks else callbacks\n    \n    # Adjust epochs and steps for quick test\n    current_epochs = 2 if quick_test else EPOCHS\n    current_steps = min(20, steps_per_epoch) if quick_test else steps_per_epoch\n    current_val_steps = min(10, validation_steps) if quick_test else validation_steps\n    \n    if quick_test:\n        print(f\"Quick test mode: {current_epochs} epochs, {current_steps} steps/epoch\")\n    \n    # Train model\n    history = model.fit(\n        train_ds,\n        epochs=current_epochs,\n        steps_per_epoch=current_steps,\n        validation_data=val_ds,\n        validation_steps=current_val_steps,\n        callbacks=training_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    return model, history\n\n# =============================================================================\n# 9. LR Finder - Fixed version with termination condition\n# =============================================================================\nclass LRFinder(keras.callbacks.Callback):\n    \"\"\"\n    Learning rate finder callback.\n    \n    This callback helps find the optimal learning rate by exponentially\n    increasing the learning rate during training and recording the loss.\n    \"\"\"\n    def __init__(self, min_lr=1e-7, max_lr=1e-2, steps=100, max_batches=1000):\n        super(LRFinder, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.steps = steps\n        self.max_batches = max_batches  # Safety limit\n        self.lrs = []\n        self.losses = []\n        self.batch_counter = 0\n        \n    def on_train_begin(self, logs=None):\n        # Store original learning rate\n        self.original_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        # Set initial learning rate to minimum\n        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.min_lr)\n        self.optimizer = self.model.optimizer\n    \n    def on_train_batch_end(self, batch, logs=None):\n        # Get current learning rate\n        lr = tf.keras.backend.get_value(self.optimizer.learning_rate)\n        self.lrs.append(lr)\n        self.losses.append(logs.get('loss'))\n        \n        # Calculate new learning rate\n        new_lr = lr * (self.max_lr / self.min_lr) ** (1/self.steps)\n        tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n        \n        # Increment counter and check for termination\n        self.batch_counter += 1\n        if batch >= self.steps or self.batch_counter >= self.max_batches:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs=None):\n        # Restore original learning rate\n        tf.keras.backend.set_value(self.optimizer.learning_rate, self.original_lr)\n        \n    def plot_lr_finder(self):\n        \"\"\"Plot the learning rate finder results.\"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            \n            plt.figure(figsize=(10, 6))\n            plt.plot(self.lrs, self.losses)\n            plt.xscale('log')\n            plt.xlabel('Learning Rate')\n            plt.ylabel('Loss')\n            plt.title('Learning Rate Finder')\n            plt.savefig('lr_finder_results.png')\n        except ImportError:\n            print(\"Matplotlib not available for plotting. Saving results to CSV instead.\")\n            import csv\n            with open('lr_finder_results.csv', 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(['learning_rate', 'loss'])\n                for lr, loss in zip(self.lrs, self.losses):\n                    writer.writerow([lr, loss])\n\n# =============================================================================\n# 10. Training with LR Finder\n# =============================================================================\ndef find_optimal_lr():\n    \"\"\"\n    Find the optimal learning rate using the LR Finder.\n    \n    Returns:\n        Suggested learning rate\n    \"\"\"\n    # Create model\n    model = create_model()\n    \n    # Create LR Finder callback\n    lr_finder = LRFinder(min_lr=1e-7, max_lr=1e-2, steps=100, max_batches=100)\n    \n    # Fit model for a few batches to find optimal LR\n    model.fit(\n        train_ds,\n        epochs=1,\n        steps_per_epoch=100,\n        callbacks=[lr_finder],\n        verbose=1\n    )\n    \n    # Plot results\n    lr_finder.plot_lr_finder()\n    \n    # Find the learning rate with the steepest negative gradient\n    losses = lr_finder.losses\n    lrs = lr_finder.lrs\n    \n    # Smoothing\n    smooth_losses = []\n    for i in range(len(losses)):\n        if i < 2 or i >= len(losses) - 2:\n            smooth_losses.append(losses[i])\n        else:\n            smooth_losses.append(sum(losses[i-2:i+3]) / 5)\n    \n    # Calculate gradients\n    gradients = []\n    for i in range(1, len(smooth_losses)):\n        gradients.append((smooth_losses[i] - smooth_losses[i-1]) / (lrs[i] - lrs[i-1]))\n    \n    # Find the point with the steepest negative gradient\n    steepest_idx = np.argmin(gradients)\n    optimal_lr = lrs[steepest_idx + 1] / 10  # Division by 10 is common practice\n    \n    print(f\"Suggested learning rate: {optimal_lr:.2e}\")\n    return optimal_lr\n\n# =============================================================================\n# 11. Evaluation\n# =============================================================================\ndef evaluate_model(model):\n    \"\"\"\n    Evaluate the model on the test set.\n    \n    Args:\n        model: Trained Keras model\n        \n    Returns:\n        Evaluation metrics\n    \"\"\"\n    # Evaluate model\n    loss, accuracy = model.evaluate(test_ds)\n    print(f\"\\nTest Loss: {loss:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    \n    # Get predictions\n    predictions = np.argmax(model.predict(test_ds), axis=-1)\n    \n    # Get true labels\n    true_labels = []\n    for _, y in test_ds.unbatch():\n        true_labels.append(np.argmax(y.numpy()))\n    true_labels = np.array(true_labels)\n    \n    # Calculate F1 score\n    f1 = f1_score(true_labels, predictions, average='weighted')\n    print(f\"\\nWeighted F1-Score: {f1:.4f}\")\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        true_labels, \n        predictions, \n        target_names=classes,\n        zero_division=0\n    ))\n    \n    return {\n        'loss': loss,\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'predictions': predictions,\n        'true_labels': true_labels\n    }\n\n# =============================================================================\n# 12. Fine-tune Model\n# =============================================================================\ndef fine_tune_model(model, epochs=5, custom_callbacks=None, quick_test=False):\n    \"\"\"\n    Fine-tune the model by unfreezing all layers.\n    \n    Args:\n        model: Trained model\n        epochs: Number of fine-tuning epochs\n        custom_callbacks: Additional callbacks to use during fine-tuning\n        quick_test: Whether this is a quick test run\n        \n    Returns:\n        Fine-tuned model and history\n    \"\"\"\n    # Unfreeze all layers\n    for layer in model.layers:\n        layer.trainable = True\n    \n    # Recompile with a lower learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    # Use custom callbacks if provided, otherwise use default callbacks\n    training_callbacks = custom_callbacks if custom_callbacks else callbacks\n    \n    # Adjust epochs and steps for quick test\n    current_epochs = 2 if quick_test else epochs\n    current_steps = min(20, steps_per_epoch) if quick_test else steps_per_epoch\n    current_val_steps = min(10, validation_steps) if quick_test else validation_steps\n    \n    if quick_test:\n        print(f\"Quick fine-tuning: {current_epochs} epochs, {current_steps} steps/epoch\")\n    \n    # Fine-tune\n    history = model.fit(\n        train_ds,\n        epochs=current_epochs,\n        steps_per_epoch=current_steps,\n        validation_data=val_ds,\n        validation_steps=current_val_steps,\n        callbacks=training_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    return model, history\n\n# =============================================================================\n# 13. Performance Monitoring\n# =============================================================================\nclass PerformanceMonitor(keras.callbacks.Callback):\n    \"\"\"\n    Monitor and log training performance metrics like time per step.\n    \"\"\"\n    def __init__(self):\n        super(PerformanceMonitor, self).__init__()\n        self.batch_times = []\n        self.epoch_start_time = None\n        \n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start_time = time.time()\n        self.batch_start_time = time.time()\n        self.batch_times = []\n        \n    def on_batch_end(self, batch, logs=None):\n        batch_time = time.time() - self.batch_start_time\n        self.batch_times.append(batch_time)\n        self.batch_start_time = time.time()\n        \n        # Log every 50 batches\n        if batch % 50 == 0:\n            avg_time = sum(self.batch_times[-50:]) / min(50, len(self.batch_times))\n            print(f\"\\nBatch {batch} - Avg time: {avg_time*1000:.2f}ms/step\")\n            \n            # Try to get GPU memory info if available\n            try:\n                import subprocess\n                gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader']).decode('utf-8')\n                print(f\"GPU Memory: {gpu_info.strip()}\")\n            except:\n                pass\n        \n    def on_epoch_end(self, epoch, logs=None):\n        epoch_time = time.time() - self.epoch_start_time\n        avg_batch_time = sum(self.batch_times) / len(self.batch_times)\n        print(f\"\\nEpoch {epoch+1} completed in {epoch_time:.2f}s - Avg: {avg_batch_time*1000:.2f}ms/step\")\n\n# =============================================================================\n# 14. Main training loop\n# =============================================================================\ndef main():\n    \"\"\"Main function to run the training pipeline.\"\"\"\n    # Print TF and GPU info\n    print(f\"TensorFlow version: {tf.__version__}\")\n    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n    \n    # Print dataset info\n    print(f\"Train samples: {len(train_df)} ({len(train_df[train_df['source'] == 'fer2013'])} FER, {len(train_df[train_df['source'] == 'affectnet'])} AffectNet)\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n    print(f\"Classes: {classes}\")\n    print(f\"Steps per epoch: {steps_per_epoch}\")\n    print(f\"Batch size: {BATCH_SIZE}\")\n    \n    # Set to True to run a quick test first, or False for full training\n    run_quick_test = True  # Change this manually if needed\n    \n    if run_quick_test:\n        print(\"\\nRunning quick test (2 epochs with limited steps)...\")\n        # Define variables for testing\n        quick_test_epochs = 2\n        quick_test_steps = min(20, steps_per_epoch)\n        print(f\"Quick test settings: {quick_test_epochs} epochs, {quick_test_steps} steps per epoch\")\n    \n    # Create performance monitor\n    perf_monitor = PerformanceMonitor()\n    \n    # Add performance monitor to callbacks\n    training_callbacks = callbacks + [perf_monitor]\n    \n    try:\n        # Find optimal learning rate (optional)\n        # optimal_lr = find_optimal_lr()\n        \n        # Train model (with quick test if selected)\n        print(\"\\n=== Starting initial training phase ===\")\n        \n        if run_quick_test:\n            # Run a quick test first\n            print(\"\\n=== Running quick test ===\")\n            model, quick_history = train_model(\n                custom_callbacks=training_callbacks,\n                quick_test=True\n            )\n            \n            # After successful quick test, continue with full training\n            print(\"\\n=== Quick test complete, continuing with full training ===\")\n            model, history = train_model(\n                custom_callbacks=training_callbacks,\n                existing_model=model\n            )\n        else:\n            # Full training from the start\n            model, history = train_model(custom_callbacks=training_callbacks)\n            \n            # Evaluate model after initial training\n            print(\"\\n=== Evaluating after initial training ===\")\n            metrics = evaluate_model(model)\n            \n            # Fine-tune model (with quick test mode if enabled)\n            print(\"\\n=== Starting fine-tuning phase ===\")\n            model, ft_history = fine_tune_model(\n                model, \n                epochs=5, \n                custom_callbacks=training_callbacks,\n                quick_test=run_quick_test\n            )\n        \n        # Final evaluation\n        print(\"\\n=== Final evaluation ===\")\n        final_metrics = evaluate_model(model)\n        \n        # Save the final model\n        model.save(\"final_emotion_model.keras\")\n        \n        print(\"\\nTraining complete! Final model saved as 'final_emotion_model.keras'\")\n        \n    except Exception as e:\n        import traceback\n        print(\"\\n*** ERROR DURING TRAINING ***\")\n        print(traceback.format_exc())\n        print(\"\\nDetailed error:\", str(e))\n        \n        # Try to save the model if it exists\n        try:\n            if 'model' in locals():\n                print(\"Attempting to save the model before exiting...\")\n                model.save(\"emergency_save_model.keras\")\n                print(\"Model saved as emergency_save_model.keras\")\n        except Exception as save_error:\n            print(f\"Failed to save model: {save_error}\")\n    \nif __name__ == \"__main__\":\n    import time\n    start_time = time.time()\n    main()\n    total_time = time.time() - start_time\n    print(f\"\\nTotal execution time: {total_time/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:42:26.281033Z","iopub.execute_input":"2025-03-10T22:42:26.281407Z","iopub.status.idle":"2025-03-10T23:28:28.958149Z","shell.execute_reply.started":"2025-03-10T22:42:26.281380Z","shell.execute_reply":"2025-03-10T23:28:28.956816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 5.0 orginal working code slightly\n\nimport os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\ntf.get_logger().setLevel(logging.INFO)\n\nimport gc\nclass MemoryCleanup(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n        tf.keras.backend.clear_session()\n\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nimg_size = 96         # We upscale FER images to 96x96\nbatch_size = 64\nepochs = 30\n\n# =============================================================================\n# Learning Rate Finder Callback (Fixed)\n# =============================================================================\nclass LRFinder(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr=1e-6, max_lr=1e-2, steps=100):\n        super(LRFinder, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.steps = steps\n        self.lrs = []\n        self.losses = []\n        \n    def on_train_begin(self, logs=None):\n        # Use optimizer.learning_rate (compatible with LossScaleOptimizer)\n        self.original_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.min_lr)\n        self.optimizer = self.model.optimizer\n    \n    def on_batch_end(self, batch, logs=None):\n        lr = tf.keras.backend.get_value(self.optimizer.learning_rate)\n        self.lrs.append(lr)\n        self.losses.append(logs.get('loss'))\n        new_lr = lr * (self.max_lr / self.min_lr) ** (1/self.steps)\n        tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n        if batch >= self.steps:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs=None):\n        tf.keras.backend.set_value(self.optimizer.learning_rate, self.original_lr)\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory (Train or Test) and returns a DataFrame with columns:\n      - filepath: full path to the image file\n      - label: the emotion (parent folder name)\n      - source: the subfolder name (e.g., fer2013 or affectnet)\n    Assumes directory structure: root_dir/emotion/subfolder/image.jpg\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")   # Case-sensitive!\ntest_dir  = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# Additional Plots: Separate by Source (Optional)\nfer_df = train_df_full[train_df_full[\"source\"] == \"fer2013\"]\naff_df = train_df_full[train_df_full[\"source\"] == \"affectnet\"]\n\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x=\"label\", data=fer_df, palette=\"coolwarm\")\nplt.title(\"FER2013 Training Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.countplot(x=\"label\", data=aff_df, palette=\"Spectral\")\nplt.title(\"AffectNet Training Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(train_df_full, test_size=0.2, stratify=train_df_full[\"label\"], random_state=42)\n\n# =============================================================================\n# X. Visualization Code for DataFrames\n# =============================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Seaborn style for a beautiful design.\nsns.set(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n\n# Plot 1: Training Data (Combined by Source)\nplt.figure(figsize=(16, 5))\nplt.subplot(1, 3, 1)\nsns.countplot(x=\"label\", hue=\"source\", data=train_df_full, palette=\"viridis\")\nplt.title(\"Training Data Volume (Combined)\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Source\")\n\n# Plot 2: Validation Data (Combined by Source)\nplt.subplot(1, 3, 2)\nif \"source\" in val_df.columns:\n    sns.countplot(x=\"label\", hue=\"source\", data=val_df, palette=\"viridis\")\n    plt.legend(title=\"Source\")\nelse:\n    sns.countplot(x=\"label\", data=val_df, palette=\"viridis\")\nplt.title(\"Validation Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\n# Plot 3: Test Data\nplt.subplot(1, 3, 3)\nif \"source\" in test_df.columns:\n    sns.countplot(x=\"label\", hue=\"source\", data=test_df, palette=\"viridis\")\n    plt.legend(title=\"Source\")\nelse:\n    sns.countplot(x=\"label\", data=test_df, palette=\"viridis\")\nplt.title(\"Test Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# =============================================================================\n# 4. Source-Specific Augmentation Functions\n# =============================================================================\ndef apply_fer_augmentations(img):\n    # Gentle augmentations for low-res FER2013 images\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, max_delta=0.1)\n    return img\n\ndef apply_affectnet_augmentations(img):\n    # Stronger augmentations for high-res AffectNet images\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, max_delta=0.3)\n    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n    return img\n\ndef augment_based_on_source(img, source):\n    # Source-aware augmentation: assume img is a tensor in [0,1]\n    if source == \"fer2013\":\n        img = tf.image.rgb_to_grayscale(img)  # Ensure it's grayscale\n        img = apply_fer_augmentations(img)\n    else:\n        img = apply_affectnet_augmentations(img)\n    return img\n\n# =============================================================================\n# 5. Create Separate Generators for FER2013 and AffectNet\n# =============================================================================\n# Create a simpler, more robust training loop\n# First, simplify your dataset creation\n\nclasses = sorted(train_df_full[\"label\"].unique())\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255\n    #preprocessing_function=lambda x: np.repeat(x, 3, axis=-1) if x.shape[-1] == 1 else x\n)\n\nsimple_train_gen = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"rgb\",  # Use rgb for all images to simplify\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=16,  # Reduced batch size\n    shuffle=True\n)\n\n# Split the training DataFrame by source\nfer_train_df = train_df[train_df[\"source\"] == \"fer2013\"]\naff_train_df = train_df[train_df[\"source\"] == \"affectnet\"]\n\n# For FER2013: load as grayscale with gentle augmentation.\nfer_aug_params = {\n    \"rescale\": 1./255,\n    \"rotation_range\": 15,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"brightness_range\": [0.8, 1.2]\n}\nfer_datagen = ImageDataGenerator(**fer_aug_params)\nfer_gen = fer_datagen.flow_from_dataframe(\n    dataframe=fer_train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=64,\n    shuffle=True\n)\n\n# For AffectNet: load as RGB with stronger augmentation.\naff_aug_params = {\n    \"rescale\": 1./255,\n    \"rotation_range\": 30,  # Reduced from 40\n    \"brightness_range\": [0.7, 1.3],  # Reduced from [0.5, 1.5]\n    \"zoom_range\": 0.1  # Reduced from 0.2\n}\naff_datagen = ImageDataGenerator(**aff_aug_params)\naff_gen = aff_datagen.flow_from_dataframe(\n    dataframe=aff_train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=64,\n    shuffle=True\n)\n\n#######################################################################################\n# If your images are grayscale and need to be converted to RGB, add this:\ndef convert_grayscale_to_rgb(batch_x, batch_y):\n    # If the last dimension is 1, repeat it 3 times\n    if batch_x.shape[-1] == 1:\n        batch_x = np.repeat(batch_x, 3, axis=-1)\n    return batch_x, batch_y\n\n# Create a wrapper for your generator\ndef rgb_generator_wrapper(gen):\n    for batch_x, batch_y in gen:\n        yield convert_grayscale_to_rgb(batch_x, batch_y)\n\n# Use the wrapped generator\nrgb_train_gen = rgb_generator_wrapper(simple_train_gen)\n\n#######################################################################################\n\n# Calculate steps for each dataset.\nsteps_fer = math.ceil(len(fer_train_df) / 64)  # 22986/32=719\nsteps_aff = math.ceil(len(aff_train_df) / 64)  # 23209/32=726\nsteps_per_epoch = steps_fer + steps_aff        # 1445\n\n#steps_fer = math.ceil(fer_train_df.shape[0] / 32)\n#steps_aff = math.ceil(aff_train_df.shape[0] / 32)\n#steps_per_epoch = steps_fer + steps_aff\n#print(f\"Steps per epoch: {steps_per_epoch}\")\n\n# After splitting in Section 3:\nprint(f\"Train samples: {len(train_df)}\")  # Should be ~36k (80% of 46k)\nprint(f\"FER samples: {len(fer_train_df)}\")  # ~18k (80% of 23k)\nprint(f\"AffectNet samples: {len(aff_train_df)}\")  # ~18k (80% of 23k)\n\n# =============================================================================\n# 6. Enhanced Upscaling for FER2013: Bicubic interpolation and channel replication.\n# =============================================================================\ndef preprocess_fer_batch(batch):\n    # Input: (batch_size, H, W, 1)\n    upscaled = tf.image.resize(batch, [img_size, img_size], method=\"bicubic\")\n    return tf.repeat(upscaled, repeats=3, axis=-1)  # Now shape: (batch_size, img_size, img_size, 3)\n\n# Wrap the FER generator into a tf.data.Dataset.\ndef fer_gen_wrapper():\n    for batch in fer_gen:\n        images, labels = batch\n        images = tf.convert_to_tensor(images)\n        images = preprocess_fer_batch(images)\n        yield (images, labels)\n\nds_fer = tf.data.Dataset.from_generator(\n    fer_gen_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(fer_gen.class_indices)), dtype=tf.float32)\n    )\n)\n\n# For AffectNet, simply convert batches to tensors.\ndef aff_gen_wrapper():\n    for batch in aff_gen:\n        images, labels = batch\n        images = tf.convert_to_tensor(images)\n        yield (images, labels)\n\nds_aff = tf.data.Dataset.from_generator(\n    aff_gen_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(aff_gen.class_indices)), dtype=tf.float32)\n    )\n)\n\n# =============================================================================\n# 7. Combine the Two Datasets and Optimize Pipeline\n# =============================================================================\n# Both ds_fer and ds_aff already yield batches, so do NOT apply an additional .batch() here.\ncombined_ds = ds_fer.concatenate(ds_aff)\ncombined_ds = combined_ds.shuffle(buffer_size=200).prefetch(2)\n\n#combined_ds = ds_fer.concatenate(ds_aff)\n#combined_ds = combined_ds.shuffle(500).prefetch(tf.data.AUTOTUNE)\n\n# =============================================================================\n# 8. Compute Class Weights\n# =============================================================================\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(train_df[\"label\"]),\n    y=train_df[\"label\"]\n)\nlabel_to_index = fer_gen.class_indices  # Assumes consistency across sources.\nclass_weights = {label_to_index[label]: weight for label, weight in zip(np.unique(train_df[\"label\"]), class_weights)}\n\n# =============================================================================\n# 9. Model Architecture\n# =============================================================================\n# (Optional) Enable mixed precision\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\n\n#from tensorflow.keras.applications import MobileNetV2\n#base_model = MobileNetV2(include_top=False, weights=\"imagenet\", \n#                        input_tensor=input_tensor,\n#                        input_shape=(img_size, img_size, 3))\n\ninput_tensor = Input(shape=(img_size, img_size, 3))\nbase_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=input_tensor)\nbase_model.trainable = True\n\nfreeze_until = int(len(base_model.layers) * 0.7)\nfor layer in base_model.layers[:freeze_until]:\n    layer.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.5)(x)\nx = Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = Dropout(0.4)(x)\noutput = Dense(len(classes), activation=\"softmax\", dtype=\"float32\")(x)\nmodel = Model(inputs=input_tensor, outputs=output)\n\n# =============================================================================\n# 10. Focal Loss (Optional)\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy\n        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n    return loss_fn\n\nloss_function = focal_loss()  # Or use \"categorical_crossentropy\"\n\nsimple_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(len(classes), activation='softmax')\n])\n\nsimple_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss=loss_function, #'categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# =============================================================================\n# 11. Training Configuration & Callbacks\n# =============================================================================\n\ndef cosine_annealing(epoch, lr):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(math.pi * epoch / epochs)) / 2\n\n# Minimal callback list\nminimal_callbacks = [\n    tf.keras.callbacks.ModelCheckpoint('checkpoint.keras', save_best_only=True),\n    tf.keras.callbacks.CSVLogger('simple_training_log.csv', append=True)\n]\n\n# Create a validation dataset from val_df using a similar pipeline for FER images.\nval_datagen = ImageDataGenerator(rescale=1./255)\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",\n    classes=classes,\n    batch_size=batch_size,\n    shuffle=False\n)\ndef val_gen_wrapper():\n    for batch in val_generator:\n        images, labels = batch\n        images = preprocess_fer_batch(images)\n        yield (images, labels)\n\nds_val = tf.data.Dataset.from_generator(\n    lambda: val_gen_wrapper(),\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(classes)), dtype=tf.float32)\n    )\n).prefetch(tf.data.AUTOTUNE)\n\nstart_time = time.time()\nprint(\"Starting simplified training...\")\nsimple_history = simple_model.fit(\n    simple_train_gen,\n    epochs=2,\n    steps_per_epoch=50,  # Drastically reduced to test\n    validation_data=ds_val, #val_generator,\n    validation_steps=10,  # Also reduced\n    callbacks=minimal_callbacks\n)\nprint(\"Simplified training completed!\")\n\n# Save the trained model\nmodel.save(\"final_emotion_model.keras\")\n\n# =============================================================================\n# 12. Evaluation & Visualization on Test Data\n# =============================================================================\n# First, create a clean test generator without any preprocessing\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",  # Load as grayscale first\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=batch_size,\n    shuffle=False\n)\n\n# Then create a simpler wrapper that ensures exactly 3 channels\ndef test_wrapper():\n    for batch_x, batch_y in test_generator:\n        # Convert grayscale to RGB once - explicitly tracking shape\n        if batch_x.shape[-1] == 1:\n            # This creates exactly 3 channels\n            batch_x_rgb = np.concatenate([batch_x, batch_x, batch_x], axis=-1)\n            yield batch_x_rgb, batch_y\n        else:\n            yield batch_x, batch_y\n\n# Create the dataset with proper output signature\ntest_ds = tf.data.Dataset.from_generator(\n    test_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(classes)), dtype=tf.float32)\n    )\n).batch(batch_size).prefetch(2)\n\n# Now evaluate with this clean dataset\nloss, accuracy = simple_model.evaluate(test_ds)\n\nprint(f\"\\nTest Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\npredictions = np.argmax(simple_model.predict(test_generator), axis=-1)\ntrue_labels = test_generator.classes\n\nprint(f\"\\nWeighted F1-Score: {f1_score(true_labels, predictions, average='weighted'):.4f}\")\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_matrix(true_labels, predictions), \n            annot=True, fmt=\"d\", \n            cmap=\"Blues\",\n            xticklabels=test_generator.class_indices.keys(),\n            yticklabels=test_generator.class_indices.keys())\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, predictions, target_names=test_generator.class_indices.keys()))\n\nplt.figure(figsize=(14, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Curves\")\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\nplt.title(\"Loss Curves\")\nplt.show()\n\n# =============================================================================\n# 13. Testing Saved Model on Individual Test Images\n# =============================================================================\nsaved_model = keras.models.load_model(\"final_emotion_model.keras\", compile=False)\nclass_labels = list(test_generator.class_indices.keys())\n\nfor class_name in class_labels:\n    class_path = os.path.join(test_dir, class_name)\n    if os.path.exists(class_path):\n        test_images = os.listdir(class_path)\n        print(f\"\\nTesting images for class: {class_name}\")\n        for img_name in test_images[:5]:\n            img_path = os.path.join(class_path, img_name)\n            # Load as grayscale then convert to RGB.\n            img = keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size), color_mode=\"grayscale\")\n            img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n            img_array = np.repeat(img_array, 3, axis=-1)\n            img_array = np.expand_dims(img_array, axis=0)\n            \n            prediction = saved_model.predict(img_array)\n            predicted_class = class_labels[np.argmax(prediction)]\n            confidence = np.max(prediction)\n            \n            plt.imshow(img_array[0].astype(\"float32\"))\n            plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n            plt.axis(\"off\")\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:39:44.181466Z","iopub.execute_input":"2025-03-10T21:39:44.181807Z","iopub.status.idle":"2025-03-10T21:44:00.496987Z","shell.execute_reply.started":"2025-03-10T21:39:44.181778Z","shell.execute_reply":"2025-03-10T21:44:00.495881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T06:04:56.087843Z","iopub.execute_input":"2025-01-21T06:04:56.088139Z","iopub.status.idle":"2025-01-21T06:04:56.443115Z","shell.execute_reply.started":"2025-01-21T06:04:56.088111Z","shell.execute_reply":"2025-01-21T06:04:56.442405Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fer_dirs = glob.glob(os.path.join(train_dir, \"*\", \"fer2013\"))\naff_dirs = glob.glob(os.path.join(train_dir, \"*\", \"affectnet\"))\nprint(\"FER directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"fer2013\")))\nprint(\"AffectNet directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"affectnet\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:53:25.033801Z","iopub.execute_input":"2025-02-01T10:53:25.034111Z","iopub.status.idle":"2025-02-01T10:53:25.061789Z","shell.execute_reply.started":"2025-02-01T10:53:25.034089Z","shell.execute_reply":"2025-02-01T10:53:25.060926Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW Experimental\n\nimport tensorflow.keras.backend as K\nimport os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0, MobileNetV3Small\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nimg_size = 96         # We upscale FER images to 96x96\nbatch_size = 64       # Smaller batch size to avoid memory issues\nepochs = 20           # Increase epochs, we'll use early stopping\nmodel_type = \"efficientnet\"  # Options: \"efficientnet\", \"mobilenet\", \"ensemble\"\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory (Train or Test) and returns a DataFrame with columns:\n      - filepath: full path to the image file\n      - label: the emotion (parent folder name)\n      - source: the subfolder name (e.g., fer2013 or affectnet)\n    Assumes directory structure: root_dir/emotion/subfolder/image.jpg\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\n# Adjust this path to your actual data directory\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")\ntest_dir  = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(train_df_full, test_size=0.2, stratify=train_df_full[\"label\"], random_state=42)\n\n# =============================================================================\n# 4. Data Visualization (Optional)\n# =============================================================================\ndef plot_data_distribution():\n    # Plot distribution of emotions by source\n    plt.figure(figsize=(16, 6))\n    \n    # Combined\n    plt.subplot(1, 3, 1)\n    sns.countplot(x=\"label\", data=train_df_full, palette=\"viridis\")\n    plt.title(\"Full Dataset Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    # FER2013\n    plt.subplot(1, 3, 2)\n    sns.countplot(x=\"label\", data=train_df_full[train_df_full[\"source\"] == \"fer2013\"], palette=\"coolwarm\")\n    plt.title(\"FER2013 Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    # AffectNet\n    plt.subplot(1, 3, 3)\n    sns.countplot(x=\"label\", data=train_df_full[train_df_full[\"source\"] == \"affectnet\"], palette=\"Spectral\")\n    plt.title(\"AffectNet Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Uncomment to visualize data distribution\n# plot_data_distribution()\n\n# =============================================================================\n# 5. Optimized Data Generators\n# =============================================================================\n# Get emotion classes\nclasses = sorted(train_df_full[\"label\"].unique())\nprint(f\"Classes: {classes}\")\n\n# Define preprocessing functions\ndef preprocess_fer(img):\n    \"\"\"Convert grayscale to RGB and resize to target size\"\"\"\n    # Convert to 3 channels by repeating the grayscale channel\n    img = tf.image.grayscale_to_rgb(img)\n    # Resize to target size\n    img = tf.image.resize(img, [img_size, img_size], method='bicubic')\n    return img / 255.0  # Normalize\n\ndef preprocess_affectnet(img):\n    \"\"\"Resize RGB image to target size\"\"\"\n    img = tf.image.resize(img, [img_size, img_size])\n    return img / 255.0  # Normalize\n\n# Create training data generators\ndef create_generators():\n    # Common augmentation parameters\n    common_aug = {\n        'horizontal_flip': True,\n        'rotation_range': 20,\n        'fill_mode': 'nearest'\n    }\n    \n    # For FER2013 (grayscale)\n    fer_datagen = ImageDataGenerator(\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        brightness_range=[0.8, 1.2],\n        **common_aug\n    )\n    \n    fer_train_gen = fer_datagen.flow_from_dataframe(\n        dataframe=train_df[train_df[\"source\"] == \"fer2013\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    # For AffectNet (RGB)\n    aff_datagen = ImageDataGenerator(\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        brightness_range=[0.7, 1.3],\n        **common_aug\n    )\n    \n    aff_train_gen = aff_datagen.flow_from_dataframe(\n        dataframe=train_df[train_df[\"source\"] == \"affectnet\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    # Validation generators - no augmentation\n    val_datagen = ImageDataGenerator()\n    \n    fer_val_gen = val_datagen.flow_from_dataframe(\n        dataframe=val_df[val_df[\"source\"] == \"fer2013\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    aff_val_gen = val_datagen.flow_from_dataframe(\n        dataframe=val_df[val_df[\"source\"] == \"affectnet\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    return fer_train_gen, aff_train_gen, fer_val_gen, aff_val_gen\n\n# Create test generator\ndef create_test_generator():\n    test_datagen = ImageDataGenerator()\n    \n    # Split test data by source\n    fer_test_df = test_df[test_df[\"source\"] == \"fer2013\"]\n    aff_test_df = test_df[test_df[\"source\"] == \"affectnet\"]\n    \n    fer_test_gen = test_datagen.flow_from_dataframe(\n        dataframe=fer_test_df,\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    aff_test_gen = test_datagen.flow_from_dataframe(\n        dataframe=aff_test_df,\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    return fer_test_gen, aff_test_gen\n\nfer_train_gen, aff_train_gen, fer_val_gen, aff_val_gen = create_generators()\nfer_test_gen, aff_test_gen = create_test_generator()\n\n# =============================================================================\n# 6. Custom Data Generator that combines both sources\n# =============================================================================\nclass CombinedGenerator:\n    def __init__(self, fer_gen, aff_gen, batch_size=32):\n        self.fer_gen = fer_gen\n        self.aff_gen = aff_gen\n        self.batch_size = batch_size\n        self.n_classes = len(classes)\n        self.fer_samples = len(fer_gen.filenames)\n        self.aff_samples = len(aff_gen.filenames)\n        self.total_samples = self.fer_samples + self.aff_samples\n        \n    def __len__(self):\n        return (self.total_samples + self.batch_size - 1) // self.batch_size\n    \n    def __iter__(self):\n        self.fer_iter = iter(self.fer_gen)\n        self.aff_iter = iter(self.aff_gen)\n        return self\n    \n    def __next__(self):\n        # Randomly choose which generator to pull from based on dataset size ratio\n        if np.random.random() < self.fer_samples / self.total_samples:\n            try:\n                batch_x, batch_y = next(self.fer_iter)\n                # Convert grayscale to RGB\n                if batch_x.shape[-1] == 1:\n                    batch_x = np.repeat(batch_x, 3, axis=-1)\n                return batch_x, batch_y\n            except StopIteration:\n                self.fer_iter = iter(self.fer_gen)\n                return next(self)\n        else:\n            try:\n                return next(self.aff_iter)\n            except StopIteration:\n                self.aff_iter = iter(self.aff_gen)\n                return next(self)\n\n# Create combined generators\ntrain_gen = CombinedGenerator(fer_train_gen, aff_train_gen, batch_size)\nval_gen = CombinedGenerator(fer_val_gen, aff_val_gen, batch_size)\ntest_gen = CombinedGenerator(fer_test_gen, aff_test_gen, batch_size)\n\n# =============================================================================\n# 7. Compute Class Weights for imbalanced dataset\n# =============================================================================\ndef compute_class_weights(train_df):\n    # Compute balanced class weights\n    class_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(train_df[\"label\"]),\n        y=train_df[\"label\"]\n    )\n    \n    # Map class names to indices\n    label_to_index = {label: i for i, label in enumerate(classes)}\n    \n    # Create dictionary of class weights\n    weights_dict = {label_to_index[label]: weight \n                    for label, weight in zip(np.unique(train_df[\"label\"]), class_weights)}\n    \n    return weights_dict\n\nclass_weights = compute_class_weights(train_df)\nprint(\"Class weights:\", class_weights)\n\n# =============================================================================\n# 8. Model Architecture\n# =============================================================================\ndef create_model(model_type=\"efficientnet\"):\n    input_tensor = Input(shape=(img_size, img_size, 3))\n    \n    if model_type == \"efficientnet\":\n        # EfficientNetB0 without any extra preprocessing\n        base_model = EfficientNetB0(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        # Freeze early layers\n        for layer in base_model.layers[:100]:\n            layer.trainable = False\n        \n        x = base_model.output\n        \n    elif model_type == \"mobilenet\":\n        # MobileNetV3Small\n        base_model = MobileNetV3Small(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        # Freeze early layers\n        for layer in base_model.layers[:50]:\n            layer.trainable = False\n        \n        x = base_model.output\n        \n    elif model_type == \"ensemble\":\n        # Use both models\n        efficient_base = EfficientNetB0(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        mobile_base = MobileNetV3Small(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        \n        # Freeze early layers\n        for layer in efficient_base.layers[:100]:\n            layer.trainable = False\n        for layer in mobile_base.layers[:50]:\n            layer.trainable = False\n        \n        # Global pooling for both models\n        efficient_features = GlobalAveragePooling2D()(efficient_base.output)\n        mobile_features = GlobalAveragePooling2D()(mobile_base.output)\n        \n        # Concatenate features\n        x = Concatenate()([efficient_features, mobile_features])\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n    \n    # Common head for all models\n    if model_type != \"ensemble\":\n        x = GlobalAveragePooling2D()(x)\n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    # Output layer\n    outputs = Dense(len(classes), activation=\"softmax\")(x)\n    \n    model = Model(inputs=input_tensor, outputs=outputs)\n\n    # Compile model with a fixed learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n# Simplify model to verify training works\nsimple_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(len(classes), activation='softmax')\n])\n\nsimple_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# =============================================================================\n# 9. Training with callbacks\n# =============================================================================\n\ncheckpoint_path = os.path.join(\"checkpoints\", f\"{model_type}_emotion_model.keras\")\nos.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n\ndef train_model(model, train_generator, val_generator, epochs=20, class_weights=None):\n    # Better learning rate schedule\n    steps_per_epoch = len(train_generator)\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-3,\n        decay_steps=steps_per_epoch * epochs,\n        alpha=1e-5\n    )\n\n    # Update the model's optimizer with the new learning rate schedule\n    model.optimizer.learning_rate = lr_schedule  # Direct assignment\n\n    checkpoint_path_keras = checkpoint_path + \".keras\"\n\n    # Callbacks\n    callbacks = [\n        # Save best model\n        ModelCheckpoint(\n            checkpoint_path,\n            save_weights_only=False,\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1\n        ),\n        # Early stopping\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            patience=7,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Reduce learning rate when plateau\n        ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=3,\n            min_lr=1e-6,\n            verbose=1\n        )\n    ]\n\n    # Calculate steps per epoch and validation steps\n    steps_per_epoch = len(train_generator)\n    validation_steps = len(val_generator)\n\n    print(f\"Steps per epoch: {steps_per_epoch}\")\n    print(f\"Validation steps: {validation_steps}\")\n\n    # Train model\n    start_time = time.time()\n    try:\n        history = model.fit(\n            train_generator,\n            epochs=epochs,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_generator,\n            validation_steps=validation_steps,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n    except Exception as e:\n        print(f\"Training error: {str(e)}\")\n        return None\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    return history\n\n# =============================================================================\n# 10. Evaluation\n# =============================================================================\ndef evaluate_model(model, test_generator):\n    # Calculate steps for test data\n    test_steps = len(test_generator)\n    \n    # Evaluate model\n    test_loss, test_acc = model.evaluate(test_generator, steps=test_steps)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    \n    # Get predictions\n    y_pred_probs = model.predict(test_generator, steps=test_steps)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    \n    # Get true labels\n    y_true = []\n    for i in range(test_steps):\n        try:\n            _, batch_y = next(iter(test_generator))\n            y_true.extend(np.argmax(batch_y, axis=1))\n        except StopIteration:\n            break\n    \n    # Limit to same size\n    y_true = y_true[:len(y_pred)]\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        y_true, \n        y_pred, \n        target_names=classes\n    ))\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt=\"d\", \n        cmap=\"Blues\",\n        xticklabels=classes,\n        yticklabels=classes\n    )\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.show()\n    \n    return y_true, y_pred\n\n# =============================================================================\n# 11. Visualization Functions\n# =============================================================================\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# =============================================================================\n# 12. Test on sample images\n# =============================================================================\ndef test_on_samples(model, num_samples=3):\n    plt.figure(figsize=(16, 12))\n    \n    # Get sample images from each class\n    for i, emotion in enumerate(classes):\n        # Get sample paths\n        sample_paths = []\n        fer_samples = test_df[(test_df['label'] == emotion) & (test_df['source'] == 'fer2013')]['filepath'].values\n        aff_samples = test_df[(test_df['label'] == emotion) & (test_df['source'] == 'affectnet')]['filepath'].values\n        \n        if len(fer_samples) > 0:\n            sample_paths.append(fer_samples[0])\n        if len(aff_samples) > 0:\n            sample_paths.append(aff_samples[0])\n        \n        # Limit to num_samples\n        sample_paths = sample_paths[:num_samples]\n        \n        for j, img_path in enumerate(sample_paths):\n            # Load and preprocess image\n            color_mode = 'grayscale' if 'fer2013' in img_path else 'rgb'\n            img = keras.preprocessing.image.load_img(\n                img_path, \n                target_size=(img_size, img_size),\n                color_mode=color_mode\n            )\n            img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n            \n            # Convert grayscale to RGB if needed\n            if img_array.shape[-1] == 1:\n                img_array = np.repeat(img_array, 3, axis=-1)\n            \n            # Add batch dimension\n            img_batch = np.expand_dims(img_array, axis=0)\n            \n            # Predict\n            predictions = model.predict(img_batch)\n            predicted_class = classes[np.argmax(predictions[0])]\n            confidence = np.max(predictions[0]) * 100\n            \n            # Plot\n            plt_idx = i * num_samples + j + 1\n            if plt_idx <= num_samples * len(classes):\n                plt.subplot(len(classes), num_samples, plt_idx)\n                plt.imshow(img_array)\n                plt.title(f\"True: {emotion}\\nPred: {predicted_class}\\nConf: {confidence:.1f}%\")\n                plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# =============================================================================\n# MAIN EXECUTION\n# =============================================================================\ndef main():\n    # Debug information\n    print(\"Starting main function\")\n    print(f\"Model type: {model_type}\")\n    \n    try:\n        # Create and train model\n        model = create_model(model_type)\n        print(f\"Created {model_type} model\")\n    \n        # Print model summary\n        model.summary()\n    \n        # Train model\n        history = train_model(\n            model, \n            train_gen, \n            val_gen, \n            epochs=epochs, \n            class_weights=class_weights\n        )\n    \n        # Plot training history\n        plot_training_history(history)\n        \n        # Evaluate model\n        y_true, y_pred = evaluate_model(model, test_gen)\n        \n        # Test on sample images\n        test_on_samples(model)\n        \n        # Save final model\n        model.save(f\"final_{model_type}_emotion_model.keras\")\n        print(f\"Model saved as final_{model_type}_emotion_model.keras\")\n        \n        return model, history\n        \n    except Exception as e:\n        print(f\"Error in main function: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nif __name__ == \"__main__\":\n    # Set random seeds for reproducibility\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    \n    # Mixed precision for faster training\n    try:\n        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n        tf.keras.mixed_precision.set_global_policy(policy)\n        print(\"Using mixed precision\")\n    except:\n        print(\"Mixed precision not available\")\n    \n    # Train model\n    model, history = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:28:38.296917Z","iopub.execute_input":"2025-03-05T05:28:38.297226Z","iopub.status.idle":"2025-03-05T05:29:36.161782Z","shell.execute_reply.started":"2025-03-05T05:28:38.297203Z","shell.execute_reply":"2025-03-05T05:29:36.160926Z"}},"outputs":[],"execution_count":null}]}
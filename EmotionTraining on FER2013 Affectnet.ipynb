{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":7984578,"sourceType":"datasetVersion","datasetId":4699957},{"sourceId":10634054,"sourceType":"datasetVersion","datasetId":6583945}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ResNet18 + MobileNetV2 + EfficientNetB0\n# folcalloss is good for fer2013\n\n!pip install tensorflow\n!pip install numpy\n!pip install matplotlib\n!pip install psutil\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.2 (Handling specific Test directory structure)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\nimport math\nimport glob # Needed for finding files\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path\n    \"TRAIN_DIR\": \"Train\", # Combined train dir name\n    \"TEST_DIR\": \"Test\",   # Base test dir name\n    \"IMG_SIZE\": 224,\n    \"BATCH_SIZE\": 64,\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE,\n    \"EPOCHS_HEAD\": 15,\n    \"EPOCHS_FINE_TUNE\": 30,\n    \"LR_HEAD\": 1e-3,\n    \"LR_FINE_TUNE_START\": 1e-4,\n    \"DROPOUT_RATE\": 0.4,\n    \"NUM_CLASSES\": 8, \n    \"MODEL_ARCH_1\": \"EfficientNetB0\",\n    \"MODEL_ARCH_2\": \"EfficientNetB4\",\n    \"ENSEMBLE_WEIGHTS\": [0.5, 0.5],\n    \"FINE_TUNE_LAYERS_B0\": 10,\n    \"FINE_TUNE_LAYERS_B4\": 15,\n    \"LOG_DIR_BASE\": \"logs/fit/\"\n}\n\n# Set Seed\ntf.random.set_seed(CONFIG[\"SEED\"])\nnp.random.seed(CONFIG[\"SEED\"])\n\n# =============================================================================\n# GPU Configuration & Mixed Precision (Same as before)\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\nelse:\n    print(\"No GPU detected. Running on CPU.\")\n\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\nprint(\"Mixed precision enabled ('mixed_float16')\")\n\n# =============================================================================\n# Data Loading (Train/Validation - Standard; Test - Custom)\n# =============================================================================\n\ndef create_train_val_tf_dataset(directory, image_size, batch_size, validation_split=0.2):\n    \"\"\"Creates tf.data.Dataset for training and validation using image_dataset_from_directory.\"\"\"\n    print(f\"Loading training/validation data from: {directory}\")\n\n    # Create the training dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=True,\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"training\",\n    )\n\n    # Create the validation dataset\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=False, # No need to shuffle validation\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"validation\",\n    )\n\n    # Get class names from the training dataset BEFORE optimization\n    class_names = train_ds.class_names\n    print(f\"Dataset loaded with classes: {class_names}\")\n\n    # Configure performance for both datasets\n    train_ds = train_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n    val_ds = val_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return train_ds, val_ds, class_names\n\n# --- NEW: Function to load test data from the specific structure ---\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size):\n    \"\"\"\n    Creates a tf.data.Dataset for testing by manually finding files in the Test/<emotion>/<target_dataset> structure.\n    \"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\")\n    all_image_paths = []\n    all_labels = []\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    if not emotion_dirs:\n         print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\")\n         return None\n\n    print(f\"Found emotion folders: {emotion_dirs}\")\n\n    for emotion in emotion_dirs:\n        if emotion not in class_names_map:\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\")\n            continue\n\n        label_index = class_names_map[emotion] # Get the integer label\n        target_path = os.path.join(base_test_dir, emotion, target_dataset)\n\n        if not tf.io.gfile.exists(target_path):\n             print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\")\n             continue # Skip if the specific dataset subdir doesn't exist for this emotion\n\n        # Find all image files (adjust extensions if needed)\n        image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpeg'))\n\n        if not image_files:\n             print(f\"Warning: No image files found in '{target_path}'.\")\n             continue\n\n        all_image_paths.extend(image_files)\n        all_labels.extend([label_index] * len(image_files))\n        print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\")\n\n    if not all_image_paths:\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\")\n        return None\n\n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\")\n\n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=CONFIG[\"NUM_CLASSES\"])) # Convert labels to one-hot\n    image_ds = path_ds.map(lambda x: load_and_preprocess_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Combine images and labels\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n\n    # Batch and prefetch\n    test_ds = image_label_ds.batch(batch_size).prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return test_ds\n\n# --- NEW: Helper function to load and preprocess images from paths ---\ndef load_and_preprocess_image(path, image_size):\n    \"\"\"Loads and preprocesses a single image file.\"\"\"\n    image = tf.io.read_file(path)\n    image = tf.image.decode_image(image, channels=3, expand_animations=False) # Decode any format\n    image = tf.image.resize(image, [image_size, image_size], method='nearest')\n    image.set_shape((image_size, image_size, 3))\n    # Rescaling: EfficientNet generally handles this internally, but if needed:\n    # image = image / 255.0\n    return image\n\n# --- Create Datasets ---\n# Training and Validation Data\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"])\ntrain_ds, val_ds, class_names = create_train_val_tf_dataset(\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n)\n\n# Create a mapping from class name to integer index for test set loading\nclass_names_map = {name: i for i, name in enumerate(class_names)}\n\n# Test Datasets (using the new custom function)\nbase_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"])\naffectnet_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\nfer2013_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\n\n\n# =============================================================================\n# Class Weights Calculation (Same as V3.1, check if needs adjustment for 8 classes)\n# =============================================================================\ndef get_class_weights(dataset, class_names_list):\n    print(\"Calculating class weights...\")\n    all_labels = []\n    num_batches = tf.data.experimental.cardinality(dataset)\n    print(f\"Approximate number of batches in training dataset: {num_batches}\")\n\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY:\n         print(\"Warning: Cannot determine dataset cardinality accurately. Iterating...\")\n         for _, labels_batch in dataset: # Iterate batches\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n         if not all_labels:\n             print(\"Error: Could not extract labels. Using uniform weights.\")\n             return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    else:\n        for _, labels_batch in dataset:\n            all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n\n    unique_classes, counts = np.unique(all_labels, return_counts=True)\n    print(f\"Unique labels found for weight calculation: {unique_classes} with counts {counts}\")\n\n    if len(unique_classes) == 0:\n        print(\"Error: No labels found. Using uniform weights.\")\n        return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=unique_classes,\n        y=all_labels\n    )\n\n    class_weights_dict = {i: 0.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    for i, cls_label in enumerate(unique_classes):\n        if cls_label < CONFIG[\"NUM_CLASSES\"]:\n             class_weights_dict[cls_label] = class_weights[i]\n        else:\n             print(f\"Warning: Label {cls_label} >= NUM_CLASSES ({CONFIG['NUM_CLASSES']}). Ignoring.\")\n\n    for i in range(CONFIG[\"NUM_CLASSES\"]):\n        if class_weights_dict[i] == 0.0 and i in unique_classes: # Check if it was actually missing or just had 0 weight\n            print(f\"Warning: Class {i} ({class_names_list[i]}) had 0 weight initially. Assigning 1.0.\")\n            class_weights_dict[i] = 1.0 # Avoid 0 weight if class exists but wasn't found / calculated\n        elif class_weights_dict[i] == 0.0:\n             print(f\"Info: Class {i} ({class_names_list[i]}) not found in iterated samples. Assigning weight 1.0.\")\n             class_weights_dict[i] = 1.0\n\n\n    print(\"Class weights calculated:\", class_weights_dict)\n    return class_weights_dict\n\nclass_weights = get_class_weights(train_ds, class_names)\n\n# =============================================================================\n# Data Augmentation Layer (Same as before)\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]),\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building (Same as before - ensure NUM_CLASSES is correct)\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate):\n    input_shape = (img_size, img_size, 3)\n    inputs = Input(shape=input_shape, name=\"input_layer\")\n    x = data_augmentation(inputs) # Apply augmentation first\n\n    if model_arch == \"EfficientNetB0\":\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # Let EffNet handle input scaling\n        # Apply augmentation *after* potential base model preprocessing if needed, but usually before is fine.\n        x_processed = base_model(x, training=False) # Pass augmented data to base model\n    elif model_arch == \"EfficientNetB4\":\n         print(f\"Warning: Building {model_arch} with image size {img_size}.\")\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg')\n         x_processed = base_model(x, training=False) # Pass augmented data to base model\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\")\n\n    base_model.trainable = False # Freeze base model\n\n    # Add classification head\n    output = Dropout(dropout_rate, name=\"top_dropout\")(x_processed) # Use output from base model\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(output)\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch)\n    print(f\"{model_arch} model built successfully.\")\n    return model\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\n\n# =============================================================================\n# Training Functions (Mostly same as V3.1, adjust fine-tune layer freezing logic)\n# =============================================================================\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0):\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name\n    checkpoint_path = f\"best_{model.name}.keras\"\n\n    # --- Callbacks ---\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=1)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1)\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback]\n    optimizer_choice = tf.keras.optimizers.Adam\n\n    # Find the base model layer to control its trainability\n    base_model_layer = None\n    for layer in model.layers:\n        if layer.name.startswith(\"efficientnet\"): # Find the actual base model layer by name\n            base_model_layer = layer\n            break\n    if base_model_layer is None and fine_tune:\n        print(\"Warning: Could not automatically find base model layer for fine-tuning.\")\n\n    # --- Compile Step ---\n    if fine_tune:\n        print(f\"Setting up fine-tuning for {model.name}...\")\n        if base_model_layer:\n            base_model_layer.trainable = True # Unfreeze the base model layer\n            # Fine-tune only the top 'fine_tune_layers' layers within the base model\n            num_base_layers = len(base_model_layer.layers)\n            freeze_until = num_base_layers - fine_tune_layers\n            print(f\"Unfreezing top {fine_tune_layers} layers of {base_model_layer.name} (out of {num_base_layers}). Freezing up to layer {freeze_until}.\")\n            if freeze_until < 0: freeze_until = 0\n\n            for layer in base_model_layer.layers[:freeze_until]:\n                 # Keep Batch Norm layers frozen, as is often recommended during fine-tuning\n                 # to prevent destabilization from small batch statistics.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                      layer.trainable = False\n                 else:\n                      # You might choose to freeze all layers up to this point\n                      layer.trainable = False\n                      pass # Or set layer.trainable = False if you want to be strict\n\n            for layer in base_model_layer.layers[freeze_until:]:\n                 # Keep Batch Norm frozen here too for consistency? Experiment needed.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                     layer.trainable = False\n                 else:\n                     layer.trainable = True # Unfreeze the top layers\n        else: # Fallback if base model layer not found\n             model.trainable = True # Unfreeze everything if base layer not identified\n\n        learning_rate_schedule = learning_rate # Use starting LR for fine-tune\n        callbacks.append(reduce_lr)\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n    else: # Head training\n        print(f\"Setting up head training for {model.name}.\")\n        if base_model_layer:\n            base_model_layer.trainable = False # Ensure base is frozen\n        else:\n             print(\"Warning: Could not find base model layer to freeze for head training.\")\n\n        # Cosine Decay for head training\n        train_cardinality = tf.data.experimental.cardinality(train_dataset)\n        if train_cardinality == tf.data.experimental.UNKNOWN_CARDINALITY:\n            print(\"Warning: Unknown training steps for CosineDecay. Estimating.\")\n            try: # Estimate steps\n                 steps_per_epoch = len(train_ds.list_files(os.path.join(train_dir,'*/*'))) // CONFIG[\"BATCH_SIZE\"]\n            except: steps_per_epoch = 1000 # Fallback\n            total_steps = steps_per_epoch * epochs\n        else:\n            total_steps = train_cardinality.numpy() * epochs\n\n        warmup_steps = int(total_steps * 0.1)\n        learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay(\n             initial_learning_rate=learning_rate, decay_steps=max(1, total_steps - warmup_steps), alpha=0.0\n        )\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n        # callbacks.append(reduce_lr) # Optionally add ReduceLR here too\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    print(\"\\n--- Model Summary After Compilation ---\")\n    model.summary(line_length=120) # Print summary after compilation and setting trainability\n\n    print(f\"\\n--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} (Epochs {initial_epoch} to {initial_epoch+epochs}) ---\")\n    history = model.fit(\n        train_dataset,\n        epochs=initial_epoch + epochs, # End epoch\n        validation_data=validation_dataset,\n        class_weight=class_weights_dict,\n        callbacks=callbacks,\n        initial_epoch=initial_epoch, # Start epoch\n        verbose=1\n    )\n    return history, model\n\n\n# =============================================================================\n# Evaluation Function (Adapting to potentially missing classes in test sets)\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating {model.name} on {dataset_name} ---\")\n    results = model.evaluate(test_dataset, verbose=1)\n    print(f\"{model.name} {dataset_name} Test Loss: {results[0]:.4f}\")\n    print(f\"{model.name} {dataset_name} Test Accuracy: {results[1]:.4f}\")\n\n    y_pred_probs = model.predict(test_dataset)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in evaluation data.\")\n         return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": 0}\n\n\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"{model.name} {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"{model.name} {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'{model.name} {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"{model.name}_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1}\n\n# =============================================================================\n# Main Training Pipeline (Adjusted epoch handling)\n# =============================================================================\n\nprint(f\"\\nClass names being used: {class_names}\")\nprint(f\"Number of classes for models: {CONFIG['NUM_CLASSES']}\")\nif len(class_names) != CONFIG['NUM_CLASSES']:\n     print(f\"Warning: Number of classes found ({len(class_names)}) differs from CONFIG['NUM_CLASSES'] ({CONFIG['NUM_CLASSES']})\")\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\")\nhistory1_head, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\")\nhistory2_head, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\n# Load best weights from head training before fine-tuning\nprint(\"\\n--- Loading best weights after head training ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best head weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model1.name} - {e}.\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best head weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model2.name} - {e}.\")\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\")\n# Start fine-tuning epochs from 0 for this phase, but total epochs define the duration\nhistory1_ft, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\")\nhistory2_ft, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\n# Load best weights saved during the entire process (should be the fine-tuned ones if they improved)\nprint(\"\\n--- Loading potentially best fine-tuned weights ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model1.name} - {e}\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model2.name} - {e}\")\n\n# =============================================================================\n# Individual Model Evaluation (Optional but recommended)\n# =============================================================================\nprint(\"\\n=== Evaluating Individual Models ===\")\nmetrics_m1_affect = evaluate_model(model1, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m1_fer = evaluate_model(model1, fer2013_test_ds, class_names, \"FER2013\")\nmetrics_m2_affect = evaluate_model(model2, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m2_fer = evaluate_model(model2, fer2013_test_ds, class_names, \"FER2013\")\n\n# =============================================================================\n# Ensemble Prediction and Evaluation (Same as V3.1, ensure test datasets loaded)\n# =============================================================================\nmodels_to_ensemble = [model1, model2]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping ensemble evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} ---\")\n    all_preds_probs = []\n    for i, model in enumerate(models):\n        print(f\"Predicting with model {i+1}: {model.name}\")\n        preds = model.predict(test_dataset)\n        all_preds_probs.append(preds)\n\n    # Weighted average of probabilities\n    weighted_preds_probs = np.tensordot(weights, all_preds_probs, axes=([0],[0]))\n    y_pred = np.argmax(weighted_preds_probs, axis=1)\n\n    # Extract true labels\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in ensemble evaluation data.\")\n         return {\"accuracy\": 0, \"f1_score\": 0}\n\n    accuracy = np.mean(y_true == y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\")\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"Ensemble {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1}\n\n# --- Evaluate Ensemble ---\nprint(\"\\n=== Evaluating Ensemble ===\")\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], affectnet_test_ds, class_names, \"AffectNet\")\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], fer2013_test_ds, class_names, \"FER2013\")\n\n# =============================================================================\n# Save Final Models (Optional)\n# =============================================================================\n# model1.save(\"final_model1_effnetb0_v3.2.h5\")\n# model2.save(\"final_model2_effnetb4_v3.2.h5\")\n# print(\"Final individual models saved.\")\n\nprint(\"\\n=== FINAL RESULTS ===\")\nprint(\"\\n--- Individual Model Performance ---\")\nif metrics_m1_affect: print(f\"{model1.name} AffectNet Test Accuracy: {metrics_m1_affect['accuracy']:.4f}, F1: {metrics_m1_affect['f1_score']:.4f}\")\nif metrics_m1_fer: print(f\"{model1.name} FER2013 Test Accuracy: {metrics_m1_fer['accuracy']:.4f}, F1: {metrics_m1_fer['f1_score']:.4f}\")\nif metrics_m2_affect: print(f\"{model2.name} AffectNet Test Accuracy: {metrics_m2_affect['accuracy']:.4f}, F1: {metrics_m2_affect['f1_score']:.4f}\")\nif metrics_m2_fer: print(f\"{model2.name} FER2013 Test Accuracy: {metrics_m2_fer['accuracy']:.4f}, F1: {metrics_m2_fer['f1_score']:.4f}\")\n\nprint(\"\\n--- Ensemble Performance ---\")\nif affectnet_metrics_ens:\n    print(f\"Ensemble AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\")\nif fer_metrics_ens:\n    print(f\"Ensemble FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\")\n\nprint(\"\\nTraining and evaluation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:02:31.612070Z","iopub.execute_input":"2025-04-02T17:02:31.612380Z","iopub.status.idle":"2025-04-02T19:49:12.235626Z","shell.execute_reply.started":"2025-04-02T17:02:31.612359Z","shell.execute_reply":"2025-04-02T19:49:12.234885Z"}},"outputs":[{"name":"stdout","text":"Found 1 GPUs: Memory growth enabled\nMixed precision enabled ('mixed_float16')\nLoading training/validation data from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Train\nFound 57744 files belonging to 8 classes.\nUsing 46196 files for training.\nFound 57744 files belonging to 8 classes.\nUsing 11548 files for validation.\nDataset loaded with classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nLoading test data for 'affectnet' from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test\nFound emotion folders: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\n  Found 4039 images for emotion 'surprise' in 'affectnet'.\n  Found 3176 images for emotion 'fear' in 'affectnet'.\n  Found 5126 images for emotion 'neutral' in 'affectnet'.\n  Found 3091 images for emotion 'sad' in 'affectnet'.\n  Found 2477 images for emotion 'disgust' in 'affectnet'.\n  Found 2871 images for emotion 'contempt' in 'affectnet'.\n  Found 5044 images for emotion 'happy' in 'affectnet'.\n  Found 3218 images for emotion 'anger' in 'affectnet'.\nTotal images found for 'affectnet' test set: 29042\nLoading test data for 'fer2013' from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test\nFound emotion folders: ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\n  Found 831 images for emotion 'surprise' in 'fer2013'.\n  Found 1024 images for emotion 'fear' in 'fer2013'.\n  Found 1233 images for emotion 'neutral' in 'fer2013'.\n  Found 1247 images for emotion 'sad' in 'fer2013'.\n  Found 111 images for emotion 'disgust' in 'fer2013'.\n  Found 54 images for emotion 'contempt' in 'fer2013'.\n  Found 1774 images for emotion 'happy' in 'fer2013'.\n  Found 958 images for emotion 'anger' in 'fer2013'.\nTotal images found for 'fer2013' test set: 7232\nCalculating class weights...\nApproximate number of batches in training dataset: 722\nUnique labels found for weight calculation: [0 1 2 3 4 5 6 7] with counts [5744 2362 2296 5804 9773 8078 6381 5758]\nClass weights calculated: {0: 1.0053098885793872, 1: 2.4447502116850126, 2: 2.515026132404181, 3: 0.9949172984148863, 4: 0.5908625805791466, 5: 0.7148427828670463, 6: 0.90495220184924, 7: 1.0028655783258076}\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEfficientNetB0 model built successfully.\nWarning: Building EfficientNetB4 with image size 224.\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEfficientNetB4 model built successfully.\n\nClass names being used: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nNumber of classes for models: 8\n\n=== Phase 1: Training Head of Model 1 ===\nSetting up head training for EfficientNetB0.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB0\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (\u001b[38;5;33mCast\u001b[0m)                                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │             \u001b[38;5;34m4,049,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m10,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">10,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,059,819\u001b[0m (15.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,059,819</span> (15.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,248\u001b[0m (40.03 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,248</span> (40.03 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Head Training for EfficientNetB0 (Epochs 0 to 15) ---\nEpoch 1/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.2370 - loss: 1.9154\nEpoch 1: val_accuracy improved from -inf to 0.49021, saving model to best_EfficientNetB0.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 137ms/step - accuracy: 0.2371 - loss: 1.9153 - val_accuracy: 0.4902 - val_loss: 1.6172\nEpoch 2/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3321 - loss: 1.7263\nEpoch 2: val_accuracy did not improve from 0.49021\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3321 - loss: 1.7263 - val_accuracy: 0.4383 - val_loss: 1.6477\nEpoch 3/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3463 - loss: 1.6971\nEpoch 3: val_accuracy did not improve from 0.49021\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3463 - loss: 1.6971 - val_accuracy: 0.4821 - val_loss: 1.5795\nEpoch 4/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3584 - loss: 1.6817\nEpoch 4: val_accuracy improved from 0.49021 to 0.52373, saving model to best_EfficientNetB0.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3584 - loss: 1.6817 - val_accuracy: 0.5237 - val_loss: 1.5249\nEpoch 5/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3618 - loss: 1.6703\nEpoch 5: val_accuracy did not improve from 0.52373\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3618 - loss: 1.6703 - val_accuracy: 0.4400 - val_loss: 1.6252\nEpoch 6/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3588 - loss: 1.6714\nEpoch 6: val_accuracy did not improve from 0.52373\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3588 - loss: 1.6714 - val_accuracy: 0.4631 - val_loss: 1.5588\nEpoch 7/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3638 - loss: 1.6647\nEpoch 7: val_accuracy did not improve from 0.52373\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 123ms/step - accuracy: 0.3638 - loss: 1.6647 - val_accuracy: 0.4777 - val_loss: 1.5668\nEpoch 8/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3623 - loss: 1.6606\nEpoch 8: val_accuracy improved from 0.52373 to 0.54070, saving model to best_EfficientNetB0.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3623 - loss: 1.6606 - val_accuracy: 0.5407 - val_loss: 1.4908\nEpoch 9/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3656 - loss: 1.6553\nEpoch 9: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 123ms/step - accuracy: 0.3656 - loss: 1.6553 - val_accuracy: 0.5242 - val_loss: 1.5306\nEpoch 10/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3698 - loss: 1.6512\nEpoch 10: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3698 - loss: 1.6512 - val_accuracy: 0.4983 - val_loss: 1.5584\nEpoch 11/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3708 - loss: 1.6422\nEpoch 11: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3708 - loss: 1.6422 - val_accuracy: 0.5198 - val_loss: 1.5267\nEpoch 12/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3721 - loss: 1.6402\nEpoch 12: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3721 - loss: 1.6402 - val_accuracy: 0.5241 - val_loss: 1.5229\nEpoch 13/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3705 - loss: 1.6413\nEpoch 13: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3705 - loss: 1.6413 - val_accuracy: 0.5191 - val_loss: 1.5320\nEpoch 14/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3710 - loss: 1.6475\nEpoch 14: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3710 - loss: 1.6474 - val_accuracy: 0.5192 - val_loss: 1.5332\nEpoch 15/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3753 - loss: 1.6447\nEpoch 15: val_accuracy did not improve from 0.54070\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 122ms/step - accuracy: 0.3753 - loss: 1.6446 - val_accuracy: 0.5192 - val_loss: 1.5332\nEpoch 15: early stopping\nRestoring model weights from the end of the best epoch: 8.\n\n=== Phase 1: Training Head of Model 2 ===\nSetting up head training for EfficientNetB4.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │            \u001b[38;5;34m17,673,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m14,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">14,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,688,167\u001b[0m (67.48 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,688,167</span> (67.48 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,344\u001b[0m (56.03 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,344</span> (56.03 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,673,823\u001b[0m (67.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> (67.42 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Head Training for EfficientNetB4 (Epochs 0 to 15) ---\nEpoch 1/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.2764 - loss: 1.8658\nEpoch 1: val_accuracy improved from -inf to 0.39557, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 266ms/step - accuracy: 0.2765 - loss: 1.8657 - val_accuracy: 0.3956 - val_loss: 1.6815\nEpoch 2/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3647 - loss: 1.6966\nEpoch 2: val_accuracy improved from 0.39557 to 0.41453, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 257ms/step - accuracy: 0.3647 - loss: 1.6966 - val_accuracy: 0.4145 - val_loss: 1.6453\nEpoch 3/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3757 - loss: 1.6775\nEpoch 3: val_accuracy improved from 0.41453 to 0.44293, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 257ms/step - accuracy: 0.3757 - loss: 1.6775 - val_accuracy: 0.4429 - val_loss: 1.5632\nEpoch 4/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.3791 - loss: 1.6613\nEpoch 4: val_accuracy improved from 0.44293 to 0.44830, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 258ms/step - accuracy: 0.3791 - loss: 1.6613 - val_accuracy: 0.4483 - val_loss: 1.5535\nEpoch 5/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3838 - loss: 1.6573\nEpoch 5: val_accuracy did not improve from 0.44830\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3838 - loss: 1.6573 - val_accuracy: 0.4444 - val_loss: 1.5810\nEpoch 6/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.3812 - loss: 1.6547\nEpoch 6: val_accuracy improved from 0.44830 to 0.46718, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 257ms/step - accuracy: 0.3812 - loss: 1.6547 - val_accuracy: 0.4672 - val_loss: 1.5347\nEpoch 7/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.3849 - loss: 1.6450\nEpoch 7: val_accuracy improved from 0.46718 to 0.48233, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 258ms/step - accuracy: 0.3849 - loss: 1.6450 - val_accuracy: 0.4823 - val_loss: 1.5053\nEpoch 8/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3903 - loss: 1.6367\nEpoch 8: val_accuracy did not improve from 0.48233\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3903 - loss: 1.6367 - val_accuracy: 0.4748 - val_loss: 1.5254\nEpoch 9/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.3884 - loss: 1.6368\nEpoch 9: val_accuracy improved from 0.48233 to 0.48511, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 258ms/step - accuracy: 0.3885 - loss: 1.6368 - val_accuracy: 0.4851 - val_loss: 1.5086\nEpoch 10/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3943 - loss: 1.6308\nEpoch 10: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3943 - loss: 1.6308 - val_accuracy: 0.4697 - val_loss: 1.5296\nEpoch 11/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3943 - loss: 1.6238\nEpoch 11: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3943 - loss: 1.6238 - val_accuracy: 0.4631 - val_loss: 1.5478\nEpoch 12/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3979 - loss: 1.6170\nEpoch 12: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3979 - loss: 1.6170 - val_accuracy: 0.4548 - val_loss: 1.5468\nEpoch 13/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3984 - loss: 1.6168\nEpoch 13: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3984 - loss: 1.6168 - val_accuracy: 0.4544 - val_loss: 1.5455\nEpoch 14/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3956 - loss: 1.6244\nEpoch 14: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3956 - loss: 1.6244 - val_accuracy: 0.4536 - val_loss: 1.5461\nEpoch 15/15\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.3938 - loss: 1.6240\nEpoch 15: val_accuracy did not improve from 0.48511\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 255ms/step - accuracy: 0.3938 - loss: 1.6240 - val_accuracy: 0.4536 - val_loss: 1.5461\nRestoring model weights from the end of the best epoch: 9.\n\n--- Loading best weights after head training ---\nLoaded best head weights for EfficientNetB0\nLoaded best head weights for EfficientNetB4\n\n=== Phase 2: Fine-tuning Model 1 ===\nSetting up fine-tuning for EfficientNetB0...\nUnfreezing top 10 layers of efficientnetb0 (out of 239). Freezing up to layer 229.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB0\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (\u001b[38;5;33mCast\u001b[0m)                                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │             \u001b[38;5;34m4,049,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m10,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">10,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,059,819\u001b[0m (15.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,059,819</span> (15.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m900,280\u001b[0m (3.43 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">900,280</span> (3.43 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,159,539\u001b[0m (12.05 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,159,539</span> (12.05 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Fine-tuning for EfficientNetB0 (Epochs 0 to 30) ---\nEpoch 1/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.3713 - loss: 1.6369\nEpoch 1: val_accuracy improved from -inf to 0.58954, saving model to best_EfficientNetB0.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 128ms/step - accuracy: 0.3713 - loss: 1.6369 - val_accuracy: 0.5895 - val_loss: 1.3427 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4305 - loss: 1.5031\nEpoch 2: val_accuracy did not improve from 0.58954\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.4305 - loss: 1.5030 - val_accuracy: 0.5838 - val_loss: 1.3134 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4603 - loss: 1.4294\nEpoch 3: val_accuracy improved from 0.58954 to 0.59820, saving model to best_EfficientNetB0.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 125ms/step - accuracy: 0.4603 - loss: 1.4294 - val_accuracy: 0.5982 - val_loss: 1.2592 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4812 - loss: 1.3853\nEpoch 4: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.4813 - loss: 1.3853 - val_accuracy: 0.5654 - val_loss: 1.3105 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4938 - loss: 1.3465\nEpoch 5: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.4938 - loss: 1.3465 - val_accuracy: 0.5740 - val_loss: 1.3042 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5055 - loss: 1.3150\nEpoch 6: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.5055 - loss: 1.3150 - val_accuracy: 0.5971 - val_loss: 1.2175 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5177 - loss: 1.2914\nEpoch 7: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.5177 - loss: 1.2914 - val_accuracy: 0.5914 - val_loss: 1.2317 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5264 - loss: 1.2639\nEpoch 8: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.5264 - loss: 1.2639 - val_accuracy: 0.5538 - val_loss: 1.2665 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5343 - loss: 1.2456\nEpoch 9: val_accuracy did not improve from 0.59820\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.5343 - loss: 1.2456 - val_accuracy: 0.5881 - val_loss: 1.2298 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5435 - loss: 1.2214\nEpoch 10: val_accuracy did not improve from 0.59820\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 124ms/step - accuracy: 0.5435 - loss: 1.2214 - val_accuracy: 0.5877 - val_loss: 1.2300 - learning_rate: 2.0000e-05\nEpoch 10: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\n=== Phase 2: Fine-tuning Model 2 ===\nSetting up fine-tuning for EfficientNetB4...\nUnfreezing top 15 layers of efficientnetb4 (out of 476). Freezing up to layer 461.\n\n--- Model Summary After Compilation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"EfficientNetB4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              Para\u001b[0m\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (\u001b[38;5;33mFunctional\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │            \u001b[38;5;34m17,673,\u001b[0m\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (\u001b[38;5;33mCast\u001b[0m)                                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                              │                \u001b[38;5;34m14,\u001b[0m\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━\n┃<span style=\"font-weight: bold\"> Layer (type)                                        </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">               Para</span>\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                    │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ efficientnetb4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,</span>\n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                           │                   \n├─────────────────────────────────────────────────────┼────────────────────────────────────────┼───────────────────\n│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                              │                <span style=\"color: #00af00; text-decoration-color: #00af00\">14,</span>\n└─────────────────────────────────────────────────────┴────────────────────────────────────────┴───────────────────\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,688,167\u001b[0m (67.48 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,688,167</span> (67.48 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,626,296\u001b[0m (10.02 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,296</span> (10.02 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m15,061,871\u001b[0m (57.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,061,871</span> (57.46 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Fine-tuning for EfficientNetB4 (Epochs 0 to 30) ---\nEpoch 1/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.3915 - loss: 1.6194\nEpoch 1: val_accuracy improved from -inf to 0.43445, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 270ms/step - accuracy: 0.3915 - loss: 1.6194 - val_accuracy: 0.4344 - val_loss: 1.5729 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.4335 - loss: 1.5155\nEpoch 2: val_accuracy improved from 0.43445 to 0.47601, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 264ms/step - accuracy: 0.4335 - loss: 1.5155 - val_accuracy: 0.4760 - val_loss: 1.4900 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.4624 - loss: 1.4583\nEpoch 3: val_accuracy improved from 0.47601 to 0.50797, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.4625 - loss: 1.4583 - val_accuracy: 0.5080 - val_loss: 1.3817 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.4817 - loss: 1.4119\nEpoch 4: val_accuracy improved from 0.50797 to 0.54105, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 264ms/step - accuracy: 0.4817 - loss: 1.4118 - val_accuracy: 0.5410 - val_loss: 1.3257 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.4892 - loss: 1.3815\nEpoch 5: val_accuracy did not improve from 0.54105\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 263ms/step - accuracy: 0.4892 - loss: 1.3814 - val_accuracy: 0.5261 - val_loss: 1.3651 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.4974 - loss: 1.3583\nEpoch 6: val_accuracy improved from 0.54105 to 0.55507, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 264ms/step - accuracy: 0.4974 - loss: 1.3583 - val_accuracy: 0.5551 - val_loss: 1.2846 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5029 - loss: 1.3357\nEpoch 7: val_accuracy did not improve from 0.55507\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 262ms/step - accuracy: 0.5029 - loss: 1.3357 - val_accuracy: 0.5346 - val_loss: 1.3547 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5136 - loss: 1.3161\nEpoch 8: val_accuracy improved from 0.55507 to 0.56936, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 264ms/step - accuracy: 0.5136 - loss: 1.3161 - val_accuracy: 0.5694 - val_loss: 1.2746 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5200 - loss: 1.2949\nEpoch 9: val_accuracy did not improve from 0.56936\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 261ms/step - accuracy: 0.5201 - loss: 1.2949 - val_accuracy: 0.5584 - val_loss: 1.2722 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.5298 - loss: 1.2706\nEpoch 10: val_accuracy did not improve from 0.56936\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 261ms/step - accuracy: 0.5298 - loss: 1.2706 - val_accuracy: 0.5561 - val_loss: 1.2554 - learning_rate: 1.0000e-04\nEpoch 11/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5347 - loss: 1.2611\nEpoch 11: val_accuracy did not improve from 0.56936\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 262ms/step - accuracy: 0.5347 - loss: 1.2611 - val_accuracy: 0.5692 - val_loss: 1.2420 - learning_rate: 1.0000e-04\nEpoch 12/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5327 - loss: 1.2485\nEpoch 12: val_accuracy improved from 0.56936 to 0.58218, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 264ms/step - accuracy: 0.5327 - loss: 1.2485 - val_accuracy: 0.5822 - val_loss: 1.2221 - learning_rate: 1.0000e-04\nEpoch 13/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5428 - loss: 1.2283\nEpoch 13: val_accuracy did not improve from 0.58218\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 262ms/step - accuracy: 0.5428 - loss: 1.2283 - val_accuracy: 0.5519 - val_loss: 1.2751 - learning_rate: 1.0000e-04\nEpoch 14/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.5467 - loss: 1.2093\nEpoch 14: val_accuracy improved from 0.58218 to 0.58911, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 265ms/step - accuracy: 0.5467 - loss: 1.2093 - val_accuracy: 0.5891 - val_loss: 1.2033 - learning_rate: 1.0000e-04\nEpoch 15/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5542 - loss: 1.1952\nEpoch 15: val_accuracy improved from 0.58911 to 0.64756, saving model to best_EfficientNetB4.keras\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 267ms/step - accuracy: 0.5542 - loss: 1.1952 - val_accuracy: 0.6476 - val_loss: 1.0916 - learning_rate: 1.0000e-04\nEpoch 16/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5547 - loss: 1.1862\nEpoch 16: val_accuracy did not improve from 0.64756\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5547 - loss: 1.1862 - val_accuracy: 0.6020 - val_loss: 1.1750 - learning_rate: 1.0000e-04\nEpoch 17/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5612 - loss: 1.1690\nEpoch 17: val_accuracy did not improve from 0.64756\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5612 - loss: 1.1690 - val_accuracy: 0.5964 - val_loss: 1.1922 - learning_rate: 1.0000e-04\nEpoch 18/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5698 - loss: 1.1604\nEpoch 18: val_accuracy did not improve from 0.64756\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5698 - loss: 1.1604 - val_accuracy: 0.6305 - val_loss: 1.1119 - learning_rate: 1.0000e-04\nEpoch 19/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5717 - loss: 1.1437\nEpoch 19: val_accuracy did not improve from 0.64756\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5717 - loss: 1.1437 - val_accuracy: 0.6318 - val_loss: 1.0929 - learning_rate: 2.0000e-05\nEpoch 20/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5750 - loss: 1.1275\nEpoch 20: val_accuracy did not improve from 0.64756\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5750 - loss: 1.1275 - val_accuracy: 0.6214 - val_loss: 1.1302 - learning_rate: 2.0000e-05\nEpoch 21/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5831 - loss: 1.1169\nEpoch 21: val_accuracy did not improve from 0.64756\n\nEpoch 21: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5831 - loss: 1.1169 - val_accuracy: 0.6155 - val_loss: 1.1186 - learning_rate: 2.0000e-05\nEpoch 22/30\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5793 - loss: 1.1246\nEpoch 22: val_accuracy did not improve from 0.64756\n\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 265ms/step - accuracy: 0.5793 - loss: 1.1245 - val_accuracy: 0.6046 - val_loss: 1.1507 - learning_rate: 4.0000e-06\nEpoch 22: early stopping\nRestoring model weights from the end of the best epoch: 15.\n\n--- Loading potentially best fine-tuned weights ---\nLoaded best weights for EfficientNetB0\nLoaded best weights for EfficientNetB4\n\n=== Evaluating Individual Models ===\n\n--- Evaluating EfficientNetB0 on AffectNet ---\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 126ms/step - accuracy: 0.5178 - loss: 1.3122\nEfficientNetB0 AffectNet Test Loss: 1.2531\nEfficientNetB0 AffectNet Test Accuracy: 0.5290\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step\nEfficientNetB0 AffectNet F1 Score (Weighted): 0.5113\nEfficientNetB0 AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.55      0.19      0.28      3218\n    contempt       0.41      0.72      0.53      2871\n     disgust       0.31      0.60      0.41      2477\n        fear       0.55      0.21      0.31      3176\n       happy       0.82      0.67      0.74      5044\n     neutral       0.65      0.83      0.73      5126\n         sad       0.39      0.24      0.30      3091\n    surprise       0.48      0.53      0.51      4039\n\n    accuracy                           0.53     29042\n   macro avg       0.52      0.50      0.47     29042\nweighted avg       0.56      0.53      0.51     29042\n\nSaved confusion matrix to EfficientNetB0_AffectNet_confusion_matrix.png\n\n--- Evaluating EfficientNetB0 on FER2013 ---\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 113ms/step - accuracy: 0.4584 - loss: 1.4429\nEfficientNetB0 FER2013 Test Loss: 1.5242\nEfficientNetB0 FER2013 Test Accuracy: 0.4204\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step\nEfficientNetB0 FER2013 F1 Score (Weighted): 0.4094\nEfficientNetB0 FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.36      0.24      0.29       958\n    contempt       0.00      0.00      0.00        54\n     disgust       0.71      0.05      0.08       111\n        fear       0.28      0.11      0.16      1024\n       happy       0.75      0.45      0.57      1774\n     neutral       0.38      0.46      0.41      1233\n         sad       0.30      0.61      0.40      1247\n    surprise       0.54      0.66      0.60       831\n\n    accuracy                           0.42      7232\n   macro avg       0.42      0.32      0.31      7232\nweighted avg       0.46      0.42      0.41      7232\n\nSaved confusion matrix to EfficientNetB0_FER2013_confusion_matrix.png\n\n--- Evaluating EfficientNetB4 on AffectNet ---\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 204ms/step - accuracy: 0.5826 - loss: 1.1422\nEfficientNetB4 AffectNet Test Loss: 1.0549\nEfficientNetB4 AffectNet Test Accuracy: 0.6128\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 197ms/step\nEfficientNetB4 AffectNet F1 Score (Weighted): 0.6068\nEfficientNetB4 AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.59      0.36      0.44      3218\n    contempt       0.43      0.82      0.56      2871\n     disgust       0.40      0.65      0.49      2477\n        fear       0.71      0.31      0.43      3176\n       happy       0.91      0.79      0.84      5044\n     neutral       0.77      0.84      0.80      5126\n         sad       0.55      0.35      0.43      3091\n    surprise       0.55      0.58      0.57      4039\n\n    accuracy                           0.61     29042\n   macro avg       0.61      0.59      0.57     29042\nweighted avg       0.65      0.61      0.61     29042\n\nSaved confusion matrix to EfficientNetB4_AffectNet_confusion_matrix.png\n\n--- Evaluating EfficientNetB4 on FER2013 ---\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 202ms/step - accuracy: 0.5162 - loss: 1.2667\nEfficientNetB4 FER2013 Test Loss: 1.2427\nEfficientNetB4 FER2013 Test Accuracy: 0.5314\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 187ms/step\nEfficientNetB4 FER2013 F1 Score (Weighted): 0.5236\nEfficientNetB4 FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.47      0.32      0.38       958\n    contempt       1.00      0.09      0.17        54\n     disgust       0.36      0.32      0.33       111\n        fear       0.41      0.22      0.28      1024\n       happy       0.77      0.76      0.77      1774\n     neutral       0.50      0.47      0.49      1233\n         sad       0.36      0.64      0.46      1247\n    surprise       0.66      0.66      0.66       831\n\n    accuracy                           0.53      7232\n   macro avg       0.57      0.43      0.44      7232\nweighted avg       0.55      0.53      0.52      7232\n\nSaved confusion matrix to EfficientNetB4_FER2013_confusion_matrix.png\n\n=== Evaluating Ensemble ===\n\n--- Evaluating Ensemble on AffectNet ---\nPredicting with model 1: EfficientNetB0\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 87ms/step\nPredicting with model 2: EfficientNetB4\n\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 187ms/step\nEnsemble AffectNet Test Accuracy: 0.6064\nEnsemble AffectNet F1 Score (Weighted): 0.5942\nEnsemble AffectNet Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.63      0.29      0.40      3218\n    contempt       0.44      0.82      0.57      2871\n     disgust       0.38      0.67      0.49      2477\n        fear       0.71      0.28      0.40      3176\n       happy       0.91      0.77      0.83      5044\n     neutral       0.75      0.88      0.81      5126\n         sad       0.53      0.32      0.40      3091\n    surprise       0.54      0.59      0.56      4039\n\n    accuracy                           0.61     29042\n   macro avg       0.61      0.58      0.56     29042\nweighted avg       0.64      0.61      0.59     29042\n\nSaved ensemble confusion matrix to ensemble_AffectNet_confusion_matrix.png\n\n--- Evaluating Ensemble on FER2013 ---\nPredicting with model 1: EfficientNetB0\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step\nPredicting with model 2: EfficientNetB4\n\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 187ms/step\nEnsemble FER2013 Test Accuracy: 0.5220\nEnsemble FER2013 F1 Score (Weighted): 0.5099\nEnsemble FER2013 Classification Report:\n              precision    recall  f1-score   support\n\n       anger       0.48      0.29      0.36       958\n    contempt       0.00      0.00      0.00        54\n     disgust       0.48      0.19      0.27       111\n        fear       0.39      0.17      0.23      1024\n       happy       0.80      0.72      0.76      1774\n     neutral       0.48      0.49      0.48      1233\n         sad       0.35      0.68      0.46      1247\n    surprise       0.64      0.69      0.66       831\n\n    accuracy                           0.52      7232\n   macro avg       0.45      0.40      0.40      7232\nweighted avg       0.54      0.52      0.51      7232\n\nSaved ensemble confusion matrix to ensemble_FER2013_confusion_matrix.png\n\n=== FINAL RESULTS ===\n\n--- Individual Model Performance ---\nEfficientNetB0 AffectNet Test Accuracy: 0.5290, F1: 0.5113\nEfficientNetB0 FER2013 Test Accuracy: 0.4204, F1: 0.4094\nEfficientNetB4 AffectNet Test Accuracy: 0.6128, F1: 0.6068\nEfficientNetB4 FER2013 Test Accuracy: 0.5314, F1: 0.5236\n\n--- Ensemble Performance ---\nEnsemble AffectNet Test Accuracy: 0.6064\nEnsemble AffectNet F1 Score: 0.5942\nEnsemble FER2013 Test Accuracy: 0.5220\nEnsemble FER2013 F1 Score: 0.5099\n\nTraining and evaluation complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# orginal --- Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.0 (incorporating memory efficiency and best practices)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n# Import specific EfficientNet models you intend to use\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil # Keep for monitoring if desired\nfrom datetime import datetime\nimport math\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path\n    \"AFFECTNET_TRAIN_DIR\": \"Train\", # Combined train dir assumed\n    \"AFFECTNET_TEST_DIR\": \"Test/affectnet\", # Specific test dir\n    \"FER2013_TEST_DIR\": \"Test/fer2013\",     # Specific test dir\n    \"IMG_SIZE\": 224, # Base image size, adjust per model if needed (B4 might prefer 380)\n    \"BATCH_SIZE\": 64, # Adjust based on Kaggle GPU memory\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE, # Optimal prefetch buffer size\n    \"EPOCHS_HEAD\": 15, # Epochs for training the head only\n    \"EPOCHS_FINE_TUNE\": 30, # Epochs for fine-tuning\n    \"LR_HEAD\": 1e-3,\n    \"LR_FINE_TUNE_START\": 1e-4, # Starting LR for fine-tuning\n    \"DROPOUT_RATE\": 0.4,\n    \"NUM_CLASSES\": 7, # Assuming 7 emotion classes\n    \"MODEL_ARCH_1\": \"EfficientNetB0\",\n    \"MODEL_ARCH_2\": \"EfficientNetB4\",\n    \"ENSEMBLE_WEIGHTS\": [0.5, 0.5], # Initial weights for B0, B4\n    \"FINE_TUNE_LAYERS_B0\": 10, # Number of layers to unfreeze in B0 for fine-tuning\n    \"FINE_TUNE_LAYERS_B4\": 15, # Number of layers to unfreeze in B4 for fine-tuning\n    \"LOG_DIR_BASE\": \"logs/fit/\"\n}\n\n# Set Seed for reproducibility\ntf.random.set_seed(CONFIG[\"SEED\"])\nnp.random.seed(CONFIG[\"SEED\"])\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n        # Set visible devices if needed, otherwise uses all available\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\nelse:\n    print(\"No GPU detected. Running on CPU.\")\n\n# Enable mixed precision training\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\nprint(\"Mixed precision enabled ('mixed_float16')\")\n\n# =============================================================================\n# Data Loading and Preprocessing (Memory Efficient)\n# =============================================================================\ndef create_tf_dataset(directory, image_size, batch_size, subset=None, validation_split=None, shuffle=True):\n    \"\"\"Creates a tf.data.Dataset from image directories.\"\"\"\n    print(f\"Loading dataset from: {directory}, Subset: {subset or 'N/A'}\")\n    dataset = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical', # Use categorical for sparse_categorical_crossentropy later requires integer labels\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=shuffle,\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=subset,\n        # Consider adding 'color_mode='grayscale'' if models expect single channel\n    )\n\n    # Basic preprocessing (rescaling) - EfficientNet models handle internal scaling,\n    # but explicit scaling is often safe. Check model docs.\n    # normalization_layer = tf.keras.layers.Rescaling(1./255)\n    # dataset = dataset.map(lambda x, y: (normalization_layer(x), y),\n    #                       num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Configure for performance\n    class_names = dataset.class_names\n    print(f\"Dataset loaded with classes: {class_names}\")\n\n    # Configure for performance\n    dataset = dataset.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    # Return the optimized dataset AND the stored class names\n    return dataset, class_names\n\n# --- Create Datasets ---\n# Training and Validation Data (from combined 'train' directory)\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"AFFECTNET_TRAIN_DIR\"])\ntrain_ds, class_names = create_tf_dataset(\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"], subset=\"training\", validation_split=0.2\n)\nval_ds, _ = create_tf_dataset(\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"], subset=\"validation\", validation_split=0.2\n)\n\n# Test Datasets (from specific 'test' subdirectories)\naffectnet_test_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"AFFECTNET_TEST_DIR\"])\nfer2013_test_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"FER2013_TEST_DIR\"])\n\naffectnet_test_ds, _ = create_tf_dataset(affectnet_test_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"], shuffle=False)\nfer2013_test_ds, _ = create_tf_dataset(fer2013_test_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"], shuffle=False)\n\n# Combine test datasets if needed for overall evaluation (Optional)\n# combined_test_ds = affectnet_test_ds.concatenate(fer2013_test_ds) # Simple concatenation\n\n# =============================================================================\n# Class Weights Calculation for tf.data.Dataset\n# =============================================================================\ndef get_class_weights(dataset):\n    print(\"Calculating class weights...\")\n    # Need to iterate through the dataset to get all labels\n    # This is less efficient than having labels upfront but necessary for image_dataset_from_directory\n    all_labels = []\n    num_batches = tf.data.experimental.cardinality(dataset).numpy()\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY:\n         print(\"Warning: Cannot determine dataset cardinality for class weight calculation. Weights might be inaccurate.\")\n         # Fallback: Iterate a fixed large number of batches or assume uniform weights\n         # For now, let's iterate - this might be slow for huge datasets\n         for _, labels_batch in dataset.take(5000): # Limit iterations for very large datasets\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n         if not all_labels:\n              print(\"Error: Could not extract labels for class weights. Using uniform weights.\")\n              return None\n    else:\n         for _, labels_batch in dataset:\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1)) # Convert one-hot back to integer labels\n\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(all_labels),\n        y=all_labels\n    )\n    class_weights_dict = dict(enumerate(class_weights))\n    print(\"Class weights calculated:\", class_weights_dict)\n    return class_weights_dict\n\nclass_weights = get_class_weights(train_ds) # Calculate based on the training dataset\n\n# =============================================================================\n# Data Augmentation Layer\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]),\n    # Add more augmentations as needed (e.g., RandomContrast, RandomBrightness)\n    # Be careful not to distort key facial features too much\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate):\n    \"\"\"Builds a model using a specified base architecture.\"\"\"\n    input_shape = (img_size, img_size, 3)\n    inputs = Input(shape=input_shape, name=\"input_layer\")\n\n    # Apply data augmentation *inside* the model\n    x = data_augmentation(inputs)\n\n    # Select base model\n    if model_arch == \"EfficientNetB0\":\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=x, pooling='avg')\n        # Base model expects pixels in [0, 255]. Augmentation happens before.\n    elif model_arch == \"EfficientNetB4\":\n         # Note: B4 default input size is 380. Using 224 might need adjustment or fine-tuning.\n         # Consider changing CONFIG[\"IMG_SIZE\"] if using B4 primarily or resizing layer.\n         print(f\"Warning: Building {model_arch} with image size {img_size}. Optimal is typically larger (e.g., 380).\")\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=x, pooling='avg')\n    # Add other architectures like MobileNetV3 here if needed\n    # elif model_arch == \"MobileNetV3Large\":\n    #    base_model = tf.keras.applications.MobileNetV3Large(include_top=False, weights='imagenet', input_tensor=x, pooling='avg')\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\")\n\n    # Freeze base model initially\n    base_model.trainable = False\n\n    # Add classification head\n    # x = base_model.output # Use the output from the base_model directly (pooling is included)\n    x = Dropout(dropout_rate, name=\"top_dropout\")(base_model.output)\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(x) # Output layer should be float32\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch)\n    print(f\"{model_arch} model built successfully.\")\n    return model\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"]) # Use same IMG_SIZE for consistency here, though B4 might prefer larger\n\n# =============================================================================\n# Ensemble Model\n# =============================================================================\ndef build_ensemble(model_list, weights, input_shape):\n    \"\"\"Builds an ensemble model by averaging outputs.\"\"\"\n    if len(model_list) != len(weights):\n        raise ValueError(\"Number of models and weights must match.\")\n\n    # Ensure models are named uniquely if not already\n    for i, model in enumerate(model_list):\n        if not model.name: # Or use a more robust check\n             model._name = f\"ensemble_member_{i}\"\n        # Set base models to non-trainable initially for the ensemble wrapper\n        model.trainable = False # We train individual models first\n\n    inputs = Input(shape=input_shape, name=\"ensemble_input\")\n    outputs = [model(inputs) for model in model_list]\n    avg_output = Average(name=\"ensemble_average\")(outputs) # Simple averaging, weights applied during prediction/evaluation later if needed, or use WeightedAverage layer\n\n    ensemble_model = Model(inputs=inputs, outputs=avg_output, name=\"Emotion_Ensemble\")\n    print(\"Ensemble model built.\")\n    return ensemble_model\n\n# Note: Building ensemble *after* individual training is common.\n# This structure allows training models separately first.\n# If you want end-to-end ensemble training, the setup needs adjustment.\n\n# =============================================================================\n# Training Functions\n# =============================================================================\n\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0):\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name\n    \n    # --- Callbacks ---\n    checkpoint_path = f\"best_{model.name}.h5\"\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_path,\n        monitor='val_accuracy',\n        save_best_only=True,\n        save_weights_only=False, # Save full model if needed, or True for weights only\n        mode='max',\n        verbose=1\n    )\n    # Use ReduceLROnPlateau for fine-tuning stage maybe\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    )\n    # Use Cosine Decay for the main training phase\n    total_steps = tf.data.experimental.cardinality(train_dataset).numpy() * epochs\n    warmup_steps = int(total_steps * 0.1) # Example: 10% warmup\n    cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n         initial_learning_rate=learning_rate,\n         decay_steps=total_steps - warmup_steps,\n         alpha=0.0 # End LR\n         # Consider adding warmup: tf.keras.optimizers.schedules.WarmUp(initial_learning_rate=learning_rate * 0.1, decay_schedule_fn=cosine_decay_schedule, warmup_steps=warmup_steps)\n    )\n\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=7, # Increase patience slightly\n        restore_best_weights=True, # Restore best weights found\n        verbose=1\n    )\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback]\n    if fine_tune:\n         callbacks.append(reduce_lr) # Use ReduceLROnPlateau for fine-tuning phase\n         optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) # Use standard LR for fine-tuning start\n    else:\n         # Use Cosine Decay for initial head training if desired, or stick with Adam + ReduceLR\n         optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_decay_schedule)\n         # callbacks.append(reduce_lr) # Can also use ReduceLR here instead of CosineDecay\n\n\n    # --- Fine-tuning setup ---\n    if fine_tune:\n        print(f\"Unfreezing last {fine_tune_layers} layers of {model.name} for fine-tuning.\")\n        model.trainable = True # Unfreeze the entire model first\n        # Then potentially re-freeze batch norm layers if issues arise (common practice)\n        # Or selectively unfreeze:\n        # for layer in model.layers[-fine_tune_layers:]:\n        #    if not isinstance(layer, tf.keras.layers.BatchNormalization): # Example: keep BN frozen\n        #        layer.trainable = True\n        # Simplified: Unfreeze all, let the low LR handle stability\n    else:\n         print(f\"Training only the head of {model.name}.\")\n         # Ensure base is frozen (should be by default from build_model)\n         for layer in model.layers:\n              if \"input_layer\" in layer.name or \"data_augmentation\" in layer.name: # Keep input & aug trainable\n                   layer.trainable = True\n              elif layer.name not in [\"top_dropout\", \"output_layer\"]: # Check base model layer names if needed\n                   layer.trainable = False\n              else: # Head layers\n                   layer.trainable = True\n\n\n    # Re-compile after changing trainable status\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary() # Print summary after compilation\n\n    print(f\"--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} ---\")\n    history = model.fit(\n        train_dataset,\n        epochs=epochs,\n        validation_data=validation_dataset,\n        class_weight=class_weights_dict,\n        callbacks=callbacks,\n        initial_epoch=initial_epoch, # For continuing training\n        verbose=1\n    )\n\n    # Load best weights saved by ModelCheckpoint\n    # Note: EarlyStopping with restore_best_weights=True might make this redundant\n    # model.load_weights(checkpoint_path)\n    # print(f\"Loaded best weights for {model.name} from {checkpoint_path}\")\n\n    return history, model\n\n# =============================================================================\n# Evaluation Function\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names, dataset_name):\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    print(f\"\\n--- Evaluating on {dataset_name} ---\")\n    results = model.evaluate(test_dataset, verbose=1)\n    print(f\"{dataset_name} Test Loss: {results[0]:.4f}\")\n    print(f\"{dataset_name} Test Accuracy: {results[1]:.4f}\")\n\n    # Predict and calculate F1, classification report, confusion matrix\n    y_pred_probs = model.predict(test_dataset)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n\n    # Extract true labels (requires iterating through dataset)\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    print(f\"{dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"{dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'{dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    # Save the plot instead of showing interactively in Kaggle/scripts\n    cm_filename = f\"{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved confusion matrix to {cm_filename}\")\n    # plt.show() # Avoid plt.show() in scripts\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1}\n\n\n# =============================================================================\n# Main Training Pipeline\n# =============================================================================\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\")\nhistory1_head, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"],\n    learning_rate=CONFIG[\"LR_HEAD\"],\n    fine_tune=False\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\")\nhistory2_head, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"],\n    learning_rate=CONFIG[\"LR_HEAD\"],\n    fine_tune=False\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\")\n# Continue from the last epoch of head training\ninitial_epoch_ft1 = CONFIG[\"EPOCHS_HEAD\"]\nhistory1_ft, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"] + CONFIG[\"EPOCHS_FINE_TUNE\"], # Total epochs\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True,\n    fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"],\n    initial_epoch=initial_epoch_ft1\n)\n# Load best weights saved during fine-tuning\nmodel1.load_weights(f\"best_{model1.name}.h5\")\nprint(f\"Loaded best fine-tuned weights for {model1.name}\")\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\")\ninitial_epoch_ft2 = CONFIG[\"EPOCHS_HEAD\"]\nhistory2_ft, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"] + CONFIG[\"EPOCHS_FINE_TUNE\"], # Total epochs\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True,\n    fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"],\n    initial_epoch=initial_epoch_ft2\n)\n# Load best weights saved during fine-tuning\nmodel2.load_weights(f\"best_{model2.name}.h5\")\nprint(f\"Loaded best fine-tuned weights for {model2.name}\")\n\n\n# =============================================================================\n# Ensemble Prediction and Evaluation\n# =============================================================================\n# We trained models separately. Now load the best versions and evaluate ensemble performance.\n\n# Load best saved models (redundant if EarlyStopping restored best weights AND saved full model)\n# model1 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_1']}.h5\")\n# model2 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_2']}.h5\")\n\nmodels_to_ensemble = [model1, model2]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names, dataset_name):\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} ---\")\n    all_preds = []\n    for model in models:\n        preds = model.predict(test_dataset)\n        all_preds.append(preds)\n\n    # Weighted average\n    weighted_preds = np.tensordot(weights, all_preds, axes=([0],[0]))\n    y_pred = np.argmax(weighted_preds, axis=1)\n\n    # Extract true labels\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    # Calculate metrics\n    accuracy = np.mean(y_true == y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\")\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"Ensemble {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_names, yticklabels=class_names) # Different color\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\")\n    # plt.show()\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1}\n\n# --- Evaluate Ensemble ---\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], affectnet_test_ds, class_names, \"AffectNet\")\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], fer2013_test_ds, class_names, \"FER2013\")\n\n# Optional: Evaluate on combined test set\n# print(\"Creating combined test dataset for final evaluation...\")\n# combined_test_ds = affectnet_test_ds.concatenate(fer2013_test_ds) # Recreate if not done before\n# combined_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], combined_test_ds, class_names, \"Combined\")\n\n\n# =============================================================================\n# Save Final Models (Optional: Save individual best or build/save ensemble model)\n# =============================================================================\n# Option 1: Save best individual models (already done via ModelCheckpoint if saving full model)\n# model1.save(\"final_model1_effnetb0.h5\")\n# model2.save(\"final_model2_effnetb4.h5\")\n\n# Option 2: Build and save an actual ensemble model object (less common if just averaging predictions)\n# ensemble_model_obj = build_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], (CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], 3))\n# ensemble_model_obj.save(\"final_emotion_ensemble_model.h5\")\n# print(\"Final ensemble model object saved.\")\n\nprint(\"\\n=== FINAL ENSEMBLE RESULTS ===\")\nprint(f\"AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\")\nprint(f\"AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\")\nprint(f\"FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\")\nprint(f\"FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\")\n# if 'combined_metrics_ens' in locals():\n#    print(f\"Combined Test Accuracy: {combined_metrics_ens['accuracy']:.4f}\")\n#    print(f\"Combined F1 Score: {combined_metrics_ens['f1_score']:.4f}\")\n\nprint(\"\\nTraining and evaluation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:51:25.025195Z","iopub.execute_input":"2025-04-02T15:51:25.025553Z","iopub.status.idle":"2025-04-02T15:52:01.698028Z","shell.execute_reply.started":"2025-04-02T15:51:25.025525Z","shell.execute_reply":"2025-04-02T15:52:01.696936Z"}},"outputs":[{"name":"stdout","text":"Found 1 GPUs: Memory growth enabled\nMixed precision enabled ('mixed_float16')\nLoading dataset from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Train, Subset: training\nFound 57744 files belonging to 8 classes.\nUsing 46196 files for training.\nDataset loaded with classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nLoading dataset from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Train, Subset: validation\nFound 57744 files belonging to 8 classes.\nUsing 11548 files for validation.\nDataset loaded with classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nLoading dataset from: /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test/affectnet, Subset: N/A\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-72f61c7cec24>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mfer2013_test_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BASE_DATA_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FER2013_TEST_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0maffectnet_test_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffectnet_test_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BATCH_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mfer2013_test_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfer2013_test_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BATCH_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-72f61c7cec24>\u001b[0m in \u001b[0;36mcreate_tf_dataset\u001b[0;34m(directory, image_size, batch_size, subset, validation_split, shuffle)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m\"\"\"Creates a tf.data.Dataset from image directories.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading dataset from: {directory}, Subset: {subset or 'N/A'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     dataset = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test/affectnet"],"ename":"NotFoundError","evalue":"Could not find directory /kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net/Test/affectnet","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# My code 3.0 (deepcloud) crash because of RAM tested with 32 and 64\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters (Adjusted for memory optimization and stability)\n# =============================================================================\nBATCH_SIZE = 32  # Maintain balance between memory and throughput\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./weights\")\n\n# Prioritize problematic classes by severity for targeted augmentation\n# The order indicates priority level for augmentation and oversampling\nPROBLEMATIC_CLASSES = ['contempt', 'fear', 'disgust', 'sad', 'anger', 'surprise']\n\n# =============================================================================\n# Enhanced Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Enhanced Image Preprocessing with Stronger Augmentation for Hard Classes\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Advanced preprocessing with class-adaptive augmentation strategies.\n    Uses stronger augmentation for difficult emotions to improve generalization.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply advanced augmentation during training\n    if training:\n        # Always flip horizontally (standard augmentation)\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced color/brightness adjustments\n        img = tf.image.random_brightness(img, 0.35)  # Further increased\n        img = tf.image.random_contrast(img, 0.65, 1.35)  # Wider range\n        img = tf.image.random_saturation(img, 0.5, 1.5)  # Added saturation adjustment\n        \n        # Stronger geometric transformations:\n        \n        # 1. More aggressive random rotation (±20 degrees)\n        angle = tf.random.uniform([], -0.349066, 0.349066)  # ±20 degrees in radians\n        img = tf.image.rot90(img, k=tf.cast(angle / (np.pi/2) * 4, tf.int32))\n        \n        # 2. Random zoom with variable ranges\n        zoom_factor = tf.random.uniform([], 0.75, 1.0)  # More aggressive zoom\n        h, w = tf.shape(img)[0], tf.shape(img)[1]\n        crop_size_h = tf.cast(tf.cast(h, tf.float32) * zoom_factor, tf.int32)\n        crop_size_w = tf.cast(tf.cast(w, tf.float32) * zoom_factor, tf.int32)\n        img = tf.image.random_crop(img, [crop_size_h, crop_size_w, 3])\n        img = tf.image.resize(img, [224, 224])\n        \n        # 3. Random translation\n        # Pad the image and then crop at random position\n        padded = tf.image.resize_with_crop_or_pad(img, 240, 240)  # Add 16 pixels of padding\n        img = tf.image.random_crop(padded, [224, 224, 3])  # Random crop back to original size\n        \n        # 4. Occasionally apply Gaussian blur (helps with generalization)\n        # Simulated with average pooling followed by upsampling\n        apply_blur = tf.random.uniform([], 0, 1) < 0.3  # 30% chance to apply blur\n        \n        def apply_gaussian_blur(image):\n            blurred = tf.nn.avg_pool2d(\n                tf.expand_dims(image, 0), \n                ksize=[1, 3, 3, 1], \n                strides=[1, 1, 1, 1], \n                padding='SAME'\n            )\n            return tf.squeeze(blurred, 0)\n        \n        img = tf.cond(apply_blur, \n                     lambda: apply_gaussian_blur(img), \n                     lambda: img)\n        \n        # 5. Add more aggressive noise\n        noise_level = tf.random.uniform([], 0.01, 0.025)  # Randomized noise level\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_level)\n        img = img + noise\n        \n        # Ensure valid range\n        img = tf.clip_by_value(img, 0.0, 255.0)\n    \n    # Basic normalization to [0,1] range for consistency\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with proper repeat mechanism\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=False):\n    \"\"\"Fixed dataset creation with proper infinite repetition for training\"\"\"\n    if dataset_type:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n    \n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create the base dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Memory optimization: Only cache validation/test datasets\n    if cache and not is_training:\n        cache_name = f'./cache/{dataset_type}_cache' if dataset_type else './cache/combined_cache'\n        ds = ds.cache(cache_name)\n    \n    # Create a constant for the training value to avoid retracing\n    training_value = tf.constant(is_training)\n    \n    # Apply preprocessing\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Better shuffling with larger buffer\n        buffer_size = min(len(dataframe), 20000)\n        \n        # CRITICAL FIX: First shuffle, then repeat BEFORE batching\n        # This ensures we get different batches in each epoch\n        ds = ds.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n        \n        # Make dataset infinite but BEFORE batching\n        ds = ds.repeat(-1)  # -1 means repeat indefinitely\n    \n    # Then batch and prefetch\n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create more aggressively balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a highly balanced dataset with progressive emphasis on problematic classes.\n    Classes are oversampled with different ratios based on their difficulty level.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (in priority order)\n        \n    Returns:\n        Balanced tf.data.Dataset with progressive emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Get class counts to determine relative scarcity\n    class_counts = dataframe[\"label\"].value_counts().to_dict()\n    max_count = max(class_counts.values())\n    \n    # Sample from each class with progressive emphasis\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        class_size = len(class_df)\n        \n        # Base sampling for non-problematic classes (reduced from previous 350)\n        samples_per_class = 300\n        \n        # Progressive oversampling based on position in emphasis_classes list\n        if class_name in emphasis_classes:\n            # Higher emphasis for classes earlier in the list\n            priority_index = emphasis_classes.index(class_name)\n            \n            # Exponential scaling based on priority (earlier = more samples)\n            priority_factor = 1.0 + (len(emphasis_classes) - priority_index) * 0.4\n            \n            # Additional scaling for extremely underrepresented classes\n            if class_size < (max_count * 0.1):  # Less than 10% of max class\n                priority_factor *= 1.5\n            \n            # Calculate final sample count\n            samples_per_class = int(800 * priority_factor)\n            \n            # Special handling for contempt which has very few samples\n            if class_name == 'contempt':\n                samples_per_class = 1200  # Extreme oversampling for contempt\n        \n        print(f\"Class {class_name}: Sampling {samples_per_class} from {class_size} images\")\n        \n        # Sample with replacement if needed\n        if class_size <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (progressive emphasis on {emphasis_classes})\")\n    \n    # Create dataset with fixed repeat mechanism\n    return create_dataset(balanced_df, is_training=is_training, cache=False)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 20  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n       \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n        # Add memory cleanup\n        del images, labels, batch_preds\n        gc.collect()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create a memory-optimized ensemble with reduced parameter count.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # MEMORY OPTIMIZATION: Use tf.keras.mixed_precision\n    # Enable mixed precision at the start of model creation\n    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n    tf.keras.mixed_precision.set_global_policy(policy)\n    print(\"Using mixed precision policy:\", policy.name)\n    \n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # ==================== MobileNetV2 Branch (Memory Optimized) ====================\n    # Use a smaller input size for MobileNetV2 to reduce memory\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    # Create MobileNetV2 as standalone model with alpha=0.75 to reduce parameters\n    try:\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(96, 96, 3),\n            alpha=0.75,  # Use a smaller model (75% of filters)\n            name='mobilenet_core'\n        )\n    except Exception as e:\n        print(f\"MobileNetV2 imagenet weights failed to load: {e}\")\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights=None,\n            input_shape=(96, 96, 3),\n            alpha=0.75,  # Use a smaller model\n            name='mobilenet_core'\n        )\n        try:\n            mobilenet_core.load_weights('weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n        except Exception as e:\n            print(f\"Failed to load local MobileNetV2 weights: {e}, continuing with random initialization\")\n    \n    mobilenet_features = mobilenet_core(mobilenet_preprocess)\n    # Use global average pooling to significantly reduce parameters\n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_features)\n\n    # ==================== Xception Branch (Memory Optimized) ====================\n    # MEMORY OPTIMIZATION: Create Xception with proper error handling\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    try:\n        xception_core = Xception(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n    except Exception as e:\n        print(f\"Xception imagenet weights failed to load: {e}\")\n        xception_core = Xception(\n            include_top=False,\n            weights=None,\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n        try:\n            xception_core.load_weights('weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n        except Exception as e:\n            print(f\"Failed to load local Xception weights: {e}, continuing with random initialization\")\n    \n    xception_features = xception_core(xception_preprocess)\n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_features)\n\n    # ==================== EfficientNetB0 Branch (Memory Optimized) ====================\n    # Use smaller input size for EfficientNetB0 to reduce memory\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    # Create EfficientNetB0 with reduced complexity for memory savings\n    try:\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n    except Exception as e:\n        print(f\"EfficientNetB0 imagenet weights failed to load: {e}\")\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights=None,\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n        try:\n            efficientnet_core.load_weights('/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5')\n        except Exception as e:\n            print(f\"Failed to load local EfficientNetB0 weights: {e}, continuing with random initialization\")\n    \n    efficientnet_features = efficientnet_core(efficientnet_preprocess)\n    # Use global average pooling to significantly reduce parameters\n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_features)\n\n    # ==================== Feature Processing (Memory Optimized) ====================\n    # Use smaller projection dimensions to reduce memory\n    def create_projection_head(inputs, name):\n        # Use 96 dimensions instead of 128 to reduce parameters\n        x = Dense(96, name=f'{name}_projection')(inputs)\n        x = BatchNormalization()(x)\n        return Activation('relu', dtype='float32')(x)\n    \n    mobilenet_features = create_projection_head(mobilenet_features, 'mobilenet')\n    xception_features = create_projection_head(xception_features, 'xception')\n    efficientnet_features = create_projection_head(efficientnet_features, 'efficientnet')\n\n    # ==================== Feature Fusion (Memory Optimized) ====================\n    merged_features = Concatenate(name='feature_fusion')([\n        mobilenet_features,\n        xception_features,\n        efficientnet_features\n    ])\n\n    # ==================== Classification Head (Memory Optimized) ====================\n    # Use smaller hidden layers to reduce parameters\n    x = Dense(192, name='fusion_dense1')(merged_features)  # Reduced from 256\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(96, name='fusion_dense2')(x)  # Reduced from 128\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n\n    # ==================== Model Assembly ====================\n    model = keras.Model(\n        inputs=inputs,\n        outputs=outputs,\n        name='emotion_ensemble'\n    )\n    \n    # Freeze base models if requested (important for memory optimization)\n    if freeze_base:\n        mobilenet_core.trainable = False\n        xception_core.trainable = False\n        efficientnet_core.trainable = False\n\n    # ==================== Compilation ====================\n    # Use a gentler initial learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n        keras.optimizers.Adam(\n            tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=5e-4,  # Reduced from original\n                decay_steps=15000\n            )\n        )\n    )\n    \n    # Add gradient clipping to avoid instability\n    optimizer.clipnorm = 1.0\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Print model memory requirements (estimate)\n    print(f\"Model created with {model.count_params():,} parameters\")\n    \n    # Force garbage collection after model creation\n    gc.collect()\n    K.clear_session()\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# FIXED: Progressive Training Strategy for Ensemble with Corrected Layer Names\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Enhanced three-stage training approach for ensemble with safeguards against dataset depletion.\n    Each fit call is wrapped in a try/except to handle potential errors gracefully.\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (15% of total epochs)\n    stage1_epochs = max(5, int(total_epochs * 0.15))  # Increased from 10% to 15%\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    mobilenet_core = model.get_layer('mobilenet_core')\n    xception_core = model.get_layer('xception_core')\n    efficientnet_core = model.get_layer('efficientnet_core')\n    \n    # Explicitly freeze all base models\n    mobilenet_core.trainable = False\n    xception_core.trainable = False\n    efficientnet_core.trainable = False\n    \n    # Recompile with stable initial learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train fusion layers with error handling\n    try:\n        # CRITICAL FIX: Ensure training steps don't exceed dataset size\n        # This is a safeguard in case the dataset repeat mechanism fails\n        effective_steps = min(steps_per_epoch, 10000)\n        \n        history1 = model.fit(\n            train_ds,\n            epochs=stage1_epochs,\n            steps_per_epoch=effective_steps,  # Use safe step count\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        histories.append(history1)\n    except Exception as e:\n        print(f\"Warning: Stage 1 training encountered an error: {e}\")\n        print(\"Continuing to next stage...\")\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (35% of total epochs)\n    stage2_epochs = max(7, int(total_epochs * 0.35))  # Increased from 30% to 35%\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Get reference to actual base models (using correct layer names)\n    mobilenet = model.get_layer('mobilenet_core')\n    efficientnet = model.get_layer('efficientnet_core')\n    \n    # Partially unfreeze base models (last 30 layers of each)\n    for base_model in [mobilenet, efficientnet]:\n        base_model.trainable = True\n        # Freeze early layers, unfreeze later layers\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 30)\n    \n    # Keep Xception frozen\n    xception_core.trainable = False\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=8e-5)  # Gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing, with error handling\n    try:\n        history2 = model.fit(\n            train_ds,\n            epochs=stage2_epochs,\n            steps_per_epoch=effective_steps,  # Use same safe step count\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        histories.append(history2)\n    except Exception as e:\n        print(f\"Warning: Stage 2 training encountered an error: {e}\")\n        print(\"Continuing to next stage...\")\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze Xception (using correct layer name)\n    xception = model.get_layer('xception_core')\n    xception.trainable = True\n    \n    # Partially unfreeze layers (last 50 layers of each model)\n    for base_model in [mobilenet, efficientnet, xception]:\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 50)\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-5)  # Even gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning with error handling\n    try:\n        history3 = model.fit(\n            train_ds,\n            epochs=stage3_epochs,\n            steps_per_epoch=effective_steps,  # Use same safe step count\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        histories.append(history3)\n    except Exception as e:\n        print(f\"Warning: Stage 3 training encountered an error: {e}\")\n    \n    K.clear_session()\n    gc.collect()\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # Create datasets with memory optimizations\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True\n    )\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True\n    )\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n\n    # Add periodic garbage collection during evaluation\n    def evaluate_with_gc(model, test_ds, steps, class_names, log_dir, dataset_name):\n        metrics = evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name)\n        K.clear_session()\n        gc.collect()\n        return metrics\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights with highly adaptive adjustments\n    print(\"\\n4. Computing class weights with adaptive adjustments based on class difficulty\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Get class counts for adaptive weighting\n    affectnet_counts = affectnet_train_df[\"label\"].value_counts().to_dict()\n    fer_counts = fer_train_df[\"label\"].value_counts().to_dict()\n    \n    # Apply progressive weight scaling based on class difficulty and sample count\n    for i, problem_class in enumerate(PROBLEMATIC_CLASSES):\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            \n            # Calculate position-based scaling factor (earlier classes get higher weights)\n            # Scale from 2.5 for first class down to 1.2 for last class\n            position_factor = 2.5 - (i * (1.3 / len(PROBLEMATIC_CLASSES)))\n            \n            # Apply adaptive weight multiplier to AffectNet\n            if class_idx in affectnet_class_weights and problem_class in affectnet_counts:\n                # Get the current count\n                count = affectnet_counts[problem_class]\n                max_count = max(affectnet_counts.values())\n                # Additional scarcity factor based on relative sample count\n                scarcity_factor = 1.0 + (1.0 - (count / max_count))\n                # Apply combined scaling\n                affectnet_class_weights[class_idx] *= position_factor * scarcity_factor\n            \n            # Apply adaptive weight multiplier to FER2013\n            if class_idx in fer_class_weights and problem_class in fer_counts:\n                # Get the current count\n                count = fer_counts[problem_class]\n                max_count = max(fer_counts.values())\n                # Additional scarcity factor based on relative sample count\n                scarcity_factor = 1.0 + (1.0 - (count / max_count))\n                # Apply combined scaling\n                fer_class_weights[class_idx] *= position_factor * scarcity_factor\n                \n                # Special handling for extremely underrepresented classes like contempt\n                if count < (max_count * 0.05):  # Less than 5% of max class\n                    fer_class_weights[class_idx] *= 1.5  # Additional boost\n    \n    # Make happy and neutral weights slightly lower to avoid bias\n    for easy_class in ['happy', 'neutral']:\n        if easy_class in class_indices:\n            easy_idx = class_indices[easy_class]\n            if easy_idx in affectnet_class_weights:\n                affectnet_class_weights[easy_idx] *= 0.9\n            if easy_idx in fer_class_weights:\n                fer_class_weights[easy_idx] *= 0.9\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping with more patience\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,  # Increased from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate monitoring\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        ),\n        # Learning rate reduction on plateau\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            verbose=1,\n            min_lr=1e-7\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.h5\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.h5'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_with_gc(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_with_gc(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_with_gc(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.h5\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.h5'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Add memory monitoring\n    print(\"Initial RAM usage:\", psutil.virtual_memory().percent)\n    model, metrics = train_emotion_ensemble(data_dir)\n    print(\"Final RAM usage:\", psutil.virtual_memory().percent)\n      \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My code 2.5 (deepcloud) best one yet\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters (Adjusted for memory optimization and stability)\n# =============================================================================\nBATCH_SIZE = 64  # Maintain balance between memory and throughput\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./weights\")\n\n# Extend problematic classes for more targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust', 'anger', 'fear']\n\n# =============================================================================\n# Enhanced Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Enhanced Image Preprocessing with Stronger Augmentation for Hard Classes\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with stronger augmentation for difficult classes.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply enhanced augmentation during training\n    if training:\n        # Basic augmentations for all images\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced brightness and contrast adjustment\n        img = tf.image.random_brightness(img, 0.3)  # Increased from 0.2\n        img = tf.image.random_contrast(img, 0.7, 1.3)  # Wider range\n        \n        # Add random rotation (±15 degrees)\n        angle = tf.random.uniform([], -0.261799, 0.261799)  # ±15 degrees in radians\n        img = tf.image.rot90(img, k=tf.cast(angle / (np.pi/2) * 4, tf.int32))\n        \n        # Random zoom\n        zoom_factor = tf.random.uniform([], 0.8, 1.0)\n        h, w = tf.shape(img)[0], tf.shape(img)[1]\n        crop_size_h = tf.cast(tf.cast(h, tf.float32) * zoom_factor, tf.int32)\n        crop_size_w = tf.cast(tf.cast(w, tf.float32) * zoom_factor, tf.int32)\n        img = tf.image.random_crop(img, [crop_size_h, crop_size_w, 3])\n        img = tf.image.resize(img, [224, 224])\n        \n        # Add more aggressive noise\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.015)  # Increased from 0.01\n        img = img + noise\n        \n        # Ensure valid range\n        img = tf.clip_by_value(img, 0.0, 255.0)\n    \n    # Basic normalization to [0,1] range for consistency\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with proper repeat mechanism\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=False):\n    \"\"\"Memory-optimized dataset creation with proper repeat mechanism\"\"\"\n    if dataset_type:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n    \n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Memory optimization: Only cache validation/test datasets\n    if cache and not is_training:\n        cache_name = f'./cache/{dataset_type}_cache' if dataset_type else './cache/combined_cache'\n        ds = ds.cache(cache_name)\n    \n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Better shuffling with larger buffer\n        buffer_size = min(len(dataframe), 20000)  # Increased from 10000\n        ds = ds.shuffle(buffer_size=buffer_size)\n        # IMPORTANT: Create an infinite dataset by repeating\n        ds = ds.repeat()\n    \n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create more aggressively balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a more aggressively balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        \n        # Base sampling - reduced for non-problematic classes\n        samples_per_class = 350  \n        \n        # Significantly increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            # 100% more samples for problematic classes (from 400 to 700)\n            samples_per_class = 700  \n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with properly working repeat\n    return create_dataset(balanced_df, is_training=is_training, cache=False)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 20  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n       \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n        # Add memory cleanup\n        del images, labels, batch_preds\n        gc.collect()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble with model-specific preprocessing layers.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # ==================== MobileNetV2 Branch ====================\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    # Create MobileNetV2 as standalone model\n    try:\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n    except Exception as e:\n        print(f\"MobileNetV2 imagenet weights failed to load: {e}\")\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights=None,\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n        mobilenet_core.load_weights('weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n    \n    mobilenet_features = mobilenet_core(mobilenet_preprocess)\n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_features)\n\n    # ==================== Xception Branch ====================\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    # Create Xception as standalone model\n    try:\n        xception_core = Xception(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n    except Exception as e:\n        print(f\"Xception imagenet weights failed to load: {e}\")\n        xception_core = Xception(\n            include_top=False,\n            weights=None,\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n        xception_core.load_weights('weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n    \n    xception_features = xception_core(xception_preprocess)\n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_features)\n\n    # ==================== EfficientNetB0 Branch ====================\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    # Create EfficientNetB0 as standalone model\n    try:\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n    except Exception as e:\n        print(f\"EfficientNetB0 imagenet weights failed to load: {e}\")\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights=None,\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n        try:\n            efficientnet_core.load_weights('/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5')\n        except Exception as e:\n            print(f\"Failed to load local EfficientNetB0 weights: {e}\")\n            print(\"Continuing with random initialization for EfficientNetB0\")\n    \n    efficientnet_features = efficientnet_core(efficientnet_preprocess)\n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_features)\n\n    # ==================== Feature Processing ====================\n    def create_projection_head(inputs, name):\n        x = Dense(128, name=f'{name}_projection')(inputs)\n        x = BatchNormalization()(x)\n        return Activation('relu', dtype='float32')(x)\n    \n    mobilenet_features = create_projection_head(mobilenet_features, 'mobilenet')\n    xception_features = create_projection_head(xception_features, 'xception')\n    efficientnet_features = create_projection_head(efficientnet_features, 'efficientnet')\n\n    # ==================== Feature Fusion ====================\n    merged_features = Concatenate(name='feature_fusion')([\n        mobilenet_features,\n        xception_features,\n        efficientnet_features\n    ])\n\n    # ==================== Classification Head ====================\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n\n    # ==================== Model Assembly ====================\n    model = keras.Model(\n        inputs=inputs,\n        outputs=outputs,\n        name='emotion_ensemble'\n    )\n    \n    # Freeze base models if requested\n    if freeze_base:\n        mobilenet_core.trainable = False\n        xception_core.trainable = False\n        efficientnet_core.trainable = False\n\n    # ==================== Compilation ====================\n    # Use a gentler initial learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n        keras.optimizers.Adam(\n            tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=5e-4,  # Reduced from 1e-3\n                decay_steps=15000\n            )\n        )\n    )\n    \n    # Add gradient clipping to avoid instability\n    optimizer.clipnorm = 1.0\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),  # Increased from 2.0 for better focus on hard examples\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# FIXED: Progressive Training Strategy for Ensemble with Corrected Layer Names\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (15% of total epochs)\n    stage1_epochs = max(5, int(total_epochs * 0.15))  # Increased from 10% to 15%\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    mobilenet_core = model.get_layer('mobilenet_core')\n    xception_core = model.get_layer('xception_core')\n    efficientnet_core = model.get_layer('efficientnet_core')\n    \n    # Explicitly freeze all base models\n    mobilenet_core.trainable = False\n    xception_core.trainable = False\n    efficientnet_core.trainable = False\n    \n    # Recompile with stable initial learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (35% of total epochs)\n    stage2_epochs = max(7, int(total_epochs * 0.35))  # Increased from 30% to 35%\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Get reference to actual base models (FIXED: using correct layer names)\n    mobilenet = model.get_layer('mobilenet_core')\n    efficientnet = model.get_layer('efficientnet_core')\n    \n    # Partially unfreeze base models (last 30 layers of each)\n    for base_model in [mobilenet, efficientnet]:\n        base_model.trainable = True\n        # Freeze early layers, unfreeze later layers\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 30)\n    \n    # Keep Xception frozen\n    xception_core.trainable = False\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=8e-5)  # Gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze Xception (FIXED: using correct layer name)\n    xception = model.get_layer('xception_core')\n    xception.trainable = True\n    \n    # Partially unfreeze layers (last 50 layers of each model)\n    for base_model in [mobilenet, efficientnet, xception]:\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 50)\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-5)  # Even gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n\n    K.clear_session()\n    gc.collect()\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # Create datasets with memory optimizations\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True\n    )\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True\n    )\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n\n    # Add periodic garbage collection during evaluation\n    def evaluate_with_gc(model, test_ds, steps, class_names, log_dir, dataset_name):\n        metrics = evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name)\n        K.clear_session()\n        gc.collect()\n        return metrics\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with more aggressive adjustments\n    print(\"\\n4. Computing class weights with more aggressive adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes more aggressively\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 50% (up from 20%)\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.5\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.5\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping with more patience\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,  # Increased from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate monitoring\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        ),\n        # Learning rate reduction on plateau\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            verbose=1,\n            min_lr=1e-7\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.h5\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.h5'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_with_gc(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_with_gc(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_with_gc(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.h5\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.h5'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Add memory monitoring\n    print(\"Initial RAM usage:\", psutil.virtual_memory().percent)\n    model, metrics = train_emotion_ensemble(data_dir)\n    print(\"Final RAM usage:\", psutil.virtual_memory().percent)\n      \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T06:59:51.250306Z","iopub.execute_input":"2025-03-25T06:59:51.250616Z","iopub.status.idle":"2025-03-25T08:33:08.750841Z","shell.execute_reply.started":"2025-03-25T06:59:51.250592Z","shell.execute_reply":"2025-03-25T08:33:08.749513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My code 2.2 (deepsea)\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters (Adjusted for memory optimization)\n# =============================================================================\nBATCH_SIZE = 64  # Maintain balance between memory and throughput\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./weights\")\n\n#===========>\n# Make sure this code block runs successfully BEFORE creating the model\n!mkdir -p weights\n!wget https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5 -P weights\n!wget https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5 -P weights\n\n# The original URL isn't working, so let's try these alternatives for EfficientNetB0\n#!wget https://github.com/Runist/BasicNet/releases/download/dataset/efficientnetb0_notop.h5 -P weights\n# If that fails, try this direct link\n!wget /kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5 -P weights\n\n# Verify downloads\n!ls -lh weights/*\n#===========>\n\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust','anger']\n\n# =============================================================================\n# Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# FIXED: Enhanced Image Preprocessing with Consistent Size\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with consistent output size for all images.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # CRITICAL: Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply basic augmentation during training\n    if training:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        img = tf.clip_by_value(img, 0.0, 255.0)  # Ensure valid range\n    \n    # Basic normalization to [0,1] range for consistency\n    # Note: Model-specific normalization will happen in the model\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with caching and repeat\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=False):\n    \"\"\"Memory-optimized dataset creation\"\"\"\n    if dataset_type:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n    \n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Memory optimization: Only cache validation/test datasets\n    if cache and not is_training:\n        ds = ds.cache(f'./cache/{dataset_type}_cache' if dataset_type else './cache/combined_cache')\n    \n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        ds = ds.repeat(1000)\n    \n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        cache: Whether to cache the dataset\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with caching\n    return create_dataset(balanced_df, is_training=is_training, cache=False)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 20  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n       \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n        # Add memory cleanup\n        del images, labels, batch_preds\n        gc.collect()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble with model-specific preprocessing layers.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # ==================== MobileNetV2 Branch ====================\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    # Create MobileNetV2 as standalone model\n    try:\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n    except Exception as e:\n        print(f\"MobileNetV2 imagenet weights failed to load: {e}\")\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights=None,\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n        mobilenet_core.load_weights('weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n    \n    mobilenet_features = mobilenet_core(mobilenet_preprocess)\n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_features)\n\n    # ==================== Xception Branch ====================\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    # Create Xception as standalone model\n    try:\n        xception_core = Xception(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n    except Exception as e:\n        print(f\"Xception imagenet weights failed to load: {e}\")\n        xception_core = Xception(\n            include_top=False,\n            weights=None,\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n        xception_core.load_weights('weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n    \n    xception_features = xception_core(xception_preprocess)\n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_features)\n\n    # ==================== EfficientNetB0 Branch ====================\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    # Create EfficientNetB0 as standalone model\n    try:\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n    except Exception as e:\n        print(f\"EfficientNetB0 imagenet weights failed to load: {e}\")\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights=None,\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n        efficientnet_core.load_weights('/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5')\n    \n    efficientnet_features = efficientnet_core(efficientnet_preprocess)\n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_features)\n\n    # ==================== Feature Processing ====================\n    def create_projection_head(inputs, name):\n        x = Dense(128, name=f'{name}_projection')(inputs)\n        x = BatchNormalization()(x)\n        return Activation('relu', dtype='float32')(x)\n    \n    mobilenet_features = create_projection_head(mobilenet_features, 'mobilenet')\n    xception_features = create_projection_head(xception_features, 'xception')\n    efficientnet_features = create_projection_head(efficientnet_features, 'efficientnet')\n\n    # ==================== Feature Fusion ====================\n    merged_features = Concatenate(name='feature_fusion')([\n        mobilenet_features,\n        xception_features,\n        efficientnet_features\n    ])\n\n    # ==================== Classification Head ====================\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n\n    # ==================== Model Assembly ====================\n    model = keras.Model(\n        inputs=inputs,\n        outputs=outputs,\n        name='emotion_ensemble'\n    )\n    \n    # Freeze base models if requested\n    if freeze_base:\n        mobilenet_core.trainable = False\n        xception_core.trainable = False\n        efficientnet_core.trainable = False\n\n    # ==================== Compilation ====================\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n        keras.optimizers.Adam(\n            tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=1e-3,\n                decay_steps=10000\n            )\n        )\n    )\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Progressive Training Strategy for Ensemble\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (10% of total epochs)\n    stage1_epochs = max(3, int(total_epochs * 0.1))\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'xception_base', 'efficientnet_base']):\n            for base_layer in layer.layers:\n                base_layer.trainable = False\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (30% of total epochs)\n    stage2_epochs = max(6, int(total_epochs * 0.3))\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Get reference to actual base models\n    mobilenet = model.get_layer('mobilenet_core')  # Correct access\n    efficientnet = model.get_layer('efficientnet_core')  # Correct access\n    for base_model in [mobilenet, efficientnet]:  # Proper iteration\n        for layer in base_model.layers[-30:]:  # Correct layer access\n                layer.trainable = True\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(1e-4)  # Lower learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Get reference to all base models\n    xception = model.get_layer('xception_core')\n    for base_model in [mobilenet, efficientnet, xception]:\n        for layer in base_model.layers[-50:]:\n            layer.trainable = True\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(5e-5)  # Very low learning rate for fine-tuning\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n\n    K.clear_session()\n    gc.collect()\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # Create datasets with memory optimizations\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True\n    )\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True\n    )\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n\n    # Add periodic garbage collection during evaluation\n    def evaluate_with_gc(model, test_ds, steps, class_names, log_dir, dataset_name):\n        metrics = evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name)\n        K.clear_session()\n        gc.collect()\n        return metrics\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=8,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.keras\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_with_gc(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_with_gc(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_with_gc(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.keras\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.keras'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Add memory monitoring\n    print(\"Initial RAM usage:\", psutil.virtual_memory().percent)\n    model, metrics = train_emotion_ensemble(data_dir)\n    print(\"Final RAM usage:\", psutil.virtual_memory().percent)\n      \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# customized 3.0 (cloudy) - CONSUME DISK SPACE .82\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport gc\nimport psutil\n\n# =============================================================================\n# Memory-optimized configuration\n# =============================================================================\n# Configure GPU and enable memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training (helps with memory)\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# Adaptive batch size based on system memory\ntotal_ram_gb = psutil.virtual_memory().total / (1024**3)\nif total_ram_gb > 32:\n    BATCH_SIZE = 48  # High memory system\nelif total_ram_gb > 16:\n    BATCH_SIZE = 32  # Medium memory system\nelse:\n    BATCH_SIZE = 16  # Low memory system\nprint(f\"Adaptive batch size set to {BATCH_SIZE} based on {total_ram_gb:.1f}GB RAM\")\n\n# Limit TensorFlow's GPU memory usage\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            # Limit GPU memory (adjust percentage based on your system)\n            memory_limit = int(total_ram_gb * 0.7 * 1024)  # 70% of RAM in MB\n            tf.config.experimental.set_virtual_device_configuration(\n                gpu,\n                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]\n            )\n        print(f\"GPU memory limited to {memory_limit}MB\")\n    except RuntimeError as e:\n        print(f\"GPU memory limitation error: {e}\")\n\n# Other key parameters\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nIMAGE_SIZE = 160  # Reduced from 224 to save memory\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# Create required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log and cache directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./cache/affectnet\")\nensure_dir(\"./cache/fer2013\")\nensure_dir(\"./cache/combined\")\n\n# =============================================================================\n# Memory monitoring\n# =============================================================================\ndef log_memory_usage(stage):\n    \"\"\"Log memory usage at various stages of training\"\"\"\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    memory_gb = memory_info.rss / (1024 ** 3)\n    \n    # Log to console\n    print(f\"\\n[MEMORY] {stage}: {memory_gb:.2f} GB\")\n    \n    # Save to log file\n    with open(os.path.join(LOG_DIR, \"memory_usage.log\"), \"a\") as f:\n        f.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {stage}: {memory_gb:.2f} GB\\n\")\n    \n    # Get GPU memory if available\n    try:\n        gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n        gpu_used_gb = gpu_info['current'] / (1024 ** 3)\n        gpu_total_gb = gpu_info['peak'] / (1024 ** 3)\n        print(f\"[GPU MEMORY] Used: {gpu_used_gb:.2f} GB, Peak: {gpu_total_gb:.2f} GB\")\n        \n        with open(os.path.join(LOG_DIR, \"memory_usage.log\"), \"a\") as f:\n            f.write(f\"GPU Memory - Used: {gpu_used_gb:.2f} GB, Peak: {gpu_total_gb:.2f} GB\\n\")\n    except:\n        pass  # Skip if GPU memory info not available\n\n# =============================================================================\n# Focal Loss - Memory efficient implementation\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Memory-efficient focal loss implementation\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Clip values to avoid numerical instability\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply focusing parameter more efficiently\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Memory-optimized image preprocessing\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Memory-efficient preprocessing with smaller images\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([IMAGE_SIZE, IMAGE_SIZE, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Convert to grayscale then back to 3 channels for consistency\n        if tf.shape(img)[-1] == 3:\n            img = tf.image.rgb_to_grayscale(img)\n        img = tf.tile(img, [1, 1, 3])\n    \n    # Resize to smaller intermediate size\n    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE], method='bilinear')\n    \n    # Apply limited augmentation during training (fewer operations)\n    if training:\n        img = tf.image.random_flip_left_right(img)\n        # Only apply brightness OR contrast, not both (saves computation)\n        if tf.random.uniform(shape=[], maxval=1.0) > 0.5:\n            img = tf.image.random_brightness(img, 0.1)\n        else:\n            img = tf.image.random_contrast(img, 0.9, 1.1)\n    \n    # Basic normalization\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)\n    \n    return img, label\n\n# =============================================================================\n# Memory-efficient dataset creation\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"], limit_samples=None):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    Adds option to limit samples per category for testing.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                # Limit samples if specified (for testing with less memory)\n                if limit_samples is not None:\n                    np.random.shuffle(img_files)\n                    img_files = img_files[:limit_samples]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=True):\n    \"\"\"\n    Creates memory-efficient tf.data.Dataset with disk-based caching.\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    \n    # Create a generator function to avoid storing all labels in memory\n    def generate_samples():\n        for i in range(len(dataframe)):\n            yield (\n                dataframe[\"filepath\"].iloc[i],\n                class_indices[dataframe[\"label\"].iloc[i]],\n                dataframe[\"source\"].iloc[i]\n            )\n    \n    # Create dataset from generator\n    ds = tf.data.Dataset.from_generator(\n        generate_samples,\n        output_signature=(\n            tf.TensorSpec(shape=(), dtype=tf.string),\n            tf.TensorSpec(shape=(), dtype=tf.int32),\n            tf.TensorSpec(shape=(), dtype=tf.string)\n        )\n    )\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Use smaller buffer size to limit memory usage\n        buffer_size = min(1000, len(dataframe))\n        ds = ds.shuffle(buffer_size=buffer_size)\n        ds = ds.repeat()\n    \n    # Use disk-based caching instead of RAM\n    if cache:\n        cache_file = f\"./cache/{dataset_type or 'combined'}/dataset_cache\"\n        ds = ds.cache(filename=cache_file)\n    \n    # Batch and limit prefetch to save memory\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(2)  # Reduced from AUTOTUNE to limit memory\n    \n    return ds, class_indices\n\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES, cache=True):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    Memory-optimized version.\n    \"\"\"\n    balanced_data = []\n    \n    # Use smaller base samples per class to save memory\n    base_samples = 300  # Reduced from 400\n    emphasis_samples = 400  # Reduced from 600\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = base_samples\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = emphasis_samples\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with disk-based caching\n    return create_dataset(balanced_df, is_training=is_training, cache=cache)\n\n# =============================================================================\n# Memory-efficient Confusion Matrix Callback\n# =============================================================================\nclass MemoryEfficientConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Memory-efficient callback to monitor metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(MemoryEfficientConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()\n        self.class_metrics_history = {cls: [] for cls in class_names}\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Only calculate metrics at the specified frequency to save computation\n        if (epoch + 1) % self.freq != 0:\n            return\n            \n        # Limit validation steps to save memory\n        val_steps = min(20, sum(1 for _ in self.validation_data))\n        y_true = []\n        y_pred = []\n        \n        # Get predictions in smaller batches\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            # Use smaller batches for prediction to save memory\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n            \n            # Force garbage collection after each batch\n            gc.collect()\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history (keep only last 5 epochs to save memory)\n            history = self.class_metrics_history[self.class_names[i]]\n            history.append(class_accuracies[i])\n            if len(history) > 5:\n                history.pop(0)\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Print confusion matrix\n        print(\"\\nConfusion Matrix:\")\n        print(cm)\n        \n        # Print per-class accuracy\n        for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n            print(f\"{name}: {acc:.4f}\", end=\"  \")\n            if (i + 1) % 4 == 0:\n                print()\n        print(\"\\n\")\n        \n        # Save confusion matrix visualization with reduced figure size\n        plt.figure(figsize=(8, 6))  # Smaller figure size\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=self.class_names,\n                   yticklabels=self.class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n        plt.tight_layout()\n        \n        try:\n            plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png', dpi=100)  # Lower DPI\n        except Exception as e:\n            print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n        plt.close()\n        \n        # Force garbage collection\n        gc.collect()\n\n# =============================================================================\n# Memory-efficient Ensemble Model\n# =============================================================================\ndef create_lightweight_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create a memory-efficient version of the ensemble\n    \"\"\"\n    # Create inputs with smaller size\n    inputs = keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='image_input')\n    \n    # === MobileNetV2 Branch with reduced alpha ===\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    mobilenet_base = MobileNetV2(\n        include_top=False, \n        weights='imagenet',\n        input_tensor=mobilenet_preprocess,\n        alpha=0.75  # Reduced from 1.0 to save memory\n    )\n    \n    if freeze_base:\n        for layer in mobilenet_base.layers:\n            layer.trainable = False\n            \n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_base.output)\n    mobilenet_features = Dense(96, name='mobilenet_projection')(mobilenet_features)  # Reduced from 128\n    mobilenet_features = BatchNormalization()(mobilenet_features)\n    mobilenet_features = Activation('relu')(mobilenet_features)\n    \n    # === EfficientNetB0 Branch ===\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    efficientnet_base = EfficientNetB0(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=efficientnet_preprocess\n    )\n    \n    if freeze_base:\n        for layer in efficientnet_base.layers:\n            layer.trainable = False\n            \n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_base.output)\n    efficientnet_features = Dense(96, name='efficientnet_projection')(efficientnet_features)  # Reduced from 128\n    efficientnet_features = BatchNormalization()(efficientnet_features)\n    efficientnet_features = Activation('relu')(efficientnet_features)\n    \n    # === Feature Fusion ===\n    merged_features = Concatenate(name='feature_fusion')([mobilenet_features, efficientnet_features])\n    \n    # Simplified classification head\n    x = Dense(128, name='fusion_dense1')(merged_features)  # Reduced from 256\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    \n    # Output layer\n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='emotion_ensemble_lite')\n    \n    # Compile with focal loss\n    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Memory-efficient Training Strategy\n# =============================================================================\ndef train_ensemble_with_memory_efficient_strategy(model, train_ds, val_ds, \n                                                steps_per_epoch, val_steps,\n                                                total_epochs=30,\n                                                callbacks=None,\n                                                class_weights=None):\n    \"\"\"\n    Memory-efficient training approach with staged unfreezing\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (keep all base models frozen)\n    stage1_epochs = max(3, int(total_epochs * 0.2))\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    log_memory_usage(\"Before Stage 1 Training\")\n    \n    # Ensure base models are frozen\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'efficientnet_base']):\n            for base_layer in layer.layers:\n                base_layer.trainable = False\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n    \n    # Memory cleanup\n    log_memory_usage(\"After Stage 1 Training\")\n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train only the last 15 layers of each base model\n    stage2_epochs = total_epochs - stage1_epochs\n    print(f\"\\nStage 2: Fine-tuning with reduced layers ({stage2_epochs} epochs)\")\n    log_memory_usage(\"Before Stage 2 Training\")\n    \n    # Unfreeze only the last 15 layers (instead of 30-50)\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'efficientnet_base']):\n            for base_layer in layer.layers[:-15]:\n                base_layer.trainable = False\n            for base_layer in layer.layers[-15:]:\n                base_layer.trainable = True\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(5e-5)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n    \n    log_memory_usage(\"After Stage 2 Training\")\n    return histories\n\n# =============================================================================\n# Memory-efficient Evaluation\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Memory-efficient model evaluation\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    log_memory_usage(f\"Before {dataset_name} Evaluation\")\n    \n    # Get predictions in smaller batches\n    y_true = []\n    y_pred = []\n    \n    # Use limited steps for evaluation\n    actual_steps = min(steps, 30)\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= actual_steps:\n            break\n        # Use smaller batches if needed\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Cleanup after each batch\n        if i % 5 == 0:\n            gc.collect()\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Save smaller confusion matrix visualization\n    plt.figure(figsize=(8, 6))  # Smaller figure\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png', dpi=100)  # Lower DPI\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    log_memory_usage(f\"After {dataset_name} Evaluation\")\n    \n    # Memory cleanup\n    gc.collect()\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Main training pipeline with memory monitoring\n# =============================================================================\ndef train_emotion_ensemble(data_dir, limit_samples=None):\n    \"\"\"\n    Memory-efficient training pipeline for emotion recognition\n    \n    Args:\n        data_dir: Path to dataset directory\n        limit_samples: Optional limit on samples per class (for testing)\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting memory-optimized ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data with optional sample limiting\n    print(\"\\n1. Loading datasets\")\n    log_memory_usage(\"Initial\")\n    \n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir, limit_samples=limit_samples)\n    test_df = build_image_df(test_dir, limit_samples=limit_samples)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    log_memory_usage(\"After dataframe creation\")\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet'].copy()  # Use copy to avoid view\n    test_fer_df = test_df[test_df['source'] == 'fer2013'].copy()  # Use copy to avoid view\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits with separate DataFrames\n    print(\"\\n2. Creating train/validation splits\")\n    \n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet'].copy()\n    affectnet_train_idx, affectnet_val_idx = train_test_split(\n        np.arange(len(affectnet_train_df)), \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    affectnet_val_df = affectnet_train_df.iloc[affectnet_val_idx].copy()\n    affectnet_train_df = affectnet_train_df.iloc[affectnet_train_idx].copy()\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013'].copy()\n    fer_train_idx, fer_val_idx = train_test_split(\n        np.arange(len(fer_train_df)), \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    fer_val_df = fer_train_df.iloc[fer_val_idx].copy()\n    fer_train_df = fer_train_df.iloc[fer_train_idx].copy()\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # Clear the full dataframe to save memory\n    del train_df_full\n    gc.collect()\n    log_memory_usage(\"After train/val split\")\n    \n    # 4. Create datasets with disk-based caching\n    print(\"\\n3. Creating memory-efficient data pipelines with disk caching\")\n    \n    # AffectNet datasets - Create only what's needed for current training stage\n    print(\"Creating AffectNet datasets...\")\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True, cache=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    log_memory_usage(\"After AffectNet dataset creation\")\n    \n    # Calculate steps with increased batch size\n    affectnet_steps_per_epoch = max(1, len(affectnet_train_df) // BATCH_SIZE // 2)  # Reduce steps to save time\n    affectnet_val_steps = max(1, len(affectnet_val_df) // BATCH_SIZE)\n    \n    # 5. Create lightweight ensemble model\n    print(\"\\n4. Creating lightweight ensemble model\")\n    ensemble_model = create_lightweight_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    log_memory_usage(\"After model creation\")\n    \n    # 6. Compute class weights for AffectNet\n    print(\"\\n5. Computing class weights for AffectNet\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    \n    # 7. Setup AffectNet callbacks\n    print(\"\\n6. Setting up AffectNet callbacks\")\n    \n    # Memory-efficient callbacks\n    affectnet_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,  # Reduced from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,  # Save only weights to reduce file size\n            verbose=1\n        ),\n        # Memory-efficient confusion matrix monitoring (reduced frequency)\n        MemoryEfficientConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=5  # Increased from 3 to check less frequently\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 8. Train ensemble on AffectNet using memory-efficient strategy\n    print(\"\\n7. STAGE 1: Training ensemble on AffectNet with memory-efficient strategy\")\n    \n    affectnet_histories = train_ensemble_with_memory_efficient_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=15,  # Reduced from 20\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model (weights only to save space)\n    ensemble_model.save_weights(\"affectnet_ensemble_weights.h5\")\n    print(\"AffectNet ensemble model weights saved\")\n    \n    log_memory_usage(\"After AffectNet training\")\n    \n    # 9. Clean up AffectNet resources\n    del affectnet_train_ds, affectnet_val_ds, affectnet_train_df, affectnet_val_df\n    gc.collect()\n    \n    # 10. Create AffectNet test dataset for evaluation\n    print(\"\\n8. Evaluating on AffectNet test set\")\n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    # Calculate test steps\n    affectnet_test_steps = max(1, len(test_affectnet_df) // BATCH_SIZE)\n    \n    # 11. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 12. Clean up AffectNet test resources\n    del affectnet_test_ds, test_affectnet_df\n    gc.collect()\n    \n    # 13. Create FER2013 datasets\n    print(\"\\n9. Creating FER2013 datasets...\")\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True, cache=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    log_memory_usage(\"After FER2013 dataset creation\")\n    \n    # Calculate steps\n    fer_steps_per_epoch = max(1, len(fer_train_df) // BATCH_SIZE // 2)  # Reduce steps\n    fer_val_steps = max(1, len(fer_val_df) // BATCH_SIZE)\n    \n    # 14. Compute class weights for FER2013\n    print(\"\\n10. Computing class weights for FER2013\")\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 15. Setup FER2013 callbacks\n    print(\"\\n11. Setting up FER2013 callbacks\")\n    \n    # Memory-efficient callbacks\n    fer_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,  # Reduced from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Memory-efficient confusion matrix monitoring\n        MemoryEfficientConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=5  # Increased from 3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 16. STAGE 2: Fine-tune on FER2013 with memory-efficient strategy\n    print(\"\\n12. STAGE 2: Fine-tuning ensemble on FER2013 with memory-efficient strategy\")\n    \n    fer_histories = train_ensemble_with_memory_efficient_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=10,  # Reduced from 15\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    log_memory_usage(\"After FER2013 training\")\n    \n    # 17. Create FER2013 test dataset\n    print(\"\\n13. Evaluating on FER2013 test set\")\n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    # Calculate test steps\n    fer_test_steps = max(1, len(test_fer_df) // BATCH_SIZE)\n    \n    # 18. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 19. Save the final model (weights only)\n    ensemble_model.save_weights(\"final_emotion_ensemble_weights.h5\")\n    print(\"Final ensemble model weights saved\")\n    \n    # 20. Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {affectnet_metrics['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {affectnet_metrics['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {fer_metrics['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {fer_metrics['f1_score']:.4f}\")\n    \n    log_memory_usage(\"Final\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics\n    }\n\n# =============================================================================\n# Main entry point with memory limits\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Optional: Limit samples for testing (uncomment to use)\n    # Set to None for full dataset, or a number to limit samples per class\n    limit_samples = None  # e.g., 100 for quick testing\n    \n    try:\n        # Train with memory monitoring\n        #model, metrics = train_emotion_ensemble(data_dir, limit_samples=100)\n        model, metrics = train_emotion_ensemble(data_dir, limit_samples=None)\n        print(\"Training completed successfully!\")\n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        # Log the error\n        with open(os.path.join(LOG_DIR, \"error_log.txt\"), \"a\") as f:\n            f.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ERROR: {str(e)}\\n\")\n        raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Customized 2.0 (RAM MOSNTER)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nBATCH_SIZE = 64  # Increased from 32 to 64 for better GPU utilization\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# FIXED: Enhanced Image Preprocessing with Consistent Size\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with consistent output size for all images.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # CRITICAL: Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply basic augmentation during training\n    if training:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        img = tf.clip_by_value(img, 0.0, 255.0)  # Ensure valid range\n    \n    # Basic normalization to [0,1] range for consistency\n    # Note: Model-specific normalization will happen in the model\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with caching and repeat\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=True):\n    \"\"\"\n    Creates enhanced tf.data.Dataset with caching.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        cache: Whether to cache the dataset\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline with shuffle\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        # FIXED: Add repeat to provide enough batches for all epochs\n        ds = ds.repeat()\n    \n    # Cache the dataset if requested (major speed improvement)\n    if cache:\n        ds = ds.cache()\n    \n    # Batch and prefetch\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES, cache=True):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        cache: Whether to cache the dataset\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with caching\n    return create_dataset(balanced_df, is_training=is_training, cache=cache)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble with model-specific preprocessing layers.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # === MobileNetV2 Branch ===\n    # Resize and normalize for MobileNetV2\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,  # Scale back to [0,255] then normalize to [-1,1]\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    mobilenet_base = MobileNetV2(\n        include_top=False, \n        weights='imagenet',\n        input_tensor=mobilenet_preprocess,\n        alpha=1.0,\n        name='mobilenet_base'\n    )\n    \n    if freeze_base:\n        for layer in mobilenet_base.layers:\n            layer.trainable = False\n            \n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_base.output)\n    mobilenet_features = Dense(128, name='mobilenet_projection')(mobilenet_features)\n    mobilenet_features = BatchNormalization()(mobilenet_features)\n    mobilenet_features = Activation('relu')(mobilenet_features)\n    \n    # === Xception Branch ===\n    # Resize and normalize for Xception\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    xception_base = Xception(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=xception_preprocess,\n        name='efficientnet_base'\n    )\n    \n    if freeze_base:\n        for layer in xception_base.layers:\n            layer.trainable = False\n            \n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_base.output)\n    xception_features = Dense(128, name='xception_projection')(xception_features)\n    xception_features = BatchNormalization()(xception_features)\n    xception_features = Activation('relu')(xception_features)\n    \n    # === EfficientNetB0 Branch ===\n    # Normalize for EfficientNetB0 (already 224x224 size)\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    efficientnet_base = EfficientNetB0(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=efficientnet_preprocess\n    )\n    \n    if freeze_base:\n        for layer in efficientnet_base.layers:\n            layer.trainable = False\n            \n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_base.output)\n    efficientnet_features = Dense(128, name='efficientnet_projection')(efficientnet_features)\n    efficientnet_features = BatchNormalization()(efficientnet_features)\n    efficientnet_features = Activation('relu')(efficientnet_features)\n    \n    # === Feature Fusion ===\n    # Combine features with concatenation\n    merged_features = Concatenate(name='feature_fusion')(\n        [mobilenet_features, xception_features, efficientnet_features]\n    )\n    \n    # Classification head\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Output layer with softmax\n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='emotion_ensemble')\n    \n    # Cosine decay learning rate\n    initial_learning_rate = 1e-3\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate, \n        decay_steps=10000  # Adjust based on epochs * steps_per_epoch\n    )\n    \n    # Optimizer with loss scaling for mixed precision\n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    # Compile with focal loss\n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),  # Using focal loss instead of label smoothing\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Progressive Training Strategy for Ensemble\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (10% of total epochs)\n    stage1_epochs = max(3, int(total_epochs * 0.1))\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'xception_base', 'efficientnet_base']):\n            for base_layer in layer.layers:\n                base_layer.trainable = False\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (30% of total epochs)\n    stage2_epochs = max(6, int(total_epochs * 0.3))\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Unfreeze EfficientNet and MobileNet, keep Xception frozen\n    for layer in model.layers:\n        if layer.name in ['efficientnet_base', 'mobilenet_base']:\n            for base_layer in layer.layers[-30:]:  # Unfreeze last 30 layers\n                base_layer.trainable = True\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(1e-4)  # Lower learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze all models including Xception (which is more complex)\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'xception_base', 'efficientnet_base']):\n            for base_layer in layer.layers[-50:]:  # Unfreeze more layers\n                base_layer.trainable = True\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(5e-5)  # Very low learning rate for fine-tuning\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True, cache=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True, cache=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=8,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.keras\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.keras\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.keras'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train ensemble model with all improvements\n    model, metrics = train_emotion_ensemble(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# customized \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nBATCH_SIZE = 64  # Increased from 32 to 64 for better GPU utilization\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Enhanced Model-specific Image Preprocessing\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with model-specific handling.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing\n    if source == 'fer2013':\n        # Handle grayscale conversion correctly\n        if tf.shape(img)[-1] == 1:\n            # Already grayscale\n            img = tf.tile(img, [1, 1, 3])\n        else:\n            # Convert to grayscale then back to 3 channels\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # Apply basic augmentation during training\n    if training:\n        # Random flip - works in graph mode\n        img = tf.image.random_flip_left_right(img)\n        \n        # Basic brightness and contrast\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n            \n        # Add random noise to improve robustness\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        \n        # Additional augmentation for problematic classes\n        # Note: We need a more sophisticated approach outside this function\n        # as we can't check label values directly in a graph-compatible way\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# Enhanced dataset creation with caching\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=True):\n    \"\"\"\n    Creates enhanced tf.data.Dataset with caching.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        cache: Whether to cache the dataset\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline with shuffle\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n    \n    # Cache the dataset if requested (major speed improvement)\n    if cache:\n        ds = ds.cache()\n    \n    # Batch and prefetch\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES, cache=True):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        cache: Whether to cache the dataset\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with caching\n    return create_dataset(balanced_df, is_training=is_training, cache=cache)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# Create Ensemble Model Architecture\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble of MobileNetV2, Xception, and EfficientNetB0\n    with shared input but model-specific preprocessing.\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs\n    inputs = keras.layers.Input(shape=(None, None, 3), name='image_input')\n    \n    # === MobileNetV2 Branch ===\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    mobilenet_base = MobileNetV2(\n        include_top=False, \n        weights='imagenet',\n        input_tensor=mobilenet_preprocess,\n        alpha=1.0\n    )\n    \n    if freeze_base:\n        for layer in mobilenet_base.layers:\n            layer.trainable = False\n            \n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_base.output)\n    mobilenet_features = Dense(128, name='mobilenet_projection')(mobilenet_features)\n    mobilenet_features = BatchNormalization()(mobilenet_features)\n    mobilenet_features = Activation('relu')(mobilenet_features)\n    \n    # === Xception Branch ===\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    xception_base = Xception(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=xception_preprocess\n    )\n    \n    if freeze_base:\n        for layer in xception_base.layers:\n            layer.trainable = False\n            \n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_base.output)\n    xception_features = Dense(128, name='xception_projection')(xception_features)\n    xception_features = BatchNormalization()(xception_features)\n    xception_features = Activation('relu')(xception_features)\n    \n    # === EfficientNetB0 Branch ===\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(\n            tf.image.resize(x, [224, 224])\n        ),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    efficientnet_base = EfficientNetB0(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=efficientnet_preprocess\n    )\n    \n    if freeze_base:\n        for layer in efficientnet_base.layers:\n            layer.trainable = False\n            \n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_base.output)\n    efficientnet_features = Dense(128, name='efficientnet_projection')(efficientnet_features)\n    efficientnet_features = BatchNormalization()(efficientnet_features)\n    efficientnet_features = Activation('relu')(efficientnet_features)\n    \n    # === Feature Fusion ===\n    # Combine features with concatenation\n    merged_features = Concatenate(name='feature_fusion')(\n        [mobilenet_features, xception_features, efficientnet_features]\n    )\n    \n    # Classification head\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Output layer with softmax\n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='emotion_ensemble')\n    \n    # Cosine decay learning rate\n    initial_learning_rate = 1e-3\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate, \n        decay_steps=10000  # Adjust based on epochs * steps_per_epoch\n    )\n    \n    # Optimizer with loss scaling for mixed precision\n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    # Compile with focal loss\n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),  # Using focal loss instead of label smoothing\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Progressive Training Strategy for Ensemble\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (10% of total epochs)\n    stage1_epochs = max(3, int(total_epochs * 0.1))\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'xception_base', 'efficientnet_base']):\n            for base_layer in layer.layers:\n                base_layer.trainable = False\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (30% of total epochs)\n    stage2_epochs = max(6, int(total_epochs * 0.3))\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Unfreeze EfficientNet and MobileNet, keep Xception frozen\n    for layer in model.layers:\n        if layer.name in ['efficientnet_base', 'mobilenet_base']:\n            for base_layer in layer.layers[-30:]:  # Unfreeze last 30 layers\n                base_layer.trainable = True\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(1e-4)  # Lower learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze all models including Xception (which is more complex)\n    for layer in model.layers:\n        if any(base_name in layer.name for base_name in ['mobilenet_base', 'xception_base', 'efficientnet_base']):\n            for base_layer in layer.layers[-50:]:  # Unfreeze more layers\n                base_layer.trainable = True\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(5e-5)  # Very low learning rate for fine-tuning\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.0),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True, cache=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True)\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True, cache=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True)\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=8,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.keras\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.keras\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.keras'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train ensemble model with all improvements\n    model, metrics = train_emotion_ensemble(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6o current accuracy\n#=== FINAL RESULTS === 96x96\n#AffectNet Test Accuracy: 0.4461\n#AffectNet F1 Score: 0.4446\n#FER2013 Test Accuracy: 0.2203\n#FER2013 F1 Score: 0.1875\n#Combined Test Accuracy: 0.3168\n#Combined F1 Score: 0.3050\n\n#=== FINAL RESULTS === 224x224 \n#AffectNet Test Accuracy: 0.4669\n#AffectNet F1 Score: 0.4799\n#FER2013 Test Accuracy: 0.2953\n#FER2013 F1 Score: 0.2474\n#Combined Test Accuracy: 0.3050\n#Combined F1 Score: 0.2910\n\n#=== FINAL RESULTS === 112x112\n#AffectNet Test Accuracy: 0.4738\n#AffectNet F1 Score: 0.4748\n#FER2013 Test Accuracy: 0.3175\n#FER2013 F1 Score: 0.3021\n#Combined Test Accuracy: 0.3431\n#Combined F1 Score: 0.3168\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 112  # 96 48 112 224\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Custom Label Smoothing Loss\n# =============================================================================\ndef label_smoothing_loss(epsilon=0.1):\n    \"\"\"\n    Cross entropy loss with label smoothing to prevent model from being too confident.\n    \n    Args:\n        epsilon: Smoothing factor (0 = no smoothing, 1 = complete smoothing)\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        \n        # Apply label smoothing\n        y_true = y_true * (1.0 - epsilon) + (epsilon / num_classes)\n        \n        # Calculate cross entropy with extra small epsilon to prevent log(0)\n        return -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7), axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# Fixed graph-compatible preprocessing function\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Fixed graph-compatible preprocessing with class-specific augmentation.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize pixel values using MobileNet standard preprocessing\n    img = tf.cast(img, tf.float32)\n    img = img / 127.5 - 1.0  # Scale to [-1, 1]\n    \n    # Apply basic augmentation during training\n    if training:\n        # Random flip - works in graph mode\n        img = tf.image.random_flip_left_right(img)\n        \n        # Basic brightness and contrast\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n            \n        # Add random noise to improve robustness\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n        img = img + noise\n        \n        # Ensure values stay in valid range\n        img = tf.clip_by_value(img, -1.0, 1.0)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None):\n    \"\"\"\n    Creates a tf.data.Dataset with fixed preprocessing.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Repeat dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset\n    return create_dataset(balanced_df, is_training=is_training)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends instead of plotting them\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization (still useful)\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# Create emotion recognition model with additional MLP head\n# =============================================================================\ndef create_emotion_model(num_classes):\n    \"\"\"\n    Create a facial emotion recognition model with enhanced classification head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model and base model\n    \"\"\"\n    # Input shape\n    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n    \n    # Create input layer\n    inputs = keras.layers.Input(shape=input_shape)\n    \n    # Use MobileNetV2 as base\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs,\n        alpha=1.0  # Controls model width\n    )\n    print(\"Using MobileNetV2 base model\")\n    \n    # Freeze base model layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom head with dropout and batch normalization\n    x = base_model.output\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # First dense block\n    x = keras.layers.Dense(256)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.4)(x)\n    \n    # Second dense block\n    x = keras.layers.Dense(128)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    \n    # Output layer with label smoothing\n    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with label smoothing loss\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.1),\n        metrics=['accuracy']\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Two-Stage Fine-Tuning with Progressive Unfreezing\n# =============================================================================\ndef train_with_progressive_unfreezing(model, base_model, train_ds, val_ds, \n                                    steps_per_epoch, val_steps, \n                                    epochs_head=10, epochs_finetune=20,\n                                    callbacks=None, class_weights=None):\n    \"\"\"\n    Two-stage training approach: first train only the head, then progressively unfreeze layers.\n    \n    Args:\n        model: The model to train\n        base_model: The base model part (for unfreezing)\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        epochs_head: Epochs for head-only training\n        epochs_finetune: Epochs for fine-tuning\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    print(f\"\\nStage 1: Training only the classification head ({epochs_head} epochs)\")\n    \n    # Ensure base model is frozen\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Compile with higher learning rate for head training\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.1),\n        metrics=['accuracy']\n    )\n    \n    # Train head only\n    history_head = model.fit(\n        train_ds,\n        epochs=epochs_head,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    print(f\"\\nStage 2: Fine-tuning with progressive unfreezing ({epochs_finetune} epochs)\")\n    \n    # Progressively unfreeze layers in groups\n    fine_tuning_history = []\n    \n    # Groups of layers to unfreeze (from last to first)\n    layer_groups = [\n        # Unfreeze last layers first (deeper = more specific features)\n        base_model.layers[-15:],  # Last block\n        base_model.layers[-30:-15],  # Second-to-last block\n        base_model.layers[-50:-30]   # Third-to-last block\n    ]\n    \n    for i, group in enumerate(layer_groups):\n        print(f\"\\nUnfreezing group {i+1}/{len(layer_groups)} ({len(group)} layers)\")\n        \n        # Unfreeze current group\n        for layer in group:\n            layer.trainable = True\n            \n        # Recompile with lower learning rate as we go deeper\n        lr = 1e-4 / (i + 1)  # Decrease learning rate for deeper layers\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=lr),\n            loss=label_smoothing_loss(epsilon=0.1),\n            metrics=['accuracy']\n        )\n        \n        # Train for a few epochs\n        epochs_per_group = max(5, epochs_finetune // len(layer_groups))\n        \n        history = model.fit(\n            train_ds,\n            epochs=epochs_per_group,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        \n        fine_tuning_history.append(history)\n    \n    # Return combined history\n    return history_head, fine_tuning_history\n\n# =============================================================================\n# Sequential Training Pipeline\n# =============================================================================\ndef train_enhanced_emotion_model(data_dir):\n    \"\"\"\n    Enhanced sequential training with all improvements.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced sequential emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes\n    print(\"\\n2. Creating enhanced data pipelines\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet')\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet')\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013')\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013')\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False)\n    \n    # 5. Calculate steps\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create enhanced model\n    print(\"\\n3. Creating enhanced model\")\n    model, base_model = create_emotion_model(num_classes)\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"]),\n        y=affectnet_train_df[\"label\"]\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"]), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"]),\n        y=fer_train_df[\"label\"]\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"]), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR,\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train on AffectNet using progressive unfreezing\n    print(\"\\n6. STAGE 1: Training on AffectNet with progressive unfreezing\")\n    \n    history_affectnet_head, history_affectnet_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        epochs_head=10, epochs_finetune=15,\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    model.save(\"affectnet_model.keras\")\n    print(\"AffectNet model saved to 'affectnet_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive unfreezing\n    print(\"\\n7. STAGE 2: Fine-tuning on FER2013 with progressive unfreezing\")\n    \n    history_fer_head, history_fer_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        epochs_head=8, epochs_finetune=12,\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    model.save(\"final_enhanced_emotion_model.keras\")\n    print(\"Final model saved to 'final_enhanced_emotion_model.keras'\")\n    \n    # Return models and metrics\n    return model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model with all improvements\n    model, metrics = train_enhanced_emotion_model(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Version 6v accuracy 0.2294\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 96  # Keep at 96x96 as specified\nBATCH_SIZE = 128\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# Create all required directories\ndef ensure_dir(directory):\n    \"\"\"Make sure a directory exists, creating it if necessary\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(\"./model_checkpoints\")\n\n# Define problematic classes for targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust']\n\n# =============================================================================\n# Custom Label Smoothing Loss\n# =============================================================================\ndef label_smoothing_loss(epsilon=0.1):\n    \"\"\"\n    Cross entropy loss with label smoothing to prevent model from being too confident.\n    \n    Args:\n        epsilon: Smoothing factor (0 = no smoothing, 1 = complete smoothing)\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        \n        # Apply label smoothing\n        y_true = y_true * (1.0 - epsilon) + (epsilon / num_classes)\n        \n        # Calculate cross entropy with extra small epsilon to prevent log(0)\n        return -tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-7), axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# Fixed graph-compatible preprocessing function with stronger augmentation\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Fixed graph-compatible preprocessing with stronger augmentation.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize pixel values using MobileNet standard preprocessing\n    img = tf.cast(img, tf.float32)\n    img = img / 127.5 - 1.0  # Scale to [-1, 1]\n    \n    # Apply enhanced augmentation during training\n    if training:\n        # Random flip - works in graph mode\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced brightness and contrast (stronger than before)\n        img = tf.image.random_brightness(img, 0.3)  # Increased from 0.2\n        img = tf.image.random_contrast(img, 0.7, 1.3)  # Increased range\n        \n        # Add saturation variation\n        img = tf.image.random_saturation(img, 0.8, 1.5)\n        \n        # Apply random crop and resize for shape variation\n        # This simulates zoom/scale augmentation\n        crop_size = tf.random.uniform([], 0.8, 1.0, dtype=tf.float32)\n        scaled_size = tf.cast(tf.cast(tf.shape(img)[:2], tf.float32) * crop_size, tf.int32)\n        img = tf.image.random_crop(img, [scaled_size[0], scaled_size[1], 3])\n        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n            \n        # Add random noise to improve robustness\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.02)  # Increased noise\n        img = img + noise\n        \n        # Ensure values stay in valid range\n        img = tf.clip_by_value(img, -1.0, 1.0)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None):\n    \"\"\"\n    Creates a tf.data.Dataset with fixed preprocessing.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether this is for training (includes augmentation)\n        dataset_type: Optional filter for specific dataset ('affectnet' or 'fer2013')\n        \n    Returns:\n        tf.data.Dataset and class mapping\n    \"\"\"\n    # Optionally filter to specific dataset\n    if dataset_type is not None:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n        print(f\"Filtered to {len(dataframe)} {dataset_type} images\")\n    \n    # Create class indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing with training flag\n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Training pipeline\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Repeat dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        samples_per_class = 400  # Base sampling\n        \n        # Increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            samples_per_class = 600  # 50% more samples for problematic classes\n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset\n    return create_dataset(balanced_df, is_training=is_training)\n\n# =============================================================================\n# Fixed Confusion Matrix Callback without plotting errors\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 30  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n⚠️ WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n        \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends instead of plotting them\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization (still useful)\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n# =============================================================================\n# Create emotion recognition model with additional MLP head\n# =============================================================================\ndef create_emotion_model(num_classes):\n    \"\"\"\n    Create a facial emotion recognition model with enhanced classification head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model and base model\n    \"\"\"\n    # Input shape\n    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n    \n    # Create input layer\n    inputs = keras.layers.Input(shape=input_shape)\n    \n    # Use MobileNetV2 as base\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs,\n        alpha=1.0  # Controls model width\n    )\n    print(\"Using MobileNetV2 base model\")\n    \n    # Freeze base model layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom head with dropout and batch normalization\n    x = base_model.output\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # First dense block - wider layers for better capacity\n    x = keras.layers.Dense(512)(x)  # Increased from 256\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.4)(x)\n    \n    # Second dense block\n    x = keras.layers.Dense(256)(x)  # Increased from 128\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    \n    # New third dense block for better capacity\n    x = keras.layers.Dense(128)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    \n    # Output layer with label smoothing\n    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with label smoothing loss\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.2),  # Increased from 0.1\n        metrics=['accuracy']\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# Two-Stage Fine-Tuning with Progressive Unfreezing\n# =============================================================================\ndef train_with_progressive_unfreezing(model, base_model, train_ds, val_ds, \n                                    steps_per_epoch, val_steps, \n                                    epochs_head=10, epochs_finetune=20,\n                                    callbacks=None, class_weights=None):\n    \"\"\"\n    Two-stage training approach: first train only the head, then progressively unfreeze layers.\n    \n    Args:\n        model: The model to train\n        base_model: The base model part (for unfreezing)\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        epochs_head: Epochs for head-only training\n        epochs_finetune: Epochs for fine-tuning\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    print(f\"\\nStage 1: Training only the classification head ({epochs_head} epochs)\")\n    \n    # Ensure base model is frozen\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Compile with higher learning rate for head training\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss=label_smoothing_loss(epsilon=0.2),\n        metrics=['accuracy']\n    )\n    \n    # Train head only\n    history_head = model.fit(\n        train_ds,\n        epochs=epochs_head,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    print(f\"\\nStage 2: Fine-tuning with progressive unfreezing ({epochs_finetune} epochs)\")\n    \n    # Progressively unfreeze layers in groups\n    fine_tuning_history = []\n    \n    # Groups of layers to unfreeze (from last to first)\n    layer_groups = [\n        # Unfreeze last layers first (deeper = more specific features)\n        base_model.layers[-15:],  # Last block\n        base_model.layers[-30:-15],  # Second-to-last block\n        base_model.layers[-50:-30]   # Third-to-last block\n    ]\n    \n    for i, group in enumerate(layer_groups):\n        print(f\"\\nUnfreezing group {i+1}/{len(layer_groups)} ({len(group)} layers)\")\n        \n        # Unfreeze current group\n        for layer in group:\n            layer.trainable = True\n            \n        # Recompile with lower learning rate as we go deeper\n        lr = 1e-4 / (i + 1)  # Decrease learning rate for deeper layers\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=lr),\n            loss=label_smoothing_loss(epsilon=0.2),  # Keep consistent\n            metrics=['accuracy']\n        )\n        \n        # Train for a few epochs\n        epochs_per_group = max(5, epochs_finetune // len(layer_groups))\n        \n        history = model.fit(\n            train_ds,\n            epochs=epochs_per_group,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=val_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            verbose=1\n        )\n        \n        fine_tuning_history.append(history)\n    \n    # Return combined history\n    return history_head, fine_tuning_history\n\n# =============================================================================\n# Sequential Training Pipeline\n# =============================================================================\ndef train_enhanced_emotion_model(data_dir):\n    \"\"\"\n    Enhanced sequential training with all improvements.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced sequential emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes\n    print(\"\\n2. Creating enhanced data pipelines\")\n    \n    # AffectNet datasets\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True)\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet')\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet')\n    \n    # FER2013 datasets\n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True)\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013')\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013')\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False)\n    \n    # 5. Calculate steps\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create enhanced model\n    print(\"\\n3. Creating enhanced model\")\n    model, base_model = create_emotion_model(num_classes)\n    \n    # 7. Compute class weights for each dataset with adjustments\n    print(\"\\n4. Computing class weights with adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"]),\n        y=affectnet_train_df[\"label\"]\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"]), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"]),\n        y=fer_train_df[\"label\"]\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"]), fer_weights)}\n    \n    # Increase weights for problematic classes\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 20%\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.2\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.2\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # TensorBoard\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR,\n            histogram_freq=1,\n            update_freq='epoch'\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring (fixed version)\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring (fixed version)\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train on AffectNet using progressive unfreezing\n    print(\"\\n6. STAGE 1: Training on AffectNet with progressive unfreezing\")\n    \n    history_affectnet_head, history_affectnet_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        epochs_head=10, epochs_finetune=15,\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    model.save(\"affectnet_model.keras\")\n    print(\"AffectNet model saved to 'affectnet_model.keras'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_model(\n        model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive unfreezing\n    print(\"\\n7. STAGE 2: Fine-tuning on FER2013 with progressive unfreezing\")\n    \n    history_fer_head, history_fer_finetune = train_with_progressive_unfreezing(\n        model, base_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        epochs_head=8, epochs_finetune=12,\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_model(\n        model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_model(\n        model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    model.save(\"final_enhanced_emotion_model.keras\")\n    print(\"Final model saved to 'final_enhanced_emotion_model.keras'\")\n    \n    # Return models and metrics\n    return model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model with all improvements\n    model, metrics = train_enhanced_emotion_model(data_dir)\n    \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6.0z accuracy 0.1882 class balance deficit \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom sklearn.model_selection import train_test_split\n\n# =============================================================================\n# Enable memory growth and mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Enable mixed precision\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# =============================================================================\n# Key parameters\n# =============================================================================\nIMG_SIZE = 96  # Keep at 96x96 as requested\nBATCH_SIZE = 128  # Moderate batch size\nEPOCHS = 50\nAUTOTUNE = tf.data.AUTOTUNE\n\n# =============================================================================\n# Custom Focal Loss Implementation\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Focal Loss implementation for multi-class classification.\n    Focal Loss is designed to address class imbalance by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter. Higher values mean more focus on hard examples.\n        alpha: Class weight factor. Higher values give more weight to minority classes.\n        \n    Returns:\n        Focal loss function\n    \"\"\"\n    def focal_loss_fn(y_true, y_pred):\n        epsilon = 1e-7\n        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n        \n        # Calculate cross entropy\n        cross_entropy = -y_true * K.log(y_pred)\n        \n        # Apply focal weight\n        weight = alpha * K.pow(1 - y_pred, gamma) * y_true\n        \n        # Sum over classes\n        focal_loss = K.sum(weight * cross_entropy, axis=-1)\n        return K.mean(focal_loss)\n    \n    return focal_loss_fn\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# Data Augmentation for Minority Classes\n# =============================================================================\ndef augment_minority_classes(df, target_count=5000, minority_classes=None):\n    \"\"\"\n    Augment minority classes by duplicating samples to achieve more balanced class distribution.\n    \n    Args:\n        df: DataFrame with image paths and labels\n        target_count: Target number of samples per class\n        minority_classes: List of specific classes to augment (if None, determined automatically)\n        \n    Returns:\n        Augmented DataFrame\n    \"\"\"\n    print(\"Class distribution before augmentation:\")\n    print(df['label'].value_counts())\n    \n    if minority_classes is None:\n        # Identify classes with fewer than target_count samples\n        class_counts = df['label'].value_counts()\n        minority_classes = class_counts[class_counts < target_count].index.tolist()\n    \n    augmented_data = []\n    for cls in minority_classes:\n        class_df = df[df['label'] == cls]\n        needed = target_count - len(class_df)\n        if needed <= 0:\n            continue\n            \n        # Sample with replacement if needed\n        print(f\"Augmenting class '{cls}': Adding {needed} samples\")\n        samples = class_df.sample(n=needed, replace=True)\n        augmented_data.append(samples)\n    \n    # Combine augmented data with original\n    augmented_df = pd.concat([df] + augmented_data, ignore_index=True)\n    \n    print(\"Class distribution after augmentation:\")\n    print(augmented_df['label'].value_counts())\n    \n    return augmented_df\n\n# =============================================================================\n# Improved preprocessing function\n# =============================================================================\ndef preprocess_image(file_path, label, source):\n    \"\"\"\n    Unified preprocessing function with consistent augmentation for both datasets.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode with better error handling\n    try:\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n    except tf.errors.InvalidArgumentError:\n        try:\n            img = tf.image.decode_image(img, channels=1, expand_animations=False)\n            img = tf.image.grayscale_to_rgb(img)\n        except:\n            # Create a blank image if decoding fails\n            img = tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n            print(f\"Warning: Failed to decode image at {file_path}\")\n    \n    # Ensure the image has the right shape and type\n    img = tf.ensure_shape(img, [None, None, 3])\n    \n    # Resize to target size\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Normalize to [0, 1]\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Apply consistent augmentation for both datasets\n    if tf.random.uniform([], 0, 1) > 0.5:\n        # Standard horizontal flipping\n        img = tf.image.random_flip_left_right(img)\n        \n        # Color augmentations\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_saturation(img, 0.8, 1.2)\n        \n        # Slight rotation (maximum 15 degrees)\n        angle = tf.random.uniform([], -0.25, 0.25)  # in radians\n        img = tf.image.rot90(img, k=tf.cast(angle * 2 / 3.14159, tf.int32))\n    \n    # Convert label to one-hot encoding\n    label = tf.one_hot(label, depth=8)  # Assuming 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Fixed dataset creation function\n# =============================================================================\ndef create_dataset(dataframe, is_training=True):\n    \"\"\"\n    Create an optimized tf.data.Dataset from a DataFrame with fixed repeating.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether to apply augmentations and shuffling\n        \n    Returns:\n        tf.data.Dataset\n    \"\"\"\n    # Convert labels to indices\n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset from file paths, labels, and sources\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing\n    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n    \n    if is_training:\n        # Training pipeline with augmentation\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Important: Always repeat the dataset for multiple epochs\n    ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Confusion Matrix Callback\n# =============================================================================\nclass ConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Callback to display confusion matrix during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, freq=5):\n        super(ConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        \n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.freq == 0:\n            # Get a batch of validation data\n            validation_steps = 30  # Limit to prevent too much computation\n            y_true = []\n            y_pred = []\n            \n            # Get predictions for validation data\n            for i, (images, labels) in enumerate(self.validation_data):\n                if i >= validation_steps:\n                    break\n                batch_preds = self.model.predict(images, verbose=0)\n                y_pred.append(np.argmax(batch_preds, axis=1))\n                y_true.append(np.argmax(labels.numpy(), axis=1))\n            \n            # Flatten the lists\n            y_true = np.concatenate(y_true)\n            y_pred = np.concatenate(y_pred)\n            \n            # Calculate confusion matrix\n            cm = confusion_matrix(y_true, y_pred)\n            \n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Calculate per-class accuracy\n            class_acc = cm.diagonal() / cm.sum(axis=1)\n            for i, (name, acc) in enumerate(zip(self.class_names, class_acc)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print()\n\n# =============================================================================\n# Model Creation\n# =============================================================================\ndef create_simplified_model(num_classes):\n    \"\"\"\n    Create a simplified EfficientNetB0 model with a smaller head.\n    \n    Args:\n        num_classes: Number of emotion classes\n        \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    # Input layer\n    inputs = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    # Load pre-trained model with imagenet weights\n    base_model = EfficientNetB0(\n        include_top=False, \n        weights=\"imagenet\", \n        input_tensor=inputs\n    )\n    \n    # Initially freeze all layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add custom classification head\n    x = base_model.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    \n    # Ensure final layer uses float32 for numerical stability\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    \n    # Create model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with focal loss\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\"accuracy\"]\n    )\n    \n    return model, base_model\n\n# =============================================================================\n# Main Training Function\n# =============================================================================\ndef train_emotion_recognition_model(data_dir):\n    \"\"\"\n    Complete training pipeline incorporating all improvements.\n    \n    Args:\n        data_dir: Path to the dataset directory\n        \n    Returns:\n        Trained model and evaluation metrics\n    \"\"\"\n    print(\"Starting improved facial emotion recognition training\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading dataset\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    print(f\"Original training data: {train_df_full.shape}\")\n    print(f\"Test data: {test_df.shape}\")\n    \n    # 2. Apply class balancing through augmentation\n    print(\"\\n2. Balancing class distribution\")\n    train_df_balanced = augment_minority_classes(train_df_full, target_count=5000)\n    \n    # 3. Split into train and validation sets\n    print(\"\\n3. Creating train/validation split\")\n    train_df, val_df = train_test_split(\n        train_df_balanced, \n        test_size=0.15, \n        stratify=train_df_balanced[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n    \n    # 4. Create fixed tf.data datasets\n    print(\"\\n4. Creating data pipelines\")\n    train_ds, class_indices = create_dataset(train_df, is_training=True)\n    val_ds, _ = create_dataset(val_df, is_training=False)\n    test_ds, _ = create_dataset(test_df, is_training=False)\n    \n    # Get class names in order\n    classes = sorted(train_df[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # Calculate steps\n    steps_per_epoch = len(train_df) // BATCH_SIZE\n    validation_steps = len(val_df) // BATCH_SIZE\n    \n    # 5. Create model\n    print(\"\\n5. Creating simplified model\")\n    model, base_model = create_simplified_model(num_classes)\n    print(\"Model created\")\n    \n    # 6. Calculate proper class weights\n    print(\"\\n6. Computing class weights\")\n    class_weights_array = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(train_df[\"label\"]),\n        y=train_df[\"label\"]\n    )\n    class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(train_df[\"label\"]), class_weights_array)}\n    print(\"Class weights:\", class_weights)\n    \n    # 7. Setup callbacks\n    print(\"\\n7. Setting up training callbacks\")\n    callbacks = [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'best_emotion_model.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=7,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate scheduler\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # Logging\n        tf.keras.callbacks.CSVLogger('training_log.csv', append=True),\n        # Confusion matrix\n        ConfusionMatrixCallback(val_ds, classes, freq=3)\n    ]\n    \n    # 8. Progressive training approach\n    print(\"\\n8. Stage 1: Training only the model head\")\n    history_stage1 = model.fit(\n        train_ds,\n        epochs=10,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    # 9. Fine-tune the upper layers\n    print(\"\\n9. Stage 2: Fine-tuning upper layers\")\n    # Unfreeze the top layers of the base model\n    for layer in base_model.layers[-30:]:\n        layer.trainable = True\n        \n    # Recompile with a lower learning rate\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\"accuracy\"]\n    )\n    \n    history_stage2 = model.fit(\n        train_ds,\n        epochs=20,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    # 10. Evaluate on test set\n    print(\"\\n10. Final evaluation\")\n    # Update test steps\n    test_steps = len(test_df) // BATCH_SIZE\n    \n    # Get predictions\n    all_predictions = []\n    all_labels = []\n    \n    # Loop through batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= test_steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        all_predictions.append(np.argmax(batch_preds, axis=1))\n        all_labels.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    all_predictions = np.concatenate(all_predictions)\n    all_labels = np.concatenate(all_labels)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(all_predictions == all_labels)\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    \n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Weighted F1-Score: {f1:.4f}\")\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        all_labels, \n        all_predictions, \n        target_names=classes,\n        zero_division=0\n    ))\n    \n    # Print confusion matrix\n    cm = confusion_matrix(all_labels, all_predictions)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # 11. Save the final model\n    model.save(\"final_improved_emotion_model.keras\")\n    print(\"Model saved to 'final_improved_emotion_model.keras'\")\n    \n    return model, {'accuracy': test_accuracy, 'f1_score': f1}\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    # Set data directory path\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Train model\n    model, metrics = train_emotion_recognition_model(data_dir)\n    \n    print(\"Training completed successfully!\")\n    print(f\"Final Test Accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"Final F1 Score: {metrics['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 6.0 accuracy 0.1832 \n\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\nfrom sklearn.model_selection import train_test_split\n\n# Enable memory growth to prevent TF from allocating all GPU memory at once\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Enable mixed precision for faster training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# Memory cleanup callback\nclass MemoryCleanupCallback(keras.callbacks.Callback):\n    \"\"\"Callback to clear TensorFlow session after each epoch to free memory.\"\"\"\n    def on_epoch_end(self, epoch, logs=None):\n        tf.keras.backend.clear_session()\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nIMG_SIZE = 96  # Unified image size for both datasets\nBATCH_SIZE = 32  # Start with a conservative value\nEPOCHS = 30\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Try to determine optimal batch size based on available GPU memory\n# Start with default batch size\noptimal_batch_size = BATCH_SIZE\n\n# Try to detect available GPU memory and adjust batch size\ntry:\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        # Get GPU memory info if possible (works on some systems)\n        gpu_details = tf.config.experimental.get_device_details(gpus[0])\n        if 'memory_limit' in gpu_details:\n            # Memory in bytes, convert to GB\n            gpu_memory_gb = gpu_details['memory_limit'] / (1024**3)\n            \n            # Simple heuristic: 1GB supports batch size of ~16 for this model\n            if gpu_memory_gb > 14:  # High-end GPU (16GB+)\n                optimal_batch_size = 128\n            elif gpu_memory_gb > 7:  # Mid-range GPU (8GB)\n                optimal_batch_size = 64\n            else:  # Lower memory GPU\n                optimal_batch_size = 32\n                \n            print(f\"Detected {gpu_memory_gb:.1f}GB GPU memory, setting batch size to {optimal_batch_size}\")\n        else:\n            # If we can't detect memory, try a reasonable default for Kaggle\n            optimal_batch_size = 64\n            print(f\"Could not detect GPU memory, using default batch size of {optimal_batch_size}\")\nexcept Exception:\n    # If anything fails, stay with the default\n    print(f\"Using default batch size of {optimal_batch_size}\")\n\n# Update batch size to optimal value\nBATCH_SIZE = optimal_batch_size\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure - Keeping your original function\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")\ntest_dir = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# Add data augmentation specifically for underrepresented classes\ndef augment_minority_classes(df, target_count=5000, minority_classes=None):\n    if minority_classes is None:\n        # Identify classes with fewer than target_count samples\n        class_counts = df['label'].value_counts()\n        minority_classes = class_counts[class_counts < target_count].index.tolist()\n    \n    augmented_data = []\n    for cls in minority_classes:\n        class_df = df[df['label'] == cls]\n        needed = target_count - len(class_df)\n        if needed <= 0:\n            continue\n            \n        # Sample with replacement if needed\n        samples = class_df.sample(n=needed, replace=True)\n        augmented_data.append(samples)\n    \n    # Combine augmented data with original\n    return pd.concat([df] + augmented_data, ignore_index=True)\n\n# Use this before train/val split\ntrain_df_full = augment_minority_classes(train_df_full)\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(\n    train_df_full, \n    test_size=0.2, \n    stratify=train_df_full[\"label\"], \n    random_state=42\n)\n\n# Get class names and create mapping\nclasses = sorted(train_df_full[\"label\"].unique())\nclass_indices = {cls: i for i, cls in enumerate(classes)}\nnum_classes = len(classes)\n\n# =============================================================================\n# 4. Create an optimized tf.data pipeline\n# =============================================================================\ndef preprocess_image(file_path, label, source):\n    \"\"\"\n    Unified preprocessing function that handles both FER2013 and AffectNet images.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        \n    Returns:\n        Preprocessed image and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode with error handling\n    try:\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n    except:\n        img = tf.zeros([IMG_SIZE, IMG_SIZE, 3], dtype=tf.uint8)\n    \n    # Ensure shape and resize\n    img = tf.ensure_shape(img, [None, None, 3])\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE], method='bilinear')\n    \n    # Apply consistent preprocessing for both datasets\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Apply the same augmentation regardless of source\n    if tf.random.uniform([], 0, 1) > 0.5:\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_brightness(img, 0.2)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=num_classes)\n    \n    return img, label\n\ndef create_dataset(dataframe, is_training=True): # , cache=True\n    \"\"\"\n    Create an optimized tf.data.Dataset from a DataFrame.\n    \n    Args:\n        dataframe: DataFrame with filepath, label, and source columns\n        is_training: Whether to apply augmentations\n        cache: Whether to cache the dataset (disable for very large datasets)\n        \n    Returns:\n        tf.data.Dataset\n    \"\"\"\n    # Convert labels to indices\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    # Create dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Apply preprocessing and batching\n    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n    \n    if is_training:\n        ds = ds.shuffle(buffer_size=min(10000, len(dataframe)))\n        \n    # Make sure to repeat the dataset for multiple epochs\n    ds = ds.repeat()  # This is crucial\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    return ds\n\n# Create datasets\ntrain_ds = create_dataset(train_df, is_training=True)\nval_ds = create_dataset(val_df, is_training=False)\ntest_ds = create_dataset(test_df, is_training=False)\n\n# Calculate steps\nsteps_per_epoch = len(train_df) // BATCH_SIZE\nvalidation_steps = min(len(val_df) // BATCH_SIZE, 100)  # Limit validation steps\n\n# Sanity check - inspect a batch to verify the dataset pipeline\ndef inspect_dataset(dataset, name):\n    \"\"\"Inspect a dataset to verify it's created correctly.\"\"\"\n    print(f\"\\nInspecting {name} dataset:\")\n    \n    try:\n        # Get one batch\n        for images, labels in dataset.take(1):\n            print(f\"  Batch shape: {images.shape}\")\n            print(f\"  Labels shape: {labels.shape}\")\n            print(f\"  Data type: {images.dtype}\")\n            print(f\"  Min/Max values: {tf.reduce_min(images).numpy():.4f}/{tf.reduce_max(images).numpy():.4f}\")\n            \n            # Check for NaNs\n            has_nans = tf.math.reduce_any(tf.math.is_nan(images))\n            print(f\"  Contains NaNs: {has_nans.numpy()}\")\n            \n            # Verify one-hot labels\n            label_sums = tf.reduce_sum(labels, axis=1)\n            all_ones = tf.reduce_all(tf.equal(label_sums, 1))\n            print(f\"  Labels are valid one-hot: {all_ones.numpy()}\")\n            \n            # All checks passed\n            print(f\"  ✅ {name} dataset looks good!\")\n            return True\n    except Exception as e:\n        print(f\"  ❌ Error inspecting {name} dataset: {str(e)}\")\n        return False\n\n# Run sanity checks\ntrain_ok = inspect_dataset(train_ds, \"Training\")\nval_ok = inspect_dataset(val_ds, \"Validation\")\ntest_ok = inspect_dataset(test_ds, \"Test\")\n\n# Abort if datasets are not created correctly\nif not (train_ok and val_ok and test_ok):\n    print(\"\\n⚠️ Dataset sanity check failed! Please check the error messages above.\")\n    print(\"You can continue but training might fail.\")\n\n# =============================================================================\n# 5. Compute Class Weights to handle imbalance\n# =============================================================================\nclass_weights_array = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(train_df[\"label\"]),\n    y=train_df[\"label\"]\n)\nclass_weights = {class_indices[label]: weight for label, weight in \n                 zip(np.unique(train_df[\"label\"]), class_weights_array)}\n\n# =============================================================================\n# 6. Model Architecture with Optimized Transfer Learning\n# =============================================================================\ndef create_model():\n    \"\"\"\n    Create an EfficientNetB0 model with frozen layers and custom top.\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    # Create input layer with the correct shape\n    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    # Load pre-trained model with imagenet weights\n    base_model = EfficientNetB0(\n        include_top=False, \n        weights=\"imagenet\", \n        input_tensor=input_tensor\n    )\n    \n    # Freeze the first 70% of layers\n    freeze_until = int(len(base_model.layers) * 0.7)\n    for layer in base_model.layers[:freeze_until]:\n        layer.trainable = False\n    \n    # Add custom classification head\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n    x = Dropout(0.4)(x)\n    # Ensure final layer uses float32 for numerical stability with softmax\n    output = Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    \n    # Create model\n    model = Model(inputs=input_tensor, outputs=output)\n    \n    # Compile with Adam optimizer\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n# =============================================================================\n# 7. Learning Rate Scheduler and Callbacks\n# =============================================================================\ndef cosine_decay_schedule(epoch, lr):\n    \"\"\"\n    Cosine decay learning rate schedule.\n    \n    Args:\n        epoch: Current epoch\n        lr: Current learning rate\n        \n    Returns:\n        New learning rate\n    \"\"\"\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(math.pi * epoch / EPOCHS)) / 2\n\n# Create callbacks\ncallbacks = [\n    # Save checkpoints (using proper file extension for weights)\n    keras.callbacks.ModelCheckpoint(\n        'best_model.weights.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        save_weights_only=True,  # Save only weights to reduce I/O\n        verbose=1\n    ),\n    # Early stopping\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    # Learning rate scheduling\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    # Log training metrics\n    keras.callbacks.CSVLogger('training_log.csv', append=True),\n    # Memory cleanup after each epoch\n    MemoryCleanupCallback()\n\n]\n\n# =============================================================================\n# 8. Training\n# =============================================================================\ndef train_model(custom_callbacks=None, existing_model=None, quick_test=False):\n    \"\"\"\n    Train the model with the optimized pipeline.\n    \n    Args:\n        custom_callbacks: Additional callbacks to use during training\n        existing_model: Continue training this model if provided\n        quick_test: Whether this is a quick test run\n        \n    Returns:\n        Trained model and history\n    \"\"\"\n    # Create new model or use existing one\n    if existing_model is None:\n        model = create_model()\n        # Print model summary\n        model.summary()\n    else:\n        model = existing_model\n        print(\"Continuing training with existing model\")\n    \n    # Use custom callbacks if provided, otherwise use default callbacks\n    training_callbacks = custom_callbacks if custom_callbacks else callbacks\n    \n    # Adjust epochs and steps for quick test\n    current_epochs = 2 if quick_test else EPOCHS\n    current_steps = min(20, steps_per_epoch) if quick_test else steps_per_epoch\n    current_val_steps = min(10, validation_steps) if quick_test else validation_steps\n    \n    if quick_test:\n        print(f\"Quick test mode: {current_epochs} epochs, {current_steps} steps/epoch\")\n    \n    # Train model\n    history = model.fit(\n        train_ds,\n        epochs=current_epochs,\n        steps_per_epoch=current_steps,\n        validation_data=val_ds,\n        validation_steps=current_val_steps,\n        callbacks=training_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    return model, history\n\n# =============================================================================\n# 9. LR Finder - Fixed version with termination condition\n# =============================================================================\nclass LRFinder(keras.callbacks.Callback):\n    \"\"\"\n    Learning rate finder callback.\n    \n    This callback helps find the optimal learning rate by exponentially\n    increasing the learning rate during training and recording the loss.\n    \"\"\"\n    def __init__(self, min_lr=1e-7, max_lr=1e-2, steps=100, max_batches=1000):\n        super(LRFinder, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.steps = steps\n        self.max_batches = max_batches  # Safety limit\n        self.lrs = []\n        self.losses = []\n        self.batch_counter = 0\n        \n    def on_train_begin(self, logs=None):\n        # Store original learning rate\n        self.original_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        # Set initial learning rate to minimum\n        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.min_lr)\n        self.optimizer = self.model.optimizer\n    \n    def on_train_batch_end(self, batch, logs=None):\n        # Get current learning rate\n        lr = tf.keras.backend.get_value(self.optimizer.learning_rate)\n        self.lrs.append(lr)\n        self.losses.append(logs.get('loss'))\n        \n        # Calculate new learning rate\n        new_lr = lr * (self.max_lr / self.min_lr) ** (1/self.steps)\n        tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n        \n        # Increment counter and check for termination\n        self.batch_counter += 1\n        if batch >= self.steps or self.batch_counter >= self.max_batches:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs=None):\n        # Restore original learning rate\n        tf.keras.backend.set_value(self.optimizer.learning_rate, self.original_lr)\n        \n    def plot_lr_finder(self):\n        \"\"\"Plot the learning rate finder results.\"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            \n            plt.figure(figsize=(10, 6))\n            plt.plot(self.lrs, self.losses)\n            plt.xscale('log')\n            plt.xlabel('Learning Rate')\n            plt.ylabel('Loss')\n            plt.title('Learning Rate Finder')\n            plt.savefig('lr_finder_results.png')\n        except ImportError:\n            print(\"Matplotlib not available for plotting. Saving results to CSV instead.\")\n            import csv\n            with open('lr_finder_results.csv', 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(['learning_rate', 'loss'])\n                for lr, loss in zip(self.lrs, self.losses):\n                    writer.writerow([lr, loss])\n\n# =============================================================================\n# 10. Training with LR Finder\n# =============================================================================\ndef find_optimal_lr():\n    \"\"\"\n    Find the optimal learning rate using the LR Finder.\n    \n    Returns:\n        Suggested learning rate\n    \"\"\"\n    # Create model\n    model = create_model()\n    \n    # Create LR Finder callback\n    lr_finder = LRFinder(min_lr=1e-7, max_lr=1e-2, steps=100, max_batches=100)\n    \n    # Fit model for a few batches to find optimal LR\n    model.fit(\n        train_ds,\n        epochs=1,\n        steps_per_epoch=100,\n        callbacks=[lr_finder],\n        verbose=1\n    )\n    \n    # Plot results\n    lr_finder.plot_lr_finder()\n    \n    # Find the learning rate with the steepest negative gradient\n    losses = lr_finder.losses\n    lrs = lr_finder.lrs\n    \n    # Smoothing\n    smooth_losses = []\n    for i in range(len(losses)):\n        if i < 2 or i >= len(losses) - 2:\n            smooth_losses.append(losses[i])\n        else:\n            smooth_losses.append(sum(losses[i-2:i+3]) / 5)\n    \n    # Calculate gradients\n    gradients = []\n    for i in range(1, len(smooth_losses)):\n        gradients.append((smooth_losses[i] - smooth_losses[i-1]) / (lrs[i] - lrs[i-1]))\n    \n    # Find the point with the steepest negative gradient\n    steepest_idx = np.argmin(gradients)\n    optimal_lr = lrs[steepest_idx + 1] / 10  # Division by 10 is common practice\n    \n    print(f\"Suggested learning rate: {optimal_lr:.2e}\")\n    return optimal_lr\n\n# =============================================================================\n# 11. Evaluation\n# =============================================================================\ndef evaluate_model(model):\n    \"\"\"\n    Evaluate the model on the test set.\n    \n    Args:\n        model: Trained Keras model\n        \n    Returns:\n        Evaluation metrics\n    \"\"\"\n    # Evaluate model\n    loss, accuracy = model.evaluate(test_ds)\n    print(f\"\\nTest Loss: {loss:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    \n    # Get predictions\n    predictions = np.argmax(model.predict(test_ds), axis=-1)\n    \n    # Get true labels\n    true_labels = []\n    for _, y in test_ds.unbatch():\n        true_labels.append(np.argmax(y.numpy()))\n    true_labels = np.array(true_labels)\n    \n    # Calculate F1 score\n    f1 = f1_score(true_labels, predictions, average='weighted')\n    print(f\"\\nWeighted F1-Score: {f1:.4f}\")\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        true_labels, \n        predictions, \n        target_names=classes,\n        zero_division=0\n    ))\n    \n    return {\n        'loss': loss,\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'predictions': predictions,\n        'true_labels': true_labels\n    }\n\n# =============================================================================\n# 12. Fine-tune Model\n# =============================================================================\ndef fine_tune_model(model, epochs=5, custom_callbacks=None, quick_test=False):\n    \"\"\"\n    Fine-tune the model by unfreezing all layers.\n    \n    Args:\n        model: Trained model\n        epochs: Number of fine-tuning epochs\n        custom_callbacks: Additional callbacks to use during fine-tuning\n        quick_test: Whether this is a quick test run\n        \n    Returns:\n        Fine-tuned model and history\n    \"\"\"\n    # Unfreeze all layers\n    for layer in model.layers:\n        layer.trainable = True\n    \n    # Recompile with a lower learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    # Use custom callbacks if provided, otherwise use default callbacks\n    training_callbacks = custom_callbacks if custom_callbacks else callbacks\n    \n    # Adjust epochs and steps for quick test\n    current_epochs = 2 if quick_test else epochs\n    current_steps = min(20, steps_per_epoch) if quick_test else steps_per_epoch\n    current_val_steps = min(10, validation_steps) if quick_test else validation_steps\n    \n    if quick_test:\n        print(f\"Quick fine-tuning: {current_epochs} epochs, {current_steps} steps/epoch\")\n    \n    # Fine-tune\n    history = model.fit(\n        train_ds,\n        epochs=current_epochs,\n        steps_per_epoch=current_steps,\n        validation_data=val_ds,\n        validation_steps=current_val_steps,\n        callbacks=training_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    return model, history\n\n# =============================================================================\n# 13. Performance Monitoring\n# =============================================================================\nclass PerformanceMonitor(keras.callbacks.Callback):\n    \"\"\"\n    Monitor and log training performance metrics like time per step.\n    \"\"\"\n    def __init__(self):\n        super(PerformanceMonitor, self).__init__()\n        self.batch_times = []\n        self.epoch_start_time = None\n        \n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start_time = time.time()\n        self.batch_start_time = time.time()\n        self.batch_times = []\n        \n    def on_batch_end(self, batch, logs=None):\n        batch_time = time.time() - self.batch_start_time\n        self.batch_times.append(batch_time)\n        self.batch_start_time = time.time()\n        \n        # Log every 50 batches\n        if batch % 50 == 0:\n            avg_time = sum(self.batch_times[-50:]) / min(50, len(self.batch_times))\n            print(f\"\\nBatch {batch} - Avg time: {avg_time*1000:.2f}ms/step\")\n            \n            # Try to get GPU memory info if available\n            try:\n                import subprocess\n                gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader']).decode('utf-8')\n                print(f\"GPU Memory: {gpu_info.strip()}\")\n            except:\n                pass\n        \n    def on_epoch_end(self, epoch, logs=None):\n        epoch_time = time.time() - self.epoch_start_time\n        avg_batch_time = sum(self.batch_times) / len(self.batch_times)\n        print(f\"\\nEpoch {epoch+1} completed in {epoch_time:.2f}s - Avg: {avg_batch_time*1000:.2f}ms/step\")\n\n# =============================================================================\n# 14. Main training loop\n# =============================================================================\ndef main():\n    \"\"\"Main function to run the training pipeline.\"\"\"\n    # Print TF and GPU info\n    print(f\"TensorFlow version: {tf.__version__}\")\n    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n    \n    # Print dataset info\n    print(f\"Train samples: {len(train_df)} ({len(train_df[train_df['source'] == 'fer2013'])} FER, {len(train_df[train_df['source'] == 'affectnet'])} AffectNet)\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n    print(f\"Classes: {classes}\")\n    print(f\"Steps per epoch: {steps_per_epoch}\")\n    print(f\"Batch size: {BATCH_SIZE}\")\n    \n    # Set to True to run a quick test first, or False for full training\n    run_quick_test = True  # Change this manually if needed\n    \n    if run_quick_test:\n        print(\"\\nRunning quick test (2 epochs with limited steps)...\")\n        # Define variables for testing\n        quick_test_epochs = 2\n        quick_test_steps = min(20, steps_per_epoch)\n        print(f\"Quick test settings: {quick_test_epochs} epochs, {quick_test_steps} steps per epoch\")\n    \n    # Create performance monitor\n    perf_monitor = PerformanceMonitor()\n    \n    # Add performance monitor to callbacks\n    training_callbacks = callbacks + [perf_monitor]\n    \n    try:\n        # Find optimal learning rate (optional)\n        # optimal_lr = find_optimal_lr()\n        \n        # Train model (with quick test if selected)\n        print(\"\\n=== Starting initial training phase ===\")\n        \n        if run_quick_test:\n            # Run a quick test first\n            print(\"\\n=== Running quick test ===\")\n            model, quick_history = train_model(\n                custom_callbacks=training_callbacks,\n                quick_test=True\n            )\n            \n            # After successful quick test, continue with full training\n            print(\"\\n=== Quick test complete, continuing with full training ===\")\n            model, history = train_model(\n                custom_callbacks=training_callbacks,\n                existing_model=model\n            )\n        else:\n            # Full training from the start\n            model, history = train_model(custom_callbacks=training_callbacks)\n            \n            # Evaluate model after initial training\n            print(\"\\n=== Evaluating after initial training ===\")\n            metrics = evaluate_model(model)\n            \n            # Fine-tune model (with quick test mode if enabled)\n            print(\"\\n=== Starting fine-tuning phase ===\")\n            model, ft_history = fine_tune_model(\n                model, \n                epochs=5, \n                custom_callbacks=training_callbacks,\n                quick_test=run_quick_test\n            )\n        \n        # Final evaluation\n        print(\"\\n=== Final evaluation ===\")\n        final_metrics = evaluate_model(model)\n        \n        # Save the final model\n        model.save(\"final_emotion_model.keras\")\n        \n        print(\"\\nTraining complete! Final model saved as 'final_emotion_model.keras'\")\n        \n    except Exception as e:\n        import traceback\n        print(\"\\n*** ERROR DURING TRAINING ***\")\n        print(traceback.format_exc())\n        print(\"\\nDetailed error:\", str(e))\n        \n        # Try to save the model if it exists\n        try:\n            if 'model' in locals():\n                print(\"Attempting to save the model before exiting...\")\n                model.save(\"emergency_save_model.keras\")\n                print(\"Model saved as emergency_save_model.keras\")\n        except Exception as save_error:\n            print(f\"Failed to save model: {save_error}\")\n    \nif __name__ == \"__main__\":\n    import time\n    start_time = time.time()\n    main()\n    total_time = time.time() - start_time\n    print(f\"\\nTotal execution time: {total_time/60:.2f} minutes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version 5.0 orginal working code slightly\n\nimport os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\ntf.get_logger().setLevel(logging.INFO)\n\nimport gc\nclass MemoryCleanup(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n        tf.keras.backend.clear_session()\n\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nimg_size = 96         # We upscale FER images to 96x96\nbatch_size = 64\nepochs = 30\n\n# =============================================================================\n# Learning Rate Finder Callback (Fixed)\n# =============================================================================\nclass LRFinder(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr=1e-6, max_lr=1e-2, steps=100):\n        super(LRFinder, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.steps = steps\n        self.lrs = []\n        self.losses = []\n        \n    def on_train_begin(self, logs=None):\n        # Use optimizer.learning_rate (compatible with LossScaleOptimizer)\n        self.original_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        tf.keras.backend.set_value(self.model.optimizer.learning_rate, self.min_lr)\n        self.optimizer = self.model.optimizer\n    \n    def on_batch_end(self, batch, logs=None):\n        lr = tf.keras.backend.get_value(self.optimizer.learning_rate)\n        self.lrs.append(lr)\n        self.losses.append(logs.get('loss'))\n        new_lr = lr * (self.max_lr / self.min_lr) ** (1/self.steps)\n        tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n        if batch >= self.steps:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs=None):\n        tf.keras.backend.set_value(self.optimizer.learning_rate, self.original_lr)\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory (Train or Test) and returns a DataFrame with columns:\n      - filepath: full path to the image file\n      - label: the emotion (parent folder name)\n      - source: the subfolder name (e.g., fer2013 or affectnet)\n    Assumes directory structure: root_dir/emotion/subfolder/image.jpg\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")   # Case-sensitive!\ntest_dir  = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# Additional Plots: Separate by Source (Optional)\nfer_df = train_df_full[train_df_full[\"source\"] == \"fer2013\"]\naff_df = train_df_full[train_df_full[\"source\"] == \"affectnet\"]\n\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x=\"label\", data=fer_df, palette=\"coolwarm\")\nplt.title(\"FER2013 Training Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.countplot(x=\"label\", data=aff_df, palette=\"Spectral\")\nplt.title(\"AffectNet Training Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(train_df_full, test_size=0.2, stratify=train_df_full[\"label\"], random_state=42)\n\n# =============================================================================\n# X. Visualization Code for DataFrames\n# =============================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Seaborn style for a beautiful design.\nsns.set(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n\n# Plot 1: Training Data (Combined by Source)\nplt.figure(figsize=(16, 5))\nplt.subplot(1, 3, 1)\nsns.countplot(x=\"label\", hue=\"source\", data=train_df_full, palette=\"viridis\")\nplt.title(\"Training Data Volume (Combined)\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Source\")\n\n# Plot 2: Validation Data (Combined by Source)\nplt.subplot(1, 3, 2)\nif \"source\" in val_df.columns:\n    sns.countplot(x=\"label\", hue=\"source\", data=val_df, palette=\"viridis\")\n    plt.legend(title=\"Source\")\nelse:\n    sns.countplot(x=\"label\", data=val_df, palette=\"viridis\")\nplt.title(\"Validation Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\n# Plot 3: Test Data\nplt.subplot(1, 3, 3)\nif \"source\" in test_df.columns:\n    sns.countplot(x=\"label\", hue=\"source\", data=test_df, palette=\"viridis\")\n    plt.legend(title=\"Source\")\nelse:\n    sns.countplot(x=\"label\", data=test_df, palette=\"viridis\")\nplt.title(\"Test Data Volume\")\nplt.xlabel(\"Emotion Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# =============================================================================\n# 4. Source-Specific Augmentation Functions\n# =============================================================================\ndef apply_fer_augmentations(img):\n    # Gentle augmentations for low-res FER2013 images\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, max_delta=0.1)\n    return img\n\ndef apply_affectnet_augmentations(img):\n    # Stronger augmentations for high-res AffectNet images\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, max_delta=0.3)\n    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n    return img\n\ndef augment_based_on_source(img, source):\n    # Source-aware augmentation: assume img is a tensor in [0,1]\n    if source == \"fer2013\":\n        img = tf.image.rgb_to_grayscale(img)  # Ensure it's grayscale\n        img = apply_fer_augmentations(img)\n    else:\n        img = apply_affectnet_augmentations(img)\n    return img\n\n# =============================================================================\n# 5. Create Separate Generators for FER2013 and AffectNet\n# =============================================================================\n# Create a simpler, more robust training loop\n# First, simplify your dataset creation\n\nclasses = sorted(train_df_full[\"label\"].unique())\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255\n    #preprocessing_function=lambda x: np.repeat(x, 3, axis=-1) if x.shape[-1] == 1 else x\n)\n\nsimple_train_gen = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"rgb\",  # Use rgb for all images to simplify\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=16,  # Reduced batch size\n    shuffle=True\n)\n\n# Split the training DataFrame by source\nfer_train_df = train_df[train_df[\"source\"] == \"fer2013\"]\naff_train_df = train_df[train_df[\"source\"] == \"affectnet\"]\n\n# For FER2013: load as grayscale with gentle augmentation.\nfer_aug_params = {\n    \"rescale\": 1./255,\n    \"rotation_range\": 15,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"brightness_range\": [0.8, 1.2]\n}\nfer_datagen = ImageDataGenerator(**fer_aug_params)\nfer_gen = fer_datagen.flow_from_dataframe(\n    dataframe=fer_train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=64,\n    shuffle=True\n)\n\n# For AffectNet: load as RGB with stronger augmentation.\naff_aug_params = {\n    \"rescale\": 1./255,\n    \"rotation_range\": 30,  # Reduced from 40\n    \"brightness_range\": [0.7, 1.3],  # Reduced from [0.5, 1.5]\n    \"zoom_range\": 0.1  # Reduced from 0.2\n}\naff_datagen = ImageDataGenerator(**aff_aug_params)\naff_gen = aff_datagen.flow_from_dataframe(\n    dataframe=aff_train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=64,\n    shuffle=True\n)\n\n#######################################################################################\n# If your images are grayscale and need to be converted to RGB, add this:\ndef convert_grayscale_to_rgb(batch_x, batch_y):\n    # If the last dimension is 1, repeat it 3 times\n    if batch_x.shape[-1] == 1:\n        batch_x = np.repeat(batch_x, 3, axis=-1)\n    return batch_x, batch_y\n\n# Create a wrapper for your generator\ndef rgb_generator_wrapper(gen):\n    for batch_x, batch_y in gen:\n        yield convert_grayscale_to_rgb(batch_x, batch_y)\n\n# Use the wrapped generator\nrgb_train_gen = rgb_generator_wrapper(simple_train_gen)\n\n#######################################################################################\n\n# Calculate steps for each dataset.\nsteps_fer = math.ceil(len(fer_train_df) / 64)  # 22986/32=719\nsteps_aff = math.ceil(len(aff_train_df) / 64)  # 23209/32=726\nsteps_per_epoch = steps_fer + steps_aff        # 1445\n\n#steps_fer = math.ceil(fer_train_df.shape[0] / 32)\n#steps_aff = math.ceil(aff_train_df.shape[0] / 32)\n#steps_per_epoch = steps_fer + steps_aff\n#print(f\"Steps per epoch: {steps_per_epoch}\")\n\n# After splitting in Section 3:\nprint(f\"Train samples: {len(train_df)}\")  # Should be ~36k (80% of 46k)\nprint(f\"FER samples: {len(fer_train_df)}\")  # ~18k (80% of 23k)\nprint(f\"AffectNet samples: {len(aff_train_df)}\")  # ~18k (80% of 23k)\n\n# =============================================================================\n# 6. Enhanced Upscaling for FER2013: Bicubic interpolation and channel replication.\n# =============================================================================\ndef preprocess_fer_batch(batch):\n    # Input: (batch_size, H, W, 1)\n    upscaled = tf.image.resize(batch, [img_size, img_size], method=\"bicubic\")\n    return tf.repeat(upscaled, repeats=3, axis=-1)  # Now shape: (batch_size, img_size, img_size, 3)\n\n# Wrap the FER generator into a tf.data.Dataset.\ndef fer_gen_wrapper():\n    for batch in fer_gen:\n        images, labels = batch\n        images = tf.convert_to_tensor(images)\n        images = preprocess_fer_batch(images)\n        yield (images, labels)\n\nds_fer = tf.data.Dataset.from_generator(\n    fer_gen_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(fer_gen.class_indices)), dtype=tf.float32)\n    )\n)\n\n# For AffectNet, simply convert batches to tensors.\ndef aff_gen_wrapper():\n    for batch in aff_gen:\n        images, labels = batch\n        images = tf.convert_to_tensor(images)\n        yield (images, labels)\n\nds_aff = tf.data.Dataset.from_generator(\n    aff_gen_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(aff_gen.class_indices)), dtype=tf.float32)\n    )\n)\n\n# =============================================================================\n# 7. Combine the Two Datasets and Optimize Pipeline\n# =============================================================================\n# Both ds_fer and ds_aff already yield batches, so do NOT apply an additional .batch() here.\ncombined_ds = ds_fer.concatenate(ds_aff)\ncombined_ds = combined_ds.shuffle(buffer_size=200).prefetch(2)\n\n#combined_ds = ds_fer.concatenate(ds_aff)\n#combined_ds = combined_ds.shuffle(500).prefetch(tf.data.AUTOTUNE)\n\n# =============================================================================\n# 8. Compute Class Weights\n# =============================================================================\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(train_df[\"label\"]),\n    y=train_df[\"label\"]\n)\nlabel_to_index = fer_gen.class_indices  # Assumes consistency across sources.\nclass_weights = {label_to_index[label]: weight for label, weight in zip(np.unique(train_df[\"label\"]), class_weights)}\n\n# =============================================================================\n# 9. Model Architecture\n# =============================================================================\n# (Optional) Enable mixed precision\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\n\n#from tensorflow.keras.applications import MobileNetV2\n#base_model = MobileNetV2(include_top=False, weights=\"imagenet\", \n#                        input_tensor=input_tensor,\n#                        input_shape=(img_size, img_size, 3))\n\ninput_tensor = Input(shape=(img_size, img_size, 3))\nbase_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=input_tensor)\nbase_model.trainable = True\n\nfreeze_until = int(len(base_model.layers) * 0.7)\nfor layer in base_model.layers[:freeze_until]:\n    layer.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.5)(x)\nx = Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = Dropout(0.4)(x)\noutput = Dense(len(classes), activation=\"softmax\", dtype=\"float32\")(x)\nmodel = Model(inputs=input_tensor, outputs=output)\n\n# =============================================================================\n# 10. Focal Loss (Optional)\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy\n        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n    return loss_fn\n\nloss_function = focal_loss()  # Or use \"categorical_crossentropy\"\n\nsimple_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(len(classes), activation='softmax')\n])\n\nsimple_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss=loss_function, #'categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# =============================================================================\n# 11. Training Configuration & Callbacks\n# =============================================================================\n\ndef cosine_annealing(epoch, lr):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(math.pi * epoch / epochs)) / 2\n\n# Minimal callback list\nminimal_callbacks = [\n    tf.keras.callbacks.ModelCheckpoint('checkpoint.keras', save_best_only=True),\n    tf.keras.callbacks.CSVLogger('simple_training_log.csv', append=True)\n]\n\n# Create a validation dataset from val_df using a similar pipeline for FER images.\nval_datagen = ImageDataGenerator(rescale=1./255)\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",\n    classes=classes,\n    batch_size=batch_size,\n    shuffle=False\n)\ndef val_gen_wrapper():\n    for batch in val_generator:\n        images, labels = batch\n        images = preprocess_fer_batch(images)\n        yield (images, labels)\n\nds_val = tf.data.Dataset.from_generator(\n    lambda: val_gen_wrapper(),\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(classes)), dtype=tf.float32)\n    )\n).prefetch(tf.data.AUTOTUNE)\n\nstart_time = time.time()\nprint(\"Starting simplified training...\")\nsimple_history = simple_model.fit(\n    simple_train_gen,\n    epochs=2,\n    steps_per_epoch=50,  # Drastically reduced to test\n    validation_data=ds_val, #val_generator,\n    validation_steps=10,  # Also reduced\n    callbacks=minimal_callbacks\n)\nprint(\"Simplified training completed!\")\n\n# Save the trained model\nmodel.save(\"final_emotion_model.keras\")\n\n# =============================================================================\n# 12. Evaluation & Visualization on Test Data\n# =============================================================================\n# First, create a clean test generator without any preprocessing\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=(img_size, img_size),\n    color_mode=\"grayscale\",  # Load as grayscale first\n    class_mode=\"categorical\",\n    classes=classes,\n    batch_size=batch_size,\n    shuffle=False\n)\n\n# Then create a simpler wrapper that ensures exactly 3 channels\ndef test_wrapper():\n    for batch_x, batch_y in test_generator:\n        # Convert grayscale to RGB once - explicitly tracking shape\n        if batch_x.shape[-1] == 1:\n            # This creates exactly 3 channels\n            batch_x_rgb = np.concatenate([batch_x, batch_x, batch_x], axis=-1)\n            yield batch_x_rgb, batch_y\n        else:\n            yield batch_x, batch_y\n\n# Create the dataset with proper output signature\ntest_ds = tf.data.Dataset.from_generator(\n    test_wrapper,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, len(classes)), dtype=tf.float32)\n    )\n).batch(batch_size).prefetch(2)\n\n# Now evaluate with this clean dataset\nloss, accuracy = simple_model.evaluate(test_ds)\n\nprint(f\"\\nTest Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\npredictions = np.argmax(simple_model.predict(test_generator), axis=-1)\ntrue_labels = test_generator.classes\n\nprint(f\"\\nWeighted F1-Score: {f1_score(true_labels, predictions, average='weighted'):.4f}\")\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_matrix(true_labels, predictions), \n            annot=True, fmt=\"d\", \n            cmap=\"Blues\",\n            xticklabels=test_generator.class_indices.keys(),\n            yticklabels=test_generator.class_indices.keys())\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, predictions, target_names=test_generator.class_indices.keys()))\n\nplt.figure(figsize=(14, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Curves\")\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\nplt.title(\"Loss Curves\")\nplt.show()\n\n# =============================================================================\n# 13. Testing Saved Model on Individual Test Images\n# =============================================================================\nsaved_model = keras.models.load_model(\"final_emotion_model.keras\", compile=False)\nclass_labels = list(test_generator.class_indices.keys())\n\nfor class_name in class_labels:\n    class_path = os.path.join(test_dir, class_name)\n    if os.path.exists(class_path):\n        test_images = os.listdir(class_path)\n        print(f\"\\nTesting images for class: {class_name}\")\n        for img_name in test_images[:5]:\n            img_path = os.path.join(class_path, img_name)\n            # Load as grayscale then convert to RGB.\n            img = keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size), color_mode=\"grayscale\")\n            img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n            img_array = np.repeat(img_array, 3, axis=-1)\n            img_array = np.expand_dims(img_array, axis=0)\n            \n            prediction = saved_model.predict(img_array)\n            predicted_class = class_labels[np.argmax(prediction)]\n            confidence = np.max(prediction)\n            \n            plt.imshow(img_array[0].astype(\"float32\"))\n            plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n            plt.axis(\"off\")\n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fer_dirs = glob.glob(os.path.join(train_dir, \"*\", \"fer2013\"))\naff_dirs = glob.glob(os.path.join(train_dir, \"*\", \"affectnet\"))\nprint(\"FER directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"fer2013\")))\nprint(\"AffectNet directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"affectnet\")))\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW Experimental\n\nimport tensorflow.keras.backend as K\nimport os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0, MobileNetV3Small\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\n\n# =============================================================================\n# Define key parameters\n# =============================================================================\nimg_size = 96         # We upscale FER images to 96x96\nbatch_size = 64       # Smaller batch size to avoid memory issues\nepochs = 20           # Increase epochs, we'll use early stopping\nmodel_type = \"efficientnet\"  # Options: \"efficientnet\", \"mobilenet\", \"ensemble\"\n\n# =============================================================================\n# 1. Build a DataFrame from the dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the given root directory (Train or Test) and returns a DataFrame with columns:\n      - filepath: full path to the image file\n      - label: the emotion (parent folder name)\n      - source: the subfolder name (e.g., fer2013 or affectnet)\n    Assumes directory structure: root_dir/emotion/subfolder/image.jpg\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                for img_file in os.listdir(sub_path):\n                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        data.append({\n                            \"filepath\": os.path.join(sub_path, img_file),\n                            \"label\": emotion,\n                            \"source\": sub\n                        })\n    return pd.DataFrame(data)\n\n# =============================================================================\n# 2. Dataset Paths & DataFrame Creation\n# =============================================================================\n# Adjust this path to your actual data directory\ndata_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\ntrain_dir = os.path.join(data_dir, \"Train\")\ntest_dir  = os.path.join(data_dir, \"Test\")\n\ntrain_df_full = build_image_df(train_dir)\ntest_df = build_image_df(test_dir)\n\nprint(\"Train DataFrame shape:\", train_df_full.shape)\nprint(\"Test DataFrame shape:\", test_df.shape)\n\n# =============================================================================\n# 3. Split Training Data into Train & Validation Sets\n# =============================================================================\ntrain_df, val_df = train_test_split(train_df_full, test_size=0.2, stratify=train_df_full[\"label\"], random_state=42)\n\n# =============================================================================\n# 4. Data Visualization (Optional)\n# =============================================================================\ndef plot_data_distribution():\n    # Plot distribution of emotions by source\n    plt.figure(figsize=(16, 6))\n    \n    # Combined\n    plt.subplot(1, 3, 1)\n    sns.countplot(x=\"label\", data=train_df_full, palette=\"viridis\")\n    plt.title(\"Full Dataset Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    # FER2013\n    plt.subplot(1, 3, 2)\n    sns.countplot(x=\"label\", data=train_df_full[train_df_full[\"source\"] == \"fer2013\"], palette=\"coolwarm\")\n    plt.title(\"FER2013 Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    # AffectNet\n    plt.subplot(1, 3, 3)\n    sns.countplot(x=\"label\", data=train_df_full[train_df_full[\"source\"] == \"affectnet\"], palette=\"Spectral\")\n    plt.title(\"AffectNet Distribution\")\n    plt.xlabel(\"Emotion\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Uncomment to visualize data distribution\n# plot_data_distribution()\n\n# =============================================================================\n# 5. Optimized Data Generators\n# =============================================================================\n# Get emotion classes\nclasses = sorted(train_df_full[\"label\"].unique())\nprint(f\"Classes: {classes}\")\n\n# Define preprocessing functions\ndef preprocess_fer(img):\n    \"\"\"Convert grayscale to RGB and resize to target size\"\"\"\n    # Convert to 3 channels by repeating the grayscale channel\n    img = tf.image.grayscale_to_rgb(img)\n    # Resize to target size\n    img = tf.image.resize(img, [img_size, img_size], method='bicubic')\n    return img / 255.0  # Normalize\n\ndef preprocess_affectnet(img):\n    \"\"\"Resize RGB image to target size\"\"\"\n    img = tf.image.resize(img, [img_size, img_size])\n    return img / 255.0  # Normalize\n\n# Create training data generators\ndef create_generators():\n    # Common augmentation parameters\n    common_aug = {\n        'horizontal_flip': True,\n        'rotation_range': 20,\n        'fill_mode': 'nearest'\n    }\n    \n    # For FER2013 (grayscale)\n    fer_datagen = ImageDataGenerator(\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        brightness_range=[0.8, 1.2],\n        **common_aug\n    )\n    \n    fer_train_gen = fer_datagen.flow_from_dataframe(\n        dataframe=train_df[train_df[\"source\"] == \"fer2013\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    # For AffectNet (RGB)\n    aff_datagen = ImageDataGenerator(\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        brightness_range=[0.7, 1.3],\n        **common_aug\n    )\n    \n    aff_train_gen = aff_datagen.flow_from_dataframe(\n        dataframe=train_df[train_df[\"source\"] == \"affectnet\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    # Validation generators - no augmentation\n    val_datagen = ImageDataGenerator()\n    \n    fer_val_gen = val_datagen.flow_from_dataframe(\n        dataframe=val_df[val_df[\"source\"] == \"fer2013\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    aff_val_gen = val_datagen.flow_from_dataframe(\n        dataframe=val_df[val_df[\"source\"] == \"affectnet\"],\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    return fer_train_gen, aff_train_gen, fer_val_gen, aff_val_gen\n\n# Create test generator\ndef create_test_generator():\n    test_datagen = ImageDataGenerator()\n    \n    # Split test data by source\n    fer_test_df = test_df[test_df[\"source\"] == \"fer2013\"]\n    aff_test_df = test_df[test_df[\"source\"] == \"affectnet\"]\n    \n    fer_test_gen = test_datagen.flow_from_dataframe(\n        dataframe=fer_test_df,\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    aff_test_gen = test_datagen.flow_from_dataframe(\n        dataframe=aff_test_df,\n        x_col=\"filepath\",\n        y_col=\"label\",\n        target_size=(img_size, img_size),\n        color_mode=\"rgb\",\n        class_mode=\"categorical\",\n        classes=classes,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    return fer_test_gen, aff_test_gen\n\nfer_train_gen, aff_train_gen, fer_val_gen, aff_val_gen = create_generators()\nfer_test_gen, aff_test_gen = create_test_generator()\n\n# =============================================================================\n# 6. Custom Data Generator that combines both sources\n# =============================================================================\nclass CombinedGenerator:\n    def __init__(self, fer_gen, aff_gen, batch_size=32):\n        self.fer_gen = fer_gen\n        self.aff_gen = aff_gen\n        self.batch_size = batch_size\n        self.n_classes = len(classes)\n        self.fer_samples = len(fer_gen.filenames)\n        self.aff_samples = len(aff_gen.filenames)\n        self.total_samples = self.fer_samples + self.aff_samples\n        \n    def __len__(self):\n        return (self.total_samples + self.batch_size - 1) // self.batch_size\n    \n    def __iter__(self):\n        self.fer_iter = iter(self.fer_gen)\n        self.aff_iter = iter(self.aff_gen)\n        return self\n    \n    def __next__(self):\n        # Randomly choose which generator to pull from based on dataset size ratio\n        if np.random.random() < self.fer_samples / self.total_samples:\n            try:\n                batch_x, batch_y = next(self.fer_iter)\n                # Convert grayscale to RGB\n                if batch_x.shape[-1] == 1:\n                    batch_x = np.repeat(batch_x, 3, axis=-1)\n                return batch_x, batch_y\n            except StopIteration:\n                self.fer_iter = iter(self.fer_gen)\n                return next(self)\n        else:\n            try:\n                return next(self.aff_iter)\n            except StopIteration:\n                self.aff_iter = iter(self.aff_gen)\n                return next(self)\n\n# Create combined generators\ntrain_gen = CombinedGenerator(fer_train_gen, aff_train_gen, batch_size)\nval_gen = CombinedGenerator(fer_val_gen, aff_val_gen, batch_size)\ntest_gen = CombinedGenerator(fer_test_gen, aff_test_gen, batch_size)\n\n# =============================================================================\n# 7. Compute Class Weights for imbalanced dataset\n# =============================================================================\ndef compute_class_weights(train_df):\n    # Compute balanced class weights\n    class_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(train_df[\"label\"]),\n        y=train_df[\"label\"]\n    )\n    \n    # Map class names to indices\n    label_to_index = {label: i for i, label in enumerate(classes)}\n    \n    # Create dictionary of class weights\n    weights_dict = {label_to_index[label]: weight \n                    for label, weight in zip(np.unique(train_df[\"label\"]), class_weights)}\n    \n    return weights_dict\n\nclass_weights = compute_class_weights(train_df)\nprint(\"Class weights:\", class_weights)\n\n# =============================================================================\n# 8. Model Architecture\n# =============================================================================\ndef create_model(model_type=\"efficientnet\"):\n    input_tensor = Input(shape=(img_size, img_size, 3))\n    \n    if model_type == \"efficientnet\":\n        # EfficientNetB0 without any extra preprocessing\n        base_model = EfficientNetB0(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        # Freeze early layers\n        for layer in base_model.layers[:100]:\n            layer.trainable = False\n        \n        x = base_model.output\n        \n    elif model_type == \"mobilenet\":\n        # MobileNetV3Small\n        base_model = MobileNetV3Small(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        # Freeze early layers\n        for layer in base_model.layers[:50]:\n            layer.trainable = False\n        \n        x = base_model.output\n        \n    elif model_type == \"ensemble\":\n        # Use both models\n        efficient_base = EfficientNetB0(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        mobile_base = MobileNetV3Small(\n            include_top=False, \n            weights=\"imagenet\", \n            input_tensor=input_tensor\n        )\n        \n        # Freeze early layers\n        for layer in efficient_base.layers[:100]:\n            layer.trainable = False\n        for layer in mobile_base.layers[:50]:\n            layer.trainable = False\n        \n        # Global pooling for both models\n        efficient_features = GlobalAveragePooling2D()(efficient_base.output)\n        mobile_features = GlobalAveragePooling2D()(mobile_base.output)\n        \n        # Concatenate features\n        x = Concatenate()([efficient_features, mobile_features])\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n    \n    # Common head for all models\n    if model_type != \"ensemble\":\n        x = GlobalAveragePooling2D()(x)\n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    # Output layer\n    outputs = Dense(len(classes), activation=\"softmax\")(x)\n    \n    model = Model(inputs=input_tensor, outputs=outputs)\n\n    # Compile model with a fixed learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n# Simplify model to verify training works\nsimple_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(len(classes), activation='softmax')\n])\n\nsimple_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# =============================================================================\n# 9. Training with callbacks\n# =============================================================================\n\ncheckpoint_path = os.path.join(\"checkpoints\", f\"{model_type}_emotion_model.keras\")\nos.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n\ndef train_model(model, train_generator, val_generator, epochs=20, class_weights=None):\n    # Better learning rate schedule\n    steps_per_epoch = len(train_generator)\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-3,\n        decay_steps=steps_per_epoch * epochs,\n        alpha=1e-5\n    )\n\n    # Update the model's optimizer with the new learning rate schedule\n    model.optimizer.learning_rate = lr_schedule  # Direct assignment\n\n    checkpoint_path_keras = checkpoint_path + \".keras\"\n\n    # Callbacks\n    callbacks = [\n        # Save best model\n        ModelCheckpoint(\n            checkpoint_path,\n            save_weights_only=False,\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1\n        ),\n        # Early stopping\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            patience=7,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Reduce learning rate when plateau\n        ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=3,\n            min_lr=1e-6,\n            verbose=1\n        )\n    ]\n\n    # Calculate steps per epoch and validation steps\n    steps_per_epoch = len(train_generator)\n    validation_steps = len(val_generator)\n\n    print(f\"Steps per epoch: {steps_per_epoch}\")\n    print(f\"Validation steps: {validation_steps}\")\n\n    # Train model\n    start_time = time.time()\n    try:\n        history = model.fit(\n            train_generator,\n            epochs=epochs,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_generator,\n            validation_steps=validation_steps,\n            class_weight=class_weights,\n            callbacks=callbacks,\n            verbose=1\n        )\n    except Exception as e:\n        print(f\"Training error: {str(e)}\")\n        return None\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    return history\n\n# =============================================================================\n# 10. Evaluation\n# =============================================================================\ndef evaluate_model(model, test_generator):\n    # Calculate steps for test data\n    test_steps = len(test_generator)\n    \n    # Evaluate model\n    test_loss, test_acc = model.evaluate(test_generator, steps=test_steps)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    \n    # Get predictions\n    y_pred_probs = model.predict(test_generator, steps=test_steps)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    \n    # Get true labels\n    y_true = []\n    for i in range(test_steps):\n        try:\n            _, batch_y = next(iter(test_generator))\n            y_true.extend(np.argmax(batch_y, axis=1))\n        except StopIteration:\n            break\n    \n    # Limit to same size\n    y_true = y_true[:len(y_pred)]\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(\n        y_true, \n        y_pred, \n        target_names=classes\n    ))\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt=\"d\", \n        cmap=\"Blues\",\n        xticklabels=classes,\n        yticklabels=classes\n    )\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.show()\n    \n    return y_true, y_pred\n\n# =============================================================================\n# 11. Visualization Functions\n# =============================================================================\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# =============================================================================\n# 12. Test on sample images\n# =============================================================================\ndef test_on_samples(model, num_samples=3):\n    plt.figure(figsize=(16, 12))\n    \n    # Get sample images from each class\n    for i, emotion in enumerate(classes):\n        # Get sample paths\n        sample_paths = []\n        fer_samples = test_df[(test_df['label'] == emotion) & (test_df['source'] == 'fer2013')]['filepath'].values\n        aff_samples = test_df[(test_df['label'] == emotion) & (test_df['source'] == 'affectnet')]['filepath'].values\n        \n        if len(fer_samples) > 0:\n            sample_paths.append(fer_samples[0])\n        if len(aff_samples) > 0:\n            sample_paths.append(aff_samples[0])\n        \n        # Limit to num_samples\n        sample_paths = sample_paths[:num_samples]\n        \n        for j, img_path in enumerate(sample_paths):\n            # Load and preprocess image\n            color_mode = 'grayscale' if 'fer2013' in img_path else 'rgb'\n            img = keras.preprocessing.image.load_img(\n                img_path, \n                target_size=(img_size, img_size),\n                color_mode=color_mode\n            )\n            img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n            \n            # Convert grayscale to RGB if needed\n            if img_array.shape[-1] == 1:\n                img_array = np.repeat(img_array, 3, axis=-1)\n            \n            # Add batch dimension\n            img_batch = np.expand_dims(img_array, axis=0)\n            \n            # Predict\n            predictions = model.predict(img_batch)\n            predicted_class = classes[np.argmax(predictions[0])]\n            confidence = np.max(predictions[0]) * 100\n            \n            # Plot\n            plt_idx = i * num_samples + j + 1\n            if plt_idx <= num_samples * len(classes):\n                plt.subplot(len(classes), num_samples, plt_idx)\n                plt.imshow(img_array)\n                plt.title(f\"True: {emotion}\\nPred: {predicted_class}\\nConf: {confidence:.1f}%\")\n                plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# =============================================================================\n# MAIN EXECUTION\n# =============================================================================\ndef main():\n    # Debug information\n    print(\"Starting main function\")\n    print(f\"Model type: {model_type}\")\n    \n    try:\n        # Create and train model\n        model = create_model(model_type)\n        print(f\"Created {model_type} model\")\n    \n        # Print model summary\n        model.summary()\n    \n        # Train model\n        history = train_model(\n            model, \n            train_gen, \n            val_gen, \n            epochs=epochs, \n            class_weights=class_weights\n        )\n    \n        # Plot training history\n        plot_training_history(history)\n        \n        # Evaluate model\n        y_true, y_pred = evaluate_model(model, test_gen)\n        \n        # Test on sample images\n        test_on_samples(model)\n        \n        # Save final model\n        model.save(f\"final_{model_type}_emotion_model.keras\")\n        print(f\"Model saved as final_{model_type}_emotion_model.keras\")\n        \n        return model, history\n        \n    except Exception as e:\n        print(f\"Error in main function: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nif __name__ == \"__main__\":\n    # Set random seeds for reproducibility\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    \n    # Mixed precision for faster training\n    try:\n        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n        tf.keras.mixed_precision.set_global_policy(policy)\n        print(\"Using mixed precision\")\n    except:\n        print(\"Mixed precision not available\")\n    \n    # Train model\n    model, history = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
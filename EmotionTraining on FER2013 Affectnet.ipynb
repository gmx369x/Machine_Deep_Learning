{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7984578,"sourceType":"datasetVersion","datasetId":4699957},{"sourceId":10634054,"sourceType":"datasetVersion","datasetId":6583945},{"sourceId":11297645,"sourceType":"datasetVersion","datasetId":7064683},{"sourceId":367694,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":304690,"modelId":325141}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ResNet18 + MobileNetV2 + EfficientNetB0 /// 224 380 336\n# folcalloss is good for fer2013\n\n!pip install tensorflow\n!pip install numpy\n!pip install matplotlib\n!pip install psutil\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:11:47.462211Z","iopub.execute_input":"2025-04-19T10:11:47.462471Z","iopub.status.idle":"2025-04-19T10:11:59.733720Z","shell.execute_reply.started":"2025-04-19T10:11:47.462442Z","shell.execute_reply":"2025-04-19T10:11:59.732796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install mtcnn requests opencv-python","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import MTCNN\nimport matplotlib.pyplot as plt\nimport time\nimport glob\n\n# =============================================================================\n# Configuration\n# =============================================================================\n# --- Input ---\nINPUT_DIR = \"/kaggle/input/testing-samples01/samples\"\n# --- Models ---\n# Only using the new ensemble models with their specific image sizes\nMODEL_PATH_104 = \"/kaggle/input/computervision_ensemble_x/tensorflow2/3.5/1/acc83-img104ensemble_final.keras\"\nMODEL_PATH_112 = \"/kaggle/input/computervision_ensemble_x/tensorflow2/3.5/1/acc85-img114ensemble_final.keras\"  # Using 112 size instead of 114\n# --- Processing ---\n# Model-specific image sizes\nIMG_SIZE_104 = 104\nIMG_SIZE_112 = 112  # Changed from 114 to 112\nCLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n# --- MTCNN ---\nMTCNN_MIN_CONFIDENCE = 0.90\n# --- Display ---\nMODEL_COLORS = {\n    'Ensemble104': (255, 100, 100),  # Light Blue\n    'Ensemble112': (100, 255, 100),  # Light Green (renamed from Ensemble114)\n    'Default': (200, 200, 200),\n    'Emotion': (100, 100, 255),      # Light Red for final emotion results\n    'Distress_LOW': (141, 182, 0),   # Apple Green for Low Distress (updated)\n    'Distress_MODERATE': (255, 191, 52), # Mango for Moderate Distress (updated)\n    'Distress_HIGH': (243, 58, 106)  # Rose Red for High Distress (updated)\n}\nTEXT_COLOR = (0, 0, 0)\nFONT = cv2.FONT_HERSHEY_SIMPLEX\nFONT_SCALE = 0.4\nFONT_THICKNESS = 1\nBOX_PADDING = 20\nCORNER_LENGTH_FACTOR = 0.15\nCORNER_THICKNESS = 2\nLINE_SPACING = 15\n\n# --- Output ---\nSAVE_OUTPUT = True\nOUTPUT_DIR = \"/kaggle/working/emotion_output_v2_distress\"\n\n# =============================================================================\n# Load Emotion Models\n# =============================================================================\nprint(\"Loading emotion models...\")\nmodels = {}\ntry:\n    print(f\"Loading Model Ensemble104: {MODEL_PATH_104}\")\n    if not os.path.exists(MODEL_PATH_104): \n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_104}\")\n    models['Ensemble104'] = tf.keras.models.load_model(MODEL_PATH_104, compile=False)\n\n    print(f\"Loading Model Ensemble112: {MODEL_PATH_112}\")\n    if not os.path.exists(MODEL_PATH_112): \n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_112}\")\n    models['Ensemble112'] = tf.keras.models.load_model(MODEL_PATH_112, compile=False)\n\n    print(\"All models loaded successfully:\")\n    for name, model in models.items():\n        print(f\"- {name}: {model.name}\")\n\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    exit()\n\n# =============================================================================\n# Initialize Face Detector (MTCNN)\n# =============================================================================\nprint(\"Initializing MTCNN face detector...\")\ntry:\n    detector = MTCNN()\n    print(\"MTCNN detector initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing MTCNN detector: {e}\")\n    exit()\n\n# =============================================================================\n# Helper Functions\n# =============================================================================\n\ndef preprocess_face_for_emotion(face_image, target_size):\n    \"\"\"\n    Preprocess a face image for emotion detection models.\n    Resizes to the target size and normalizes pixel values.\n    \"\"\"\n    try:\n        # Use TensorFlow's resize function with specific size for each model\n        face_resized = tf.image.resize(face_image, [target_size, target_size], method='nearest')\n        face_processed = tf.cast(face_resized, tf.float32)\n        face_batch = tf.expand_dims(face_processed, axis=0)\n        return face_batch\n    except Exception as e:\n        print(f\"Error during face preprocessing: {e}\")\n        return None\n\ndef predict_emotions_all_models(face_crop, loaded_models, class_names):\n    \"\"\"\n    Predict emotions using both models with their appropriate image sizes.\n    Returns predictions and raw probabilities for averaging.\n    \"\"\"\n    predictions = {}\n    raw_predictions = {}\n    \n    try:\n        # Process for Ensemble104 model (104x104 image size)\n        if 'Ensemble104' in loaded_models:\n            face_batch_104 = preprocess_face_for_emotion(face_crop, IMG_SIZE_104)\n            if face_batch_104 is not None:\n                preds_probs = loaded_models['Ensemble104'].predict(face_batch_104, verbose=0)\n                predicted_class_index = np.argmax(preds_probs[0])\n                confidence = np.max(preds_probs[0])\n                predicted_emotion = class_names[predicted_class_index]\n                predictions['Ensemble104'] = (predicted_emotion, confidence)\n                raw_predictions['Ensemble104'] = preds_probs[0]\n        \n        # Process for Ensemble112 model (112x112 image size) - renamed from Ensemble114\n        if 'Ensemble112' in loaded_models:\n            face_batch_112 = preprocess_face_for_emotion(face_crop, IMG_SIZE_112)\n            if face_batch_112 is not None:\n                preds_probs = loaded_models['Ensemble112'].predict(face_batch_112, verbose=0)\n                predicted_class_index = np.argmax(preds_probs[0])\n                confidence = np.max(preds_probs[0])\n                predicted_emotion = class_names[predicted_class_index]\n                predictions['Ensemble112'] = (predicted_emotion, confidence)\n                raw_predictions['Ensemble112'] = preds_probs[0]\n        \n        # Create averaged prediction\n        if len(raw_predictions) >= 1:\n            # Initialize with zeros\n            avg_predictions = np.zeros(len(CLASS_NAMES))\n            \n            # Sum all predictions\n            for model_name, preds in raw_predictions.items():\n                avg_predictions += preds\n            \n            # Divide by number of models to get average\n            avg_predictions /= len(raw_predictions)\n            \n            # Get top prediction from averaged results\n            avg_class_index = np.argmax(avg_predictions)\n            avg_confidence = np.max(avg_predictions)\n            avg_emotion = class_names[avg_class_index]\n            \n            # Add to predictions\n            predictions['Emotion'] = (avg_emotion, avg_confidence)\n            \n        return predictions, raw_predictions\n    \n    except Exception as e:\n        print(f\"Error during emotion prediction: {e}\")\n        return {\"Error\": (f\"Prediction failed: {e}\", 0.0)}, None\n\n# --- Heuristic Distress Level Calculation Function (Using Averaged Model Predictions) ---\ndef calculate_heuristic_distress(emotion_predictions, raw_predictions=None):\n    \"\"\"\n    Calculates a heuristic 'distress' score based on weighted emotion predictions.\n    Uses averaged predictions from both models when available.\n    WARNING: This is an experimental heuristic and NOT a validated measure of stress.\n    \"\"\"\n    # Define heuristic stress mappings (emotion -> weight)\n    distress_weights = {\n        'anger': 0.9,    # High associated weight\n        'fear': 0.85,    # High associated weight\n        'disgust': 0.75, # High associated weight\n        'contempt': 0.40, # Moderate associated weight\n        'sad': 0.75,     # Moderate associated weight\n        'surprise': 0.55, # Neutral associated weight (can be positive or negative stress)\n        'neutral': 0.2,  # Low associated weight\n        'happy': 0.1     # Very low associated weight\n    }\n\n    # Prioritize the 'Emotion' prediction if it exists (renamed from 'Averaged')\n    if 'Emotion' in emotion_predictions:\n        emotion, confidence = emotion_predictions['Emotion']\n        if emotion in distress_weights:\n            distress_weight = distress_weights[emotion]\n            normalized_score = distress_weight * confidence\n            \n            # Map to categories based on arbitrary thresholds\n            if normalized_score > 0.65:  # Threshold for HIGH\n                level = \"HIGH\"\n            elif normalized_score > 0.35:  # Threshold for MODERATE\n                level = \"MODERATE\"\n            else:\n                level = \"LOW\"\n            return level, normalized_score\n    \n    # Fall back to weighted average of individual model predictions if no averaged prediction\n    weighted_score_sum = 0.0\n    confidence_sum = 0.0\n\n    # Calculate score based on top prediction from each model\n    for model_name, prediction in emotion_predictions.items():\n        if model_name == \"Error\" or model_name == \"Emotion\": continue\n        emotion, confidence = prediction\n        if emotion in distress_weights:\n            weighted_score_sum += distress_weights[emotion] * confidence\n            confidence_sum += confidence  # Use confidence as the denominator weight\n\n    if confidence_sum > 0:\n        # Normalize score by sum of confidences\n        normalized_score = weighted_score_sum / confidence_sum\n\n        # Map to categories based on arbitrary thresholds\n        if normalized_score > 0.65:  # Threshold for HIGH\n            level = \"HIGH\"\n        elif normalized_score > 0.35:  # Threshold for MODERATE\n            level = \"MODERATE\"\n        else:\n            level = \"LOW\"\n        return level, normalized_score\n    else:\n        return \"UNKNOWN\", 0.0\n\n# --- Drawing Function ---\ndef draw_results_custom_v2_distress(frame, faces_data):\n    \"\"\"Draws corner boxes, stacked emotion labels, and heuristic distress level.\"\"\"\n    for data in faces_data:\n        box = data[\"box\"]\n        emotion_predictions = data[\"emotion_predictions\"] # Dict of top emotion preds\n        distress_info = data[\"distress\"] # Tuple (level, score)\n        x1, y1, x2, y2 = box\n\n        # --- Draw Bounding Box Corners ---\n        corner_length = int(min(x2 - x1, y2 - y1) * CORNER_LENGTH_FACTOR)\n        corner_color = MODEL_COLORS.get('Emotion', MODEL_COLORS['Default'])\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2 - corner_length, y2), (x2, y2), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2, y2 - corner_length), (x2, y2), corner_color, CORNER_THICKNESS)\n\n        # --- Prepare and Draw Emotion Text Labels ---\n        y_offset = y1 - 7 # Starting position\n        max_w = 0 # Calculate max width needed for background\n        labels_to_draw = []\n        \n        # Only show the final Emotion prediction (renamed from 'Averaged')\n        if 'Emotion' in emotion_predictions:\n            emotion, confidence = emotion_predictions['Emotion']\n            label = f\"Emotion: {emotion} ({confidence:.2f})\"\n            labels_to_draw.append((label, 'Emotion'))\n            (w, h), _ = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n            if w > max_w: max_w = w\n\n        # Add Distress info label with color based on level\n        distress_level, distress_score = distress_info\n        distress_label = f\"Distress: {distress_level} ({distress_score:.2f})\"\n        distress_color_key = f\"Distress_{distress_level}\"\n        labels_to_draw.append((distress_label, distress_color_key))\n        (w, h), _ = cv2.getTextSize(distress_label, FONT, FONT_SCALE, FONT_THICKNESS)\n        if w > max_w: max_w = w\n\n        # Draw background and text for each label, moving upwards\n        num_labels = len(labels_to_draw)\n        bg_bottom = y1\n        for i, (label, label_type) in enumerate(reversed(labels_to_draw)):\n            text_y = y_offset - (i * LINE_SPACING)\n            bg_y1 = text_y - (LINE_SPACING - 3)\n            bg_y2 = text_y + 3\n\n            bg_y1 = max(0, bg_y1)\n            if bg_y1 >= bg_bottom or bg_y1 >= y1: continue\n            bg_bottom = bg_y1\n\n            # Use model color or specific distress color\n            bg_color = MODEL_COLORS.get(label_type, MODEL_COLORS['Default'])\n            cv2.rectangle(frame, (x1, bg_y1), (x1 + max_w + 5, bg_y2), bg_color, -1)\n            cv2.putText(frame, label, (x1 + 2, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS)\n\n\n# =============================================================================\n# Processing Functions for Image/Video Files\n# =============================================================================\n\ndef process_frame(frame, frame_rgb=None):\n    \"\"\"Detects faces, predicts emotions, calculates heuristic distress.\"\"\"\n    if frame_rgb is None:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    faces_data = []\n    try:\n        detections = detector.detect_faces(frame_rgb)\n    except Exception as e:\n        print(f\"Error during face detection: {e}\")\n        cv2.putText(frame, \"Face Detection Error\", (20, 40), FONT, 1, (0, 0, 255), 2)\n        return faces_data\n\n    if detections:\n         for face_info in detections:\n             try:\n                 confidence_face = face_info['confidence']\n                 if confidence_face < MTCNN_MIN_CONFIDENCE: continue\n\n                 x, y, w, h = face_info['box']\n                 x1 = max(0, x); y1 = max(0, y)\n                 x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                 if x1 >= x2 or y1 >= y2: continue\n\n                 x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                 x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                 if x1p >= x2p or y1p >= y2p: continue\n\n                 face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                 if face_crop_rgb.size == 0: continue\n\n                 # Process with both models and get raw predictions for averaging\n                 all_emotion_predictions, raw_predictions = predict_emotions_all_models(face_crop_rgb, models, CLASS_NAMES)\n\n                 # --- Calculate Heuristic Distress using the averaged predictions ---\n                 distress_level, distress_score = calculate_heuristic_distress(all_emotion_predictions, raw_predictions)\n\n                 faces_data.append({\n                     \"box\": (x1, y1, x2, y2),\n                     \"emotion_predictions\": all_emotion_predictions, # Store dict of top predictions\n                     \"distress\": (distress_level, distress_score) # Store distress info\n                 })\n             except Exception as e:\n                 print(f\"Error processing detected face: {e}\")\n                 try: cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                 except: pass\n    return faces_data\n\ndef process_image_file(image_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a single image file.\"\"\"\n    print(f\"\\nProcessing image: {image_path}\")\n    frame = cv2.imread(image_path)\n    if frame is None:\n        print(f\"Error: Could not read image file: {image_path}\")\n        return\n\n    faces_data = process_frame(frame)\n    # --- Use updated drawing function ---\n    draw_results_custom_v2_distress(frame, faces_data)\n\n    print(\"Prediction Results:\")\n    for i, res in enumerate(faces_data):\n        print(f\"  Face {i+1} - Box: {res['box']}, Emotions: {res['emotion_predictions']}, DistressIdx: {res['distress']}\") # Updated print\n\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(image_path))\n        try:\n            cv2.imwrite(output_filename, frame)\n            print(f\"Annotated image saved to {output_filename}\")\n        except Exception as e:\n            print(f\"Error saving image {output_filename}: {e}\")\n    else:\n        plt.figure(figsize=(12, 9))\n        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Detected Faces, Emotions & Distress Index (MTCNN - V2 Style) - {os.path.basename(image_path)}\") # Updated title\n        plt.axis('off')\n        plt.show()\n\ndef draw_futuristic_side_panel(frame, face_coordinates, phase, data=None):\n    \"\"\"\n    Draw a futuristic side panel UI for emotion analysis instead of overlays.\n    \n    Args:\n        frame: The video frame to draw on\n        face_coordinates: Tuple of (x1, y1, x2, y2) for face position\n        phase: Either \"analyzing\" or \"results\"\n        data: For results phase, includes emotion, confidence, etc.\n    \"\"\"\n    x1, y1, x2, y2 = face_coordinates\n    frame_height, frame_width = frame.shape[:2]\n    face_width = x2 - x1\n    face_height = y2 - y1\n    face_center_x = (x1 + x2) // 2\n    face_center_y = (y1 + y2) // 2\n    \n    # Determine if panel should be on left or right side of face\n    # Use right side if face is on left half of frame, left side if face is on right half\n    is_face_on_left = face_center_x < frame_width // 2\n    \n    # Panel dimensions - slightly increased\n    panel_width = int(min(320, frame_width * 0.16))  # Slightly wider panel (was 0.15)\n    panel_height = int(min(130, frame_height * 0.16))  # Slightly taller panel (was 0.15)\n    \n    # Panel position - place to the side of the face, vertically centered with the face\n    if is_face_on_left:\n        # Place panel on right side of face\n        panel_x = min(x2 + 10, frame_width - panel_width)  # 10px margin, ensure it fits in frame\n    else:\n        # Place panel on left side of face\n        panel_x = max(x1 - 10 - panel_width, 0)  # 10px margin, ensure it fits in frame\n    \n    # Vertical centering with the face\n    panel_y = max(min(face_center_y - panel_height // 2, frame_height - panel_height), 0)\n    \n    # Panel colors based on phase\n    if phase == \"analyzing\":\n        # ANALYZING PHASE UI\n        # Mango theme for analyzing (was yellow)\n        panel_color = (255, 191, 52)  # mango\n        accent_color = (0, 100, 200)  # blue accent\n        text_color = (0, 0, 0)  # black\n        \n        # Draw panel background\n        cv2.rectangle(frame, \n                     (panel_x, panel_y), \n                     (panel_x + panel_width, panel_y + panel_height), \n                     panel_color, -1)\n        \n        # Draw panel border\n        cv2.rectangle(frame, \n                     (panel_x, panel_y), \n                     (panel_x + panel_width, panel_y + panel_height), \n                     panel_color, 1)\n        \n        # Draw scan progress (0.0 to 1.0)\n        scan_progress = data if data is not None else 0.5\n        \n        # Add horizontal accent line at top\n        cv2.line(frame, \n                (panel_x, panel_y + 5), \n                (panel_x + panel_width, panel_y + 5), \n                accent_color, 1)\n        \n        # Progress bar\n        progress_width = int(panel_width * 0.9)  # 90% of panel width\n        progress_height = 4\n        progress_x = panel_x + (panel_width - progress_width) // 2\n        progress_y = panel_y + 20\n        \n        # Background bar\n        cv2.rectangle(frame, \n                     (progress_x, progress_y), \n                     (progress_x + progress_width, progress_y + progress_height), \n                     (80, 80, 80), -1)\n        \n        # Progress fill\n        filled_width = int(progress_width * scan_progress)\n        cv2.rectangle(frame, \n                     (progress_x, progress_y), \n                     (progress_x + filled_width, progress_y + progress_height), \n                     accent_color, -1)\n        \n        # Add \"Analyzing\" text - slightly larger font\n        progress_text = f\"Analyzing: {int(scan_progress * 100)}%\"\n        text_size = cv2.getTextSize(progress_text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)[0]  # Was 0.4\n        text_x = panel_x + (panel_width - text_size[0]) // 2\n        cv2.putText(frame, progress_text, \n                   (text_x, panel_y + 15), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, text_color, 1)\n        \n        # Draw scanning effect on the face (minimal)\n        corner_length = int(min(face_width, face_height) * 0.15)\n        \n        # Draw only corners, not full rectangle\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), accent_color, 1)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x2, y1), (x2 - corner_length, y1), accent_color, 1)\n        cv2.line(frame, (x2, y1), (x2, y1 + corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x1, y2), (x1 + corner_length, y2), accent_color, 1)\n        cv2.line(frame, (x1, y2), (x1, y2 - corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x2, y2), (x2 - corner_length, y2), accent_color, 1)\n        cv2.line(frame, (x2, y2), (x2, y2 - corner_length), accent_color, 1)\n        \n        # Horizontal scanning line\n        scan_y = int(y1 + (y2 - y1) * scan_progress)\n        cv2.line(frame, (x1, scan_y), (x2, scan_y), accent_color, 1)\n    \n    elif phase == \"results\":\n        # RESULTS PHASE UI\n        # Unpack data\n        emotion = data.get(\"emotion\", \"UNKNOWN\")\n        confidence = data.get(\"confidence\", 0.0)\n        distress_level = data.get(\"distress\", \"UNKNOWN\")\n        stability = data.get(\"stability\", 0.0)\n        countdown = data.get(\"countdown\", 0.0)\n        \n        # Updated color theme based on distress level - consistent with other UI\n        if distress_level.startswith(\"HIGH\"):\n            panel_color = (243, 58, 106)  # rose (unchanged)\n            accent_color = (0, 0, 255)    # Bright red accent (unchanged)\n        elif distress_level.startswith(\"MOD\"):\n            panel_color = (255, 191, 52)  # Mango (unchanged)\n            accent_color = (0, 50, 200)   # Blue-yellow accent (updated for consistency)\n        else:\n            panel_color = (141, 182, 0)   # Apple green (unchanged)\n            accent_color = (0, 255, 0)    # Bright green accent (unchanged)\n        \n        text_color = (255, 255, 255)  # White text for contrast (except for MOD level)\n        if distress_level.startswith(\"MOD\"):\n            text_color = (0, 0, 0)  # Black text for mango background\n        \n        # Draw panel background\n        cv2.rectangle(frame, \n                     (panel_x, panel_y), \n                     (panel_x + panel_width, panel_y + panel_height), \n                     (20, 20, 40), -1)  # Dark background\n        \n        # Draw panel border with theme color\n        cv2.rectangle(frame, \n                     (panel_x, panel_y), \n                     (panel_x + panel_width, panel_y + panel_height), \n                     panel_color, 1)\n        \n        # Add top accent line\n        cv2.line(frame, \n                (panel_x, panel_y + 5), \n                (panel_x + panel_width, panel_y + 5), \n                accent_color, 1)\n        \n        # Add bottom accent line\n        cv2.line(frame, \n                (panel_x, panel_y + panel_height - 5), \n                (panel_x + panel_width, panel_y + panel_height - 5), \n                accent_color, 1)\n        \n        # Add countdown\n        countdown_text = f\"{countdown:.1f}s\"\n        text_size = cv2.getTextSize(countdown_text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)[0]  # Increased from 0.4\n        countdown_x = panel_x + panel_width - text_size[0] - 5\n        cv2.putText(frame, countdown_text, \n                   (countdown_x, panel_y + 15), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, text_color, 1)\n        \n        # Add emotion information - slightly increased font and spacing\n        line_spacing = 20  # Increased from 18\n        text_x = panel_x + 8\n        text_y_start = panel_y + 32  # Increased from 30\n        \n        # Emotion - slightly larger font\n        emotion_text = f\"EMOTION: {emotion}\"\n        cv2.putText(frame, emotion_text, \n                   (text_x, text_y_start), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, accent_color, 1)  # Increased from 0.4\n        \n        # Confidence bar\n        conf_bar_width = panel_width - 16\n        conf_bar_height = 3\n        conf_bar_x = text_x\n        conf_bar_y = text_y_start + 5\n        \n        # Background bar\n        cv2.rectangle(frame, \n                     (conf_bar_x, conf_bar_y), \n                     (conf_bar_x + conf_bar_width, conf_bar_y + conf_bar_height), \n                     (80, 80, 80), -1)\n        \n        # Filled portion\n        filled_width = int(conf_bar_width * confidence)\n        cv2.rectangle(frame, \n                     (conf_bar_x, conf_bar_y), \n                     (conf_bar_x + filled_width, conf_bar_y + conf_bar_height), \n                     accent_color, -1)\n        \n        # Confidence text\n        conf_text = f\"{confidence*100:.0f}%\"\n        cv2.putText(frame, conf_text, \n                   (conf_bar_x + conf_bar_width + 2, conf_bar_y + 3), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1)\n        \n        # Distress level\n        cv2.putText(frame, f\"DISTRESS: {distress_level}\", \n                   (text_x, text_y_start + line_spacing * 1), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, accent_color, 1)  # Increased from 0.4\n        \n        # Stability\n        cv2.putText(frame, f\"STABILITY: {stability*100:.0f}%\", \n                   (text_x, text_y_start + line_spacing * 2), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, text_color, 1)  # Increased from 0.4\n        \n        # Draw minimal corner brackets on the face with theme color\n        corner_length = int(min(face_width, face_height) * 0.15)\n        \n        # Draw only corner brackets to indicate the face being analyzed\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), accent_color, 1)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x2, y1), (x2 - corner_length, y1), accent_color, 1)\n        cv2.line(frame, (x2, y1), (x2, y1 + corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x1, y2), (x1 + corner_length, y2), accent_color, 1)\n        cv2.line(frame, (x1, y2), (x1, y2 - corner_length), accent_color, 1)\n        \n        cv2.line(frame, (x2, y2), (x2 - corner_length, y2), accent_color, 1)\n        cv2.line(frame, (x2, y2), (x2, y2 - corner_length), accent_color, 1)\n        \n        # Connection line from face to panel\n        if is_face_on_left:\n            # Line from right side of face to left side of panel\n            cv2.line(frame, (x2, face_center_y), (panel_x, face_center_y), accent_color, 1)\n        else:\n            # Line from left side of face to right side of panel\n            cv2.line(frame, (x1, face_center_y), (panel_x + panel_width, face_center_y), accent_color, 1)\n    \n    return frame\n\n\ndef process_video_file(video_path, output_dir):\n    \"\"\"\n    Enhanced video processing with completely redesigned UI rendering.\n    Fixes ghosting effects and improves face detection consistency.\n    \"\"\"\n    print(f\"\\nProcessing video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file: {video_path}\")\n        return\n\n    # Get video properties\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0: fps = 25  # Default FPS if not available\n    \n    # Calculate frames needed for collection and display phases\n    frames_in_collection_phase = int(3 * fps)  # 3 seconds of collection\n    frames_in_display_phase = int(5 * fps)     # 5 seconds of display\n    \n    # Set up video writer if saving output\n    writer = None\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(video_path))\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        writer = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n        print(f\"Saving annotated video to {output_filename}\")\n\n    # Initialize variables for processing\n    current_phase = \"collection\"\n    collection_frames = 0\n    display_frames = 0\n    \n    # Track the primary faces to avoid detecting parts of the same face\n    primary_faces = []  # Store consistent face positions across frames\n    \n    # Buffers for storing emotion data during collection phase\n    emotion_buffers = {}  # Format: {face_id: [emotions_list]}\n    display_results = {}  # Results to display in the display phase\n    \n    # Main processing loop\n    while True:\n        ret, original_frame = cap.read()\n        if not ret: break\n        \n        # IMPORTANT: Always work with a completely fresh copy of the frame\n        frame = original_frame.copy()\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        # Phase management\n        if current_phase == \"collection\":\n            # COLLECTION PHASE - Gather data for 3 seconds\n            collection_frames += 1\n            collection_progress = collection_frames / frames_in_collection_phase\n            \n            # Detect faces in this frame\n            detections = detector.detect_faces(frame_rgb)\n            \n            # Filter detections to avoid parts of the same face\n            valid_faces = filter_overlapping_faces(detections)\n            \n            # Track consistent faces across frames\n            if len(primary_faces) == 0 and len(valid_faces) > 0:\n                # Initialize tracking with the first frame's detections\n                primary_faces = valid_faces\n            else:\n                # Update tracking with current detections\n                primary_faces = update_face_tracking(primary_faces, valid_faces)\n            \n            # Process each tracked face\n            for face_idx, face_info in enumerate(primary_faces):\n                try:\n                    # Generate a stable face ID based on tracking index\n                    face_id = f\"face_{face_idx}\"\n                    \n                    # Get face coordinates\n                    x, y, w, h = face_info['box']\n                    x1 = max(0, x); y1 = max(0, y)\n                    x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                    \n                    # Process face for emotion detection\n                    x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                    x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                    if x1p >= x2p or y1p >= y2p: continue\n\n                    face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                    if face_crop_rgb.size == 0: continue\n\n                    # Get emotions for this face\n                    emotions, raw_emotions = predict_emotions_all_models(face_crop_rgb, models, CLASS_NAMES)\n                    \n                    # Store in buffer\n                    if face_id not in emotion_buffers:\n                        emotion_buffers[face_id] = []\n                    \n                    emotion_buffers[face_id].append({\n                        \"emotions\": emotions,\n                        \"position\": (x1, y1, x2, y2)\n                    })\n                    \n                    # Draw minimal UI for analyzing phase\n                    draw_analyzing_ui(frame, (x1, y1, x2, y2), collection_progress)\n                    \n                except Exception as e:\n                    print(f\"Error processing face during collection: {e}\")\n            \n            # Switch to display mode after 3 seconds of collection\n            if collection_frames >= frames_in_collection_phase:\n                current_phase = \"display\"\n                display_frames = 0\n                \n                # Process the collected emotion data for each face\n                processed_results = {}\n                \n                for face_id, data_list in emotion_buffers.items():\n                    if len(data_list) == 0:\n                        continue\n                    \n                    # Get the most recent face position (for more stable UI placement)\n                    latest_position = data_list[-1][\"position\"]\n                    \n                    # Extract just the emotion dictionaries\n                    emotions_list = [data[\"emotions\"] for data in data_list]\n                    \n                    # Count occurrences of each emotion across frames\n                    emotion_counts = {}\n                    for emotions_dict in emotions_list:\n                        if 'Emotion' in emotions_dict:\n                            emotion, confidence = emotions_dict['Emotion']\n                            if emotion not in emotion_counts:\n                                emotion_counts[emotion] = {'count': 0, 'confidence_sum': 0}\n                            emotion_counts[emotion]['count'] += 1\n                            emotion_counts[emotion]['confidence_sum'] += confidence\n                    \n                    # Find the most frequent emotion\n                    most_frequent_emotion = None\n                    most_frequent_count = 0\n                    avg_confidence = 0\n                    \n                    for emotion, data in emotion_counts.items():\n                        if data['count'] > most_frequent_count:\n                            most_frequent_emotion = emotion\n                            most_frequent_count = data['count']\n                            avg_confidence = data['confidence_sum'] / data['count']\n                    \n                    # Calculate a stability score (how consistent the emotion was)\n                    stability = most_frequent_count / len(emotions_list) if len(emotions_list) > 0 else 0\n                    \n                    # Calculate overall distress level from all frames\n                    distress_levels = {\"LOW\": 0, \"MODERATE\": 0, \"HIGH\": 0}\n                    distress_score_sum = 0\n                    distress_count = 0\n                    \n                    for emotions_dict in emotions_list:\n                        if 'Emotion' in emotions_dict:\n                            # Use the latest distress calculation function\n                            distress_level, distress_score = calculate_heuristic_distress(emotions_dict)\n                            if distress_level != \"UNKNOWN\":\n                                distress_levels[distress_level] += 1\n                                distress_score_sum += distress_score\n                                distress_count += 1\n                    \n                    # Find most frequent distress level\n                    most_frequent_distress = \"UNKNOWN\"\n                    most_distress_count = 0\n                    for level, count in distress_levels.items():\n                        if count > most_distress_count:\n                            most_frequent_distress = level\n                            most_distress_count = count\n                    \n                    avg_distress_score = distress_score_sum / distress_count if distress_count > 0 else 0\n                    \n                    # Store processed results\n                    processed_results[face_id] = {\n                        \"emotion\": most_frequent_emotion,\n                        \"confidence\": avg_confidence,\n                        \"stability\": stability,\n                        \"distress\": most_frequent_distress,\n                        \"distress_score\": avg_distress_score,\n                        \"position\": latest_position\n                    }\n                \n                # Clear buffers for next collection cycle\n                emotion_buffers = {}\n                primary_faces = []\n                \n                # Store processed results for display phase\n                display_results = processed_results\n                \n        else:  # DISPLAY PHASE - Show results for 5 seconds\n            display_frames += 1\n            \n            # Calculate remaining time in display phase\n            remaining_time = (frames_in_display_phase - display_frames) / fps\n            \n            # Display the processed results\n            for face_id, result in display_results.items():\n                position = result[\"position\"]\n                \n                # Add countdown to the result data\n                result[\"countdown\"] = remaining_time\n                \n                # Draw the results UI\n                draw_results_ui(frame, position, result)\n            \n            # Switch back to collection after display period\n            if display_frames >= frames_in_display_phase:\n                current_phase = \"collection\"\n                collection_frames = 0\n                display_results = {}  # Clear results to prevent any leftover displays\n                primary_faces = []    # Reset face tracking\n        \n        # Write or display the frame (always using the newly rendered frame)\n        if writer:\n            writer.write(frame)\n        else:\n            cv2.imshow(\"Emotion Analysis\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n    \n    # Cleanup\n    cap.release()\n    if writer:\n        writer.release()\n    cv2.destroyAllWindows()\n    print(f\"Finished processing video: {video_path}\")\n\n\ndef filter_overlapping_faces(detections, overlap_threshold=0.5):\n    \"\"\"\n    Filter out overlapping face detections to avoid duplicate readings.\n    Returns a list of valid face detections.\n    \"\"\"\n    if not detections:\n        return []\n    \n    # Sort by confidence (higher first)\n    sorted_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n    \n    valid_detections = []\n    for detection in sorted_detections:\n        # Skip if confidence is too low\n        if detection['confidence'] < MTCNN_MIN_CONFIDENCE:\n            continue\n        \n        # Get current box\n        x, y, w, h = detection['box']\n        current_box = [x, y, x+w, y+h]\n        \n        # Check if this overlaps too much with any existing valid detection\n        should_skip = False\n        for valid in valid_detections:\n            vx, vy, vw, vh = valid['box']\n            valid_box = [vx, vy, vx+vw, vy+vh]\n            \n            # Calculate IoU (Intersection over Union)\n            iou = calculate_iou(current_box, valid_box)\n            \n            if iou > overlap_threshold:\n                should_skip = True\n                break\n        \n        if not should_skip:\n            valid_detections.append(detection)\n    \n    return valid_detections\n\n\ndef calculate_iou(box1, box2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) between two bounding boxes.\n    Each box is [x1, y1, x2, y2] format.\n    \"\"\"\n    # Determine the coordinates of the intersection rectangle\n    x_left = max(box1[0], box2[0])\n    y_top = max(box1[1], box2[1])\n    x_right = min(box1[2], box2[2])\n    y_bottom = min(box1[3], box2[3])\n    \n    # If the boxes don't intersect, return 0\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n    \n    # Calculate area of intersection\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calculate area of both boxes\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    # Calculate IoU\n    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n    \n    return iou\n\n\ndef update_face_tracking(tracked_faces, current_faces, iou_threshold=0.3):\n    \"\"\"\n    Update tracked faces with new detections to maintain consistency.\n    Returns updated list of tracked faces.\n    \"\"\"\n    if not tracked_faces:\n        return current_faces\n    \n    if not current_faces:\n        return tracked_faces\n    \n    updated_faces = []\n    unmatched_current = list(current_faces)\n    \n    for tracked in tracked_faces:\n        tx, ty, tw, th = tracked['box']\n        tracked_box = [tx, ty, tx+tw, ty+th]\n        \n        best_match = None\n        best_iou = 0\n        best_idx = -1\n        \n        # Find best matching current face\n        for i, current in enumerate(unmatched_current):\n            cx, cy, cw, ch = current['box']\n            current_box = [cx, cy, cx+cw, cy+ch]\n            \n            iou = calculate_iou(tracked_box, current_box)\n            if iou > best_iou:\n                best_iou = iou\n                best_match = current\n                best_idx = i\n        \n        # If found a good match, update tracked face with current detection\n        if best_match and best_iou > iou_threshold:\n            updated_faces.append(best_match)\n            if best_idx >= 0:\n                del unmatched_current[best_idx]\n        else:\n            # Keep the tracked face if no good match\n            updated_faces.append(tracked)\n    \n    # Add any unmatched current faces\n    updated_faces.extend(unmatched_current)\n    \n    return updated_faces\n\n\ndef draw_analyzing_ui(frame, face_coordinates, progress):\n    \"\"\"\n    Draw minimalist UI for the analyzing phase.\n    Uses clean corner brackets and a scanning line.\n    \"\"\"\n    x1, y1, x2, y2 = face_coordinates\n    face_width = x2 - x1\n    face_height = y2 - y1\n    \n    # Use mango accent color for analyzing phase (was yellow)\n    accent_color = (255, 191, 52)  # Mango\n    \n    # Draw minimal corner brackets (only at corners, not full rectangle)\n    corner_length = int(min(face_width, face_height) * 0.08)\n    line_thickness = 2  # Thin lines\n    \n    # Top-left\n    cv2.line(frame, (x1, y1), (x1 + corner_length, y1), accent_color, line_thickness)\n    cv2.line(frame, (x1, y1), (x1, y1 + corner_length), accent_color, line_thickness)\n    \n    # Top-right\n    cv2.line(frame, (x2, y1), (x2 - corner_length, y1), accent_color, line_thickness)\n    cv2.line(frame, (x2, y1), (x2, y1 + corner_length), accent_color, line_thickness)\n    \n    # Bottom-left\n    cv2.line(frame, (x1, y2), (x1 + corner_length, y2), accent_color, line_thickness)\n    cv2.line(frame, (x1, y2), (x1, y2 - corner_length), accent_color, line_thickness)\n    \n    # Bottom-right\n    cv2.line(frame, (x2, y2), (x2 - corner_length, y2), accent_color, line_thickness)\n    cv2.line(frame, (x2, y2), (x2, y2 - corner_length), accent_color, line_thickness)\n    \n    # Scanning line animation\n    scan_y = int(y1 + (y2 - y1) * progress)\n    cv2.line(frame, (x1, scan_y), (x2, scan_y), accent_color, line_thickness)\n    \n    # Small analyzing indicator with percentage - keep it minimal\n    progress_percent = int(progress * 100)\n    if progress_percent % 25 == 0:  # Only show at 0%, 25%, 50%, 75%, 100%\n        # Draw a very small indicator at one corner - slightly increased font\n        cv2.putText(frame, f\"{progress_percent}%\", \n                  (x1, y1 - 5), \n                  cv2.FONT_HERSHEY_SIMPLEX, 0.35, accent_color, 1)  # Increased from 0.3\n\n\ndef draw_results_ui(frame, face_coordinates, result_data):\n    \"\"\"\n    Draw ultra-compact side panel UI for displaying emotion results.\n    Uses abbreviations and optimal spacing for better readability.\n    \"\"\"\n    x1, y1, x2, y2 = face_coordinates\n    frame_height, frame_width = frame.shape[:2]\n    face_width = x2 - x1\n    face_height = y2 - y1\n    face_center_x = (x1 + x2) // 2\n    face_center_y = (y1 + y2) // 2\n    \n    # Unpack result data\n    emotion = result_data.get(\"emotion\", \"UNKNOWN\")\n    confidence = result_data.get(\"confidence\", 0.0)\n    distress_level = result_data.get(\"distress\", \"UNKNOWN\")\n    stability = result_data.get(\"stability\", 0.0)\n    countdown = result_data.get(\"countdown\", 0.0)\n    \n    # Abbreviate long emotion names\n    if len(emotion) > 5:\n        if emotion.lower() == \"contempt\":\n            emotion = \"cntmpt\"\n        elif emotion.lower() == \"disgust\":\n            emotion = \"dsgst\"\n        elif emotion.lower() == \"surprise\":\n            emotion = \"surpr\"\n        elif emotion.lower() == \"neutral\":\n            emotion = \"neutr\"\n        # Keep short names as is: sad, fear, anger, happy\n    \n    # Abbreviate distress level\n    if distress_level.startswith(\"MODERATE\"):\n        distress_level = \"MOD\"\n    elif distress_level.startswith(\"HIGH\"):\n        distress_level = \"HIGH\"\n    elif distress_level.startswith(\"LOW\"):\n        distress_level = \"LOW\"\n    \n    # Determine if panel should be on left or right side of face\n    is_face_on_left = face_center_x < frame_width // 2\n    \n    # Increased panel dimensions\n    panel_width = min(95, int(face_width * 0.75))  # Slightly wider panel (was 85)\n    panel_height = 55  # Taller panel for better spacing (was 46)\n    \n    # Panel position\n    panel_margin = 10\n    \n    if is_face_on_left:\n        # Place panel on right side of face\n        panel_x = min(x2 + panel_margin, frame_width - panel_width - 5)\n    else:\n        # Place panel on left side of face\n        panel_x = max(x1 - panel_width - panel_margin, 5)\n    \n    # Vertical centering with face\n    panel_y = max(min(face_center_y - panel_height // 2, frame_height - panel_height - 5), 5)\n    \n    # Updated color schemes with new palette\n    if distress_level == \"HIGH\":\n        # Rose theme for high distress\n        bg_color = (243, 58, 106)       # Rose red background (was dark red)\n        border_color = (0, 0, 255)      # Bright red border\n        text_color = (255, 255, 255)    # White text for contrast\n    elif distress_level == \"MOD\":\n        # Mango theme for moderate distress\n        bg_color = (255, 191, 52)       # Mango background (was dark yellow)\n        border_color = (0, 50, 200)     # Blue-yellow accent\n        text_color = (0, 0, 0)          # Black text for contrast\n    else:\n        # Apple green theme for low distress\n        bg_color = (141, 182, 0)        # Apple green background (was dark green)\n        border_color = (0, 255, 0)      # Bright green border\n        text_color = (255, 255, 255)    # White text for contrast\n    \n    # Draw minimal face corner brackets with theme color\n    corner_length = int(min(face_width, face_height) * 0.08)  # Smaller corners\n    line_thickness = 2  # Thin lines\n    \n    # Draw corner brackets\n    cv2.line(frame, (x1, y1), (x1 + corner_length, y1), border_color, line_thickness)\n    cv2.line(frame, (x1, y1), (x1, y1 + corner_length), border_color, line_thickness)\n    \n    cv2.line(frame, (x2, y1), (x2 - corner_length, y1), border_color, line_thickness)\n    cv2.line(frame, (x2, y1), (x2, y1 + corner_length), border_color, line_thickness)\n    \n    cv2.line(frame, (x1, y2), (x1 + corner_length, y2), border_color, line_thickness)\n    cv2.line(frame, (x1, y2), (x1, y2 - corner_length), border_color, line_thickness)\n    \n    cv2.line(frame, (x2, y2), (x2 - corner_length, y2), border_color, line_thickness)\n    cv2.line(frame, (x2, y2), (x2, y2 - corner_length), border_color, line_thickness)\n    \n    # Draw side panel with colored background based on distress level\n    # 1. Panel background\n    cv2.rectangle(frame, \n                 (panel_x, panel_y), \n                 (panel_x + panel_width, panel_y + panel_height), \n                 bg_color, -1)\n    \n    # 2. Panel border\n    cv2.rectangle(frame, \n                 (panel_x, panel_y), \n                 (panel_x + panel_width, panel_y + panel_height), \n                 border_color, 1)\n    \n    # 3. Add connection line from face to panel (single clean line)\n    if is_face_on_left:\n        # Line from right side of face to left side of panel\n        cv2.line(frame, (x2, face_center_y), (panel_x, panel_y + panel_height // 2), border_color, 1)\n    else:\n        # Line from left side of face to right side of panel\n        cv2.line(frame, (x1, face_center_y), (panel_x + panel_width, panel_y + panel_height // 2), border_color, 1)\n    \n    # 4. Add content with slightly increased spacing\n    text_x = panel_x + 4\n    line_spacing = 17  # Increased from 15\n    text_y_start = panel_y + 14  # Increased from 12\n    small_font_scale = 0.42  # Increased from 0.38\n    \n    # Only show abbreviated labels with ultra-compact layout\n    # Emotion (abbreviated)\n    emotion_text = f\"EMO: {emotion}\"\n    cv2.putText(frame, emotion_text, \n               (text_x, text_y_start), \n               cv2.FONT_HERSHEY_SIMPLEX, small_font_scale, text_color, 1)\n    \n    # Distress level (abbreviated)\n    distress_text = f\"DST: {distress_level}\"\n    cv2.putText(frame, distress_text, \n               (text_x, text_y_start + line_spacing), \n               cv2.FONT_HERSHEY_SIMPLEX, small_font_scale, text_color, 1)\n    \n    # Stability (abbreviated)\n    stab_text = f\"STB: {int(stability*100)}%\"\n    cv2.putText(frame, stab_text, \n               (text_x, text_y_start + 2*line_spacing), \n               cv2.FONT_HERSHEY_SIMPLEX, small_font_scale, text_color, 1)\n    \n    # Optional: Add small timer indicator on the top-right\n    if countdown > 0:\n        cv2.putText(frame, f\"{int(countdown)}s\", \n                   (panel_x + panel_width - 18, panel_y + 10), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.35, text_color, 1)\n\n\ndef draw_analyzing_ui(frame, face_coordinates, progress):\n    \"\"\"\n    Draw minimalist UI for the analyzing phase.\n    Uses clean corner brackets and a scanning line.\n    \"\"\"\n    x1, y1, x2, y2 = face_coordinates\n    face_width = x2 - x1\n    face_height = y2 - y1\n    \n    # Use yellow accent color for analyzing phase\n    accent_color = (0, 255, 255)  # Yellow\n    \n    # Draw minimal corner brackets (only at corners, not full rectangle)\n    corner_length = int(min(face_width, face_height) * 0.08)\n    line_thickness = 2  # Thin lines\n    \n    # Top-left\n    cv2.line(frame, (x1, y1), (x1 + corner_length, y1), accent_color, line_thickness)\n    cv2.line(frame, (x1, y1), (x1, y1 + corner_length), accent_color, line_thickness)\n    \n    # Top-right\n    cv2.line(frame, (x2, y1), (x2 - corner_length, y1), accent_color, line_thickness)\n    cv2.line(frame, (x2, y1), (x2, y1 + corner_length), accent_color, line_thickness)\n    \n    # Bottom-left\n    cv2.line(frame, (x1, y2), (x1 + corner_length, y2), accent_color, line_thickness)\n    cv2.line(frame, (x1, y2), (x1, y2 - corner_length), accent_color, line_thickness)\n    \n    # Bottom-right\n    cv2.line(frame, (x2, y2), (x2 - corner_length, y2), accent_color, line_thickness)\n    cv2.line(frame, (x2, y2), (x2, y2 - corner_length), accent_color, line_thickness)\n    \n    # Scanning line animation\n    scan_y = int(y1 + (y2 - y1) * progress)\n    cv2.line(frame, (x1, scan_y), (x2, scan_y), accent_color, line_thickness)\n    \n    # Small analyzing indicator with percentage - keep it minimal\n    progress_percent = int(progress * 100)\n    if progress_percent % 25 == 0:  # Only show at 0%, 25%, 50%, 75%, 100%\n        # Draw a very small indicator at one corner\n        cv2.putText(frame, f\"{progress_percent}%\", \n                  (x1, y1 - 5), \n                  cv2.FONT_HERSHEY_SIMPLEX, 0.3, accent_color, 1)\n\n# =============================================================================\n# Main Execution - Iterate through Input Directory\n# =============================================================================\nif __name__ == \"__main__\":\n    if not os.path.isdir(INPUT_DIR):\n        print(f\"Error: Input directory not found: {INPUT_DIR}\")\n        exit()\n\n    print(f\"Scanning directory: {INPUT_DIR}\")\n    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff')\n    video_extensions = ('*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv')\n\n    # Create output dir if saving\n    if SAVE_OUTPUT and not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if image_files:\n        print(f\"\\nFound {len(image_files)} image files to process...\")\n        for img_path in image_files:\n            process_image_file(img_path, OUTPUT_DIR)\n    else:\n        print(\"No image files found in the directory.\")\n\n    video_files = []\n    for ext in video_extensions:\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if video_files:\n        print(f\"\\nFound {len(video_files)} video files to process...\")\n        for vid_path in video_files:\n            process_video_file(vid_path, OUTPUT_DIR)\n    else:\n        print(\"No video files found in the directory.\")\n\n    print(\"\\nAll processing complete.\")","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# emotion_predictor_multi_model_dir_v2_distress.py\n\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import MTCNN\nimport matplotlib.pyplot as plt\nimport time\nimport glob\n\n# =============================================================================\n# Configuration\n# =============================================================================\n# --- Input ---\nINPUT_DIR = \"/kaggle/input/image-samples/image_samples\"\n# --- Models ---\nMODEL_PATH_B0 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\"\nMODEL_PATH_B4 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\"\nENSEMBLE_MODEL_PATH = \"/kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\"\n# --- Processing ---\nIMG_SIZE = 112\nCLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n# --- MTCNN ---\nMTCNN_MIN_CONFIDENCE = 0.90\n# --- Display ---\nMODEL_COLORS = {\n    'B0': (255, 100, 100), # Light Blue\n    'B4': (100, 255, 100), # Light Green\n    'Ensemble': (100, 100, 255), # Light Red\n    'Default': (200, 200, 200),\n    'Distress': (255, 165, 0) # Orange for Distress Index\n}\nTEXT_COLOR = (0, 0, 0)\nFONT = cv2.FONT_HERSHEY_SIMPLEX\nFONT_SCALE = 0.4\nFONT_THICKNESS = 1\nBOX_PADDING = 20\nCORNER_LENGTH_FACTOR = 0.15\nCORNER_THICKNESS = 2\nLINE_SPACING = 15\n\n# --- Output ---\nSAVE_OUTPUT = True\nOUTPUT_DIR = \"/kaggle/working/emotion_output_v2_distress\" # Changed output dir name\n\n# =============================================================================\n# Load ALL Emotion Models (Same as before)\n# =============================================================================\nprint(\"Loading emotion models...\")\nmodels = {}\ntry:\n    # ...(Loading code remains the same)...\n    print(f\"Loading Model B0: {MODEL_PATH_B0}\")\n    if not os.path.exists(MODEL_PATH_B0): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B0}\")\n    models['B0'] = tf.keras.models.load_model(MODEL_PATH_B0, compile=False)\n\n    print(f\"Loading Model B4: {MODEL_PATH_B4}\")\n    if not os.path.exists(MODEL_PATH_B4): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B4}\")\n    models['B4'] = tf.keras.models.load_model(MODEL_PATH_B4, compile=False)\n\n    print(f\"Loading Ensemble Model: {ENSEMBLE_MODEL_PATH}\")\n    if not os.path.exists(ENSEMBLE_MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {ENSEMBLE_MODEL_PATH}\")\n    models['Ensemble'] = tf.keras.models.load_model(ENSEMBLE_MODEL_PATH, compile=False)\n\n    print(\"All models loaded successfully:\")\n    for name, model in models.items():\n        print(f\"- {name}: {model.name}\")\n\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    exit()\n\n# =============================================================================\n# Initialize Face Detector (MTCNN) (Same as before)\n# =============================================================================\nprint(\"Initializing MTCNN face detector...\")\ntry:\n    detector = MTCNN()\n    print(\"MTCNN detector initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing MTCNN detector: {e}\")\n    exit()\n\n# =============================================================================\n# Helper Functions\n# =============================================================================\n\ndef preprocess_face_for_emotion(face_image, target_size):\n    # ...(Preprocessing code remains the same)...\n    try:\n        face_resized = tf.image.resize(face_image, [target_size, target_size], method='nearest')\n        face_processed = tf.cast(face_resized, tf.float32)\n        face_batch = tf.expand_dims(face_processed, axis=0)\n        return face_batch\n    except Exception as e:\n        print(f\"Error during face preprocessing: {e}\")\n        return None\n\ndef predict_emotions_all_models(face_batch, loaded_models, class_names):\n    # ...(Prediction code remains the same - returns top predictions)...\n    predictions = {}\n    if face_batch is None:\n        return {\"Error\": (\"Preprocessing failed\", 0.0)}\n    try:\n        for model_name, model in loaded_models.items():\n            preds_probs = model.predict(face_batch, verbose=0)\n            predicted_class_index = np.argmax(preds_probs[0])\n            confidence = np.max(preds_probs[0])\n            predicted_emotion = class_names[predicted_class_index]\n            predictions[model_name] = (predicted_emotion, confidence)\n        return predictions\n    except Exception as e:\n        print(f\"Error during emotion prediction: {e}\")\n        return {\"Error\": (f\"Prediction failed: {e}\", 0.0)}\n\n# --- NEW: Heuristic Distress Level Calculation Function ---\ndef calculate_heuristic_distress(emotion_predictions):\n    \"\"\"\n    Calculates a heuristic 'distress' score based on weighted emotion predictions.\n    WARNING: This is an experimental heuristic and NOT a validated measure of stress.\n    \"\"\"\n    # Define heuristic stress mappings (emotion -> weight)\n    distress_weights = {\n        'anger': 0.9,    # High associated weight\n        'fear': 0.85,    # High associated weight\n        'disgust': 0.75, # High associated weight\n        'contempt': 0.6, # Moderate associated weight\n        'sad': 0.55,     # Moderate associated weight\n        'surprise': 0.5, # Neutral associated weight (can be positive or negative stress)\n        'neutral': 0.2,  # Low associated weight\n        'happy': 0.1     # Very low associated weight\n    }\n\n    weighted_score_sum = 0.0\n    confidence_sum = 0.0\n\n    # Calculate score based on top prediction from each model\n    for model_name, prediction in emotion_predictions.items():\n         if model_name == \"Error\": continue\n         emotion, confidence = prediction\n         if emotion in distress_weights:\n             weighted_score_sum += distress_weights[emotion] * confidence\n             confidence_sum += confidence # Use confidence as the denominator weight\n\n    if confidence_sum > 0:\n        # Normalize score by sum of confidences\n        # This bounds the score between min_weight and max_weight approximately\n        normalized_score = weighted_score_sum / confidence_sum\n\n        # Map to categories based on arbitrary thresholds\n        if normalized_score > 0.65: # Threshold for HIGH\n            level = \"HIGH\"\n        elif normalized_score > 0.35: # Threshold for MODERATE\n            level = \"MODERATE\"\n        else:\n            level = \"LOW\"\n        # Return the category and the normalized score itself\n        return level, normalized_score\n    else:\n        return \"UNKNOWN\", 0.0\n\n# --- UPDATED Drawing Function ---\ndef draw_results_custom_v2_distress(frame, faces_data):\n    \"\"\"Draws corner boxes, stacked emotion labels, and heuristic distress level.\"\"\"\n    for data in faces_data:\n        box = data[\"box\"]\n        emotion_predictions = data[\"emotion_predictions\"] # Dict of top emotion preds\n        distress_info = data[\"distress\"] # Tuple (level, score)\n        x1, y1, x2, y2 = box\n\n        # --- Draw Bounding Box Corners ---\n        corner_length = int(min(x2 - x1, y2 - y1) * CORNER_LENGTH_FACTOR)\n        corner_color = MODEL_COLORS.get('Ensemble', MODEL_COLORS['Default'])\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2 - corner_length, y2), (x2, y2), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2, y2 - corner_length), (x2, y2), corner_color, CORNER_THICKNESS)\n\n        # --- Prepare and Draw Emotion Text Labels ---\n        y_offset = y1 - 7 # Starting position\n        max_w = 0 # Calculate max width needed for background\n        labels_to_draw = []\n        for model_name in ['B0', 'B4', 'Ensemble']: # Define order\n            if model_name in emotion_predictions:\n                emotion, confidence = emotion_predictions[model_name]\n                if model_name == \"Error\": continue\n                label = f\"{model_name}: {emotion} ({confidence:.2f})\"\n                labels_to_draw.append((label, model_name))\n                (w, h), _ = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n                if w > max_w: max_w = w\n\n        # Add Distress info label\n        distress_level, distress_score = distress_info\n        distress_label = f\"Distress Idx: {distress_level} ({distress_score:.2f})\" # Careful Labeling!\n        labels_to_draw.append((distress_label, 'Distress')) # Use specific key for color\n        (w, h), _ = cv2.getTextSize(distress_label, FONT, FONT_SCALE, FONT_THICKNESS)\n        if w > max_w: max_w = w\n\n        # Draw background and text for each label, moving upwards\n        num_labels = len(labels_to_draw)\n        bg_bottom = y1\n        for i, (label, label_type) in enumerate(reversed(labels_to_draw)):\n            text_y = y_offset - (i * LINE_SPACING)\n            bg_y1 = text_y - (LINE_SPACING - 3)\n            bg_y2 = text_y + 3\n\n            bg_y1 = max(0, bg_y1)\n            if bg_y1 >= bg_bottom or bg_y1 >= y1: continue\n            bg_bottom = bg_y1\n\n            # Use model color or specific distress color\n            bg_color = MODEL_COLORS.get(label_type, MODEL_COLORS['Default'])\n            cv2.rectangle(frame, (x1, bg_y1), (x1 + max_w + 5, bg_y2), bg_color, -1)\n            cv2.putText(frame, label, (x1 + 2, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS)\n\n\n# =============================================================================\n# Processing Functions for Image/Video Files (Integrate distress calc)\n# =============================================================================\n\ndef process_frame(frame, frame_rgb=None):\n    \"\"\"Detects faces, predicts emotions, calculates heuristic distress.\"\"\"\n    if frame_rgb is None:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    faces_data = []\n    try:\n        detections = detector.detect_faces(frame_rgb)\n    except Exception as e:\n        print(f\"Error during face detection: {e}\")\n        cv2.putText(frame, \"Face Detection Error\", (20, 40), FONT, 1, (0, 0, 255), 2)\n        return faces_data\n\n    if detections:\n         for face_info in detections:\n             try:\n                 confidence_face = face_info['confidence']\n                 if confidence_face < MTCNN_MIN_CONFIDENCE: continue\n\n                 x, y, w, h = face_info['box']\n                 x1 = max(0, x); y1 = max(0, y)\n                 x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                 if x1 >= x2 or y1 >= y2: continue\n\n                 x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                 x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                 if x1p >= x2p or y1p >= y2p: continue\n\n                 face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                 if face_crop_rgb.size == 0: continue\n\n                 face_batch = preprocess_face_for_emotion(face_crop_rgb, IMG_SIZE)\n                 # Get TOP prediction per model\n                 all_emotion_predictions = predict_emotions_all_models(face_batch, models, CLASS_NAMES)\n\n                 # --- Calculate Heuristic Distress ---\n                 distress_level, distress_score = calculate_heuristic_distress(all_emotion_predictions)\n\n                 faces_data.append({\n                     \"box\": (x1, y1, x2, y2),\n                     \"emotion_predictions\": all_emotion_predictions, # Store dict of top predictions\n                     \"distress\": (distress_level, distress_score) # Store distress info\n                 })\n             except Exception as e:\n                 print(f\"Error processing detected face: {e}\")\n                 try: cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                 except: pass\n    return faces_data\n\ndef process_image_file(image_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a single image file.\"\"\"\n    print(f\"\\nProcessing image: {image_path}\")\n    frame = cv2.imread(image_path)\n    if frame is None:\n        print(f\"Error: Could not read image file: {image_path}\")\n        return\n\n    faces_data = process_frame(frame)\n    # --- Use updated drawing function ---\n    draw_results_custom_v2_distress(frame, faces_data)\n\n    print(\"Prediction Results:\")\n    for i, res in enumerate(faces_data):\n        print(f\"  Face {i+1} - Box: {res['box']}, Emotions: {res['emotion_predictions']}, DistressIdx: {res['distress']}\") # Updated print\n\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(image_path))\n        try:\n            cv2.imwrite(output_filename, frame)\n            print(f\"Annotated image saved to {output_filename}\")\n        except Exception as e:\n            print(f\"Error saving image {output_filename}: {e}\")\n    else:\n        plt.figure(figsize=(12, 9))\n        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Detected Faces, Emotions & Distress Index (MTCNN - V2 Style) - {os.path.basename(image_path)}\") # Updated title\n        plt.axis('off')\n        plt.show()\n\ndef process_video_file(video_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a video file.\"\"\"\n    print(f\"\\nProcessing video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file: {video_path}\")\n        return\n\n    writer = None\n    if SAVE_OUTPUT:\n        # ...(VideoWriter setup remains the same)...\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(video_path))\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        if fps <= 0: fps = 25 # Default FPS\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        writer = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n        print(f\"Saving annotated video to {output_filename}\")\n\n\n    frame_count = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n\n        faces_data = process_frame(frame)\n        # --- Use updated drawing function ---\n        draw_results_custom_v2_distress(frame, faces_data)\n\n        # ...(FPS display remains the same)...\n        frame_count += 1\n        if frame_count > 1:\n             elapsed_time = time.time() - start_time\n             fps_display = frame_count / elapsed_time if elapsed_time > 0 else 0\n             cv2.putText(frame, f\"FPS: {fps_display:.2f}\", (10, 30), FONT, 0.7, (255, 255, 255), 2)\n\n\n        if writer:\n             writer.write(frame)\n        else:\n            cv2.imshow(f\"Emotion & Distress Index (MTCNN - V2 Style - Press 'q' to quit)\", frame) # Updated title\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    cap.release()\n    if writer:\n        writer.release()\n    cv2.destroyAllWindows()\n    print(f\"Finished processing video: {video_path}\")\n\n# =============================================================================\n# Main Execution - Iterate through Input Directory (Same as before)\n# =============================================================================\nif __name__ == \"__main__\":\n    # ...(Directory scanning logic remains the same)...\n    if not os.path.isdir(INPUT_DIR):\n        print(f\"Error: Input directory not found: {INPUT_DIR}\")\n        exit()\n\n    print(f\"Scanning directory: {INPUT_DIR}\")\n    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff')\n    video_extensions = ('*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv')\n\n    # Create output dir if saving\n    if SAVE_OUTPUT and not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if image_files:\n        print(f\"\\nFound {len(image_files)} image files to process...\")\n        for img_path in image_files:\n            process_image_file(img_path, OUTPUT_DIR)\n    else:\n        print(\"No image files found in the directory.\")\n\n    video_files = []\n    for ext in video_extensions:\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n\n    if video_files:\n        print(f\"\\nFound {len(video_files)} video files to process...\")\n        for vid_path in video_files:\n            process_video_file(vid_path, OUTPUT_DIR)\n    else:\n        print(\"No video files found in the directory.\")\n\n    print(\"\\nAll processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:33:44.700927Z","iopub.execute_input":"2025-05-01T09:33:44.701145Z","iopub.status.idle":"2025-05-01T09:33:58.803212Z","shell.execute_reply.started":"2025-05-01T09:33:44.701124Z","shell.execute_reply":"2025-05-01T09:33:58.801994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# emotion_predictor_multi_model_dir_v2.py\n\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import MTCNN\nimport matplotlib.pyplot as plt\nimport time\nimport glob\n\n# =============================================================================\n# Configuration\n# =============================================================================\n# --- Input ---\nINPUT_DIR = \"/kaggle/input/image-samples/image_samples\"\n# --- Models ---\nMODEL_PATH_B0 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB0.keras\"\nMODEL_PATH_B4 = \"/kaggle/input/model-3-3/model3.3/best_EfficientNetB4.keras\"\nENSEMBLE_MODEL_PATH = \"/kaggle/input/model-3-3/model3.3/emotion_ensemble_final.keras\"\n# --- Processing ---\nIMG_SIZE = 112\nCLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n# --- MTCNN ---\nMTCNN_MIN_CONFIDENCE = 0.90\n# --- Display ---\n# NEW: Define colors for each model type\nMODEL_COLORS = {\n    'B0': (255, 100, 100), # Light Blue\n    'B4': (100, 255, 100), # Light Green\n    'Ensemble': (100, 100, 255), # Light Red\n    'Default': (200, 200, 200) # Grey for others/errors\n}\nTEXT_COLOR = (0, 0, 0) # Black text for contrast on light backgrounds\nFONT = cv2.FONT_HERSHEY_SIMPLEX\n# NEW: Reduced font size\nFONT_SCALE = 0.4\nFONT_THICKNESS = 1\nBOX_PADDING = 20\nCORNER_LENGTH_FACTOR = 0.15 # Proportion of box size for corner lines\nCORNER_THICKNESS = 2 # Thickness for corner lines\n\n# --- Output ---\nSAVE_OUTPUT = True\nOUTPUT_DIR = \"/kaggle/working/emotion_output_v2\" # Changed output dir name\n\n# =============================================================================\n# Load ALL Emotion Models (Same as before)\n# =============================================================================\nprint(\"Loading emotion models...\")\nmodels = {}\ntry:\n    print(f\"Loading Model B0: {MODEL_PATH_B0}\")\n    if not os.path.exists(MODEL_PATH_B0): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B0}\")\n    models['B0'] = tf.keras.models.load_model(MODEL_PATH_B0, compile=False)\n\n    print(f\"Loading Model B4: {MODEL_PATH_B4}\")\n    if not os.path.exists(MODEL_PATH_B4): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH_B4}\")\n    models['B4'] = tf.keras.models.load_model(MODEL_PATH_B4, compile=False)\n\n    print(f\"Loading Ensemble Model: {ENSEMBLE_MODEL_PATH}\")\n    if not os.path.exists(ENSEMBLE_MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {ENSEMBLE_MODEL_PATH}\")\n    models['Ensemble'] = tf.keras.models.load_model(ENSEMBLE_MODEL_PATH, compile=False)\n\n    print(\"All models loaded successfully:\")\n    for name, model in models.items():\n        print(f\"- {name}: {model.name}\")\n\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    exit()\n\n# =============================================================================\n# Initialize Face Detector (MTCNN) (Same as before)\n# =============================================================================\nprint(\"Initializing MTCNN face detector...\")\ntry:\n    detector = MTCNN()\n    print(\"MTCNN detector initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing MTCNN detector: {e}\")\n    exit()\n\n# =============================================================================\n# Helper Functions (preprocess_face, predict_emotions_all_models - Same as before)\n# =============================================================================\n\ndef preprocess_face_for_emotion(face_image, target_size):\n    \"\"\"Preprocesses a face image (NumPy array) for the emotion model.\"\"\"\n    try:\n        face_resized = tf.image.resize(face_image, [target_size, target_size], method='nearest')\n        face_processed = tf.cast(face_resized, tf.float32)\n        face_batch = tf.expand_dims(face_processed, axis=0)\n        return face_batch\n    except Exception as e:\n        print(f\"Error during face preprocessing: {e}\")\n        return None\n\ndef predict_emotions_all_models(face_batch, loaded_models, class_names):\n    \"\"\"Predicts emotion using all loaded models.\"\"\"\n    predictions = {}\n    if face_batch is None:\n        return {\"Error\": (\"Preprocessing failed\", 0.0)}\n    try:\n        for model_name, model in loaded_models.items():\n            preds_probs = model.predict(face_batch, verbose=0)\n            predicted_class_index = np.argmax(preds_probs[0])\n            confidence = np.max(preds_probs[0])\n            predicted_emotion = class_names[predicted_class_index]\n            predictions[model_name] = (predicted_emotion, confidence) # Store tuple (label, confidence)\n        return predictions\n    except Exception as e:\n        print(f\"Error during emotion prediction: {e}\")\n        return {\"Error\": (f\"Prediction failed: {e}\", 0.0)}\n\n# --- *** UPDATED Drawing Function *** ---\ndef draw_results_custom(frame, faces_data):\n    \"\"\"Draws corner boxes and color-coded, aligned emotion labels.\"\"\"\n    for data in faces_data:\n        box = data[\"box\"]\n        predictions = data[\"predictions\"] # Dictionary of predictions\n        x1, y1, x2, y2 = box\n\n        # --- Draw Bounding Box Corners ---\n        corner_length = int(min(x2 - x1, y2 - y1) * CORNER_LENGTH_FACTOR)\n        # Define the main color for corners (can be model-specific if desired, but using one color is simpler)\n        corner_color = MODEL_COLORS.get('Ensemble', MODEL_COLORS['Default']) # Use Ensemble color for corners\n\n        # Top-left corner\n        cv2.line(frame, (x1, y1), (x1 + corner_length, y1), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x1, y1), (x1, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        # Bottom-right corner\n        cv2.line(frame, (x2 - corner_length, y2), (x2, y2), corner_color, CORNER_THICKNESS)\n        cv2.line(frame, (x2, y2 - corner_length), (x2, y2), corner_color, CORNER_THICKNESS)\n        # Optional: Top-right corner\n        # cv2.line(frame, (x2 - corner_length, y1), (x2, y1), corner_color, CORNER_THICKNESS)\n        # cv2.line(frame, (x2, y1), (x2, y1 + corner_length), corner_color, CORNER_THICKNESS)\n        # Optional: Bottom-left corner\n        # cv2.line(frame, (x1, y2), (x1 + corner_length, y2), corner_color, CORNER_THICKNESS)\n        # cv2.line(frame, (x1, y2 - corner_length), (x1, y2), corner_color, CORNER_THICKNESS)\n\n        # --- Prepare and Draw Text Labels ---\n        y_offset = y1 - 7 # Starting position slightly higher for smaller font\n        line_height = int(FONT_SCALE * 30) # Adjust spacing based on font scale\n        max_w = 0 # Calculate max width needed for background\n\n        # Determine text width first\n        labels_to_draw = []\n        for model_name, (emotion, confidence) in predictions.items():\n             if model_name == \"Error\": continue\n             label = f\"{model_name}: {emotion} ({confidence:.2f})\"\n             labels_to_draw.append((label, model_name)) # Store label and model name for color lookup\n             (w, h), _ = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n             if w > max_w: max_w = w\n\n        # Draw background and text for each label, moving upwards\n        num_labels = len(labels_to_draw)\n        for i, (label, model_name) in enumerate(reversed(labels_to_draw)): # Draw from bottom up\n            text_y = y_offset - (i * line_height)\n            bg_y1 = text_y - (line_height - int(FONT_SCALE*10)) # Adjust background size based on font\n            bg_y2 = text_y + int(FONT_SCALE*5)\n\n            # Ensure background is within frame boundaries\n            bg_y1 = max(0, bg_y1)\n            if bg_y1 >= bg_y2 or bg_y1 >= y1: continue # Prevent overlap with box top or invalid rect\n\n            # Get model-specific color for background\n            bg_color = MODEL_COLORS.get(model_name, MODEL_COLORS['Default'])\n\n            # Draw background rectangle\n            cv2.rectangle(frame, (x1, bg_y1), (x1 + max_w + 5, bg_y2), bg_color, -1) # Add padding to width\n            # Draw text\n            cv2.putText(frame, label, (x1 + 2, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS) # Add small x offset\n\n\n# =============================================================================\n# Processing Functions for Image/Video Files (Updated to use draw_results_custom)\n# =============================================================================\n\ndef process_frame(frame, frame_rgb=None):\n    \"\"\"Detects faces and predicts emotions on a single frame.\"\"\"\n    if frame_rgb is None:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    faces_data = []\n    try:\n        detections = detector.detect_faces(frame_rgb)\n    except Exception as e:\n        print(f\"Error during face detection: {e}\")\n        cv2.putText(frame, \"Face Detection Error\", (20, 40), FONT, 1, (0, 0, 255), 2)\n        return faces_data\n\n    if detections:\n         for face_info in detections:\n             try:\n                 confidence_face = face_info['confidence']\n                 if confidence_face < MTCNN_MIN_CONFIDENCE: continue\n\n                 x, y, w, h = face_info['box']\n                 x1 = max(0, x); y1 = max(0, y)\n                 x2 = min(frame_rgb.shape[1], x + w); y2 = min(frame_rgb.shape[0], y + h)\n                 if x1 >= x2 or y1 >= y2: continue\n\n                 x1p = max(0, x1 - BOX_PADDING); y1p = max(0, y1 - BOX_PADDING)\n                 x2p = min(frame_rgb.shape[1], x2 + BOX_PADDING); y2p = min(frame_rgb.shape[0], y2 + BOX_PADDING)\n                 if x1p >= x2p or y1p >= y2p: continue\n\n                 face_crop_rgb = frame_rgb[y1p:y2p, x1p:x2p]\n                 if face_crop_rgb.size == 0: continue\n\n                 face_batch = preprocess_face_for_emotion(face_crop_rgb, IMG_SIZE)\n                 all_predictions = predict_emotions_all_models(face_batch, models, CLASS_NAMES)\n\n                 faces_data.append({\n                     \"box\": (x1, y1, x2, y2),\n                     \"predictions\": all_predictions\n                 })\n             except Exception as e:\n                 print(f\"Error processing detected face: {e}\")\n                 try: cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                 except: pass\n    return faces_data\n\ndef process_image_file(image_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a single image file.\"\"\"\n    print(f\"\\nProcessing image: {image_path}\")\n    frame = cv2.imread(image_path)\n    if frame is None:\n        print(f\"Error: Could not read image file: {image_path}\")\n        return\n\n    faces_data = process_frame(frame)\n    # --- Use updated drawing function ---\n    draw_results_custom(frame, faces_data)\n\n    print(\"Prediction Results:\")\n    for i, res in enumerate(faces_data):\n        print(f\"  Face {i+1} - Box: {res['box']}, Predictions: {res['predictions']}\")\n\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(image_path))\n        try:\n            cv2.imwrite(output_filename, frame)\n            print(f\"Annotated image saved to {output_filename}\")\n        except Exception as e:\n            print(f\"Error saving image {output_filename}: {e}\")\n    else:\n        plt.figure(figsize=(12, 9))\n        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Detected Faces and Emotions (MTCNN) - {os.path.basename(image_path)}\")\n        plt.axis('off')\n        plt.show()\n\ndef process_video_file(video_path, output_dir):\n    \"\"\"Loads, processes, and displays/saves a video file.\"\"\"\n    print(f\"\\nProcessing video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file: {video_path}\")\n        return\n\n    writer = None\n    if SAVE_OUTPUT:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_filename = os.path.join(output_dir, \"annotated_\" + os.path.basename(video_path))\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        if fps <= 0: # Handle case where fps is not read correctly\n            print(\"Warning: Could not read video FPS, defaulting to 25.\")\n            fps = 25\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        writer = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n        print(f\"Saving annotated video to {output_filename}\")\n\n\n    frame_count = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n\n        faces_data = process_frame(frame)\n        # --- Use updated drawing function ---\n        draw_results_custom(frame, faces_data)\n\n        frame_count += 1\n        if frame_count > 1 and is_video: # Added is_video check\n             elapsed_time = time.time() - start_time\n             fps_display = frame_count / elapsed_time if elapsed_time > 0 else 0\n             cv2.putText(frame, f\"FPS: {fps_display:.2f}\", (10, 30), FONT, 0.7, (255, 255, 255), 2)\n\n        if writer:\n             writer.write(frame)\n        else:\n            cv2.imshow(f\"Emotion Recognition (MTCNN - Press 'q' to quit) - {os.path.basename(video_path)}\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    cap.release()\n    if writer:\n        writer.release()\n    cv2.destroyAllWindows()\n    print(f\"Finished processing video: {video_path}\")\n\n# =============================================================================\n# Main Execution - Iterate through Input Directory (Same as before)\n# =============================================================================\nif __name__ == \"__main__\":\n    if not os.path.isdir(INPUT_DIR):\n        print(f\"Error: Input directory not found: {INPUT_DIR}\")\n        exit()\n\n    print(f\"Scanning directory: {INPUT_DIR}\")\n    image_extensions = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff')\n    video_extensions = ('*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv')\n\n    # Create output dir if saving\n    if SAVE_OUTPUT and not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper()))) # Include uppercase extensions\n\n    if image_files:\n        print(f\"\\nFound {len(image_files)} image files to process...\")\n        for img_path in image_files:\n            process_image_file(img_path, OUTPUT_DIR)\n    else:\n        print(\"No image files found in the directory.\")\n\n    video_files = []\n    for ext in video_extensions:\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper()))) # Include uppercase extensions\n\n\n    if video_files:\n        print(f\"\\nFound {len(video_files)} video files to process...\")\n        for vid_path in video_files:\n            process_video_file(vid_path, OUTPUT_DIR)\n    else:\n        print(\"No video files found in the directory.\")\n\n    print(\"\\nAll processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:48:08.798369Z","iopub.execute_input":"2025-04-06T15:48:08.798863Z","iopub.status.idle":"2025-04-06T15:51:29.945822Z","shell.execute_reply.started":"2025-04-06T15:48:08.798807Z","shell.execute_reply":"2025-04-06T15:51:29.944600Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data analysis \nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom PIL import Image\nimport io\nimport cv2\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Set Seed for reproducibility\nSEED = 42\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# Configuration \nCONFIG = {\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\",  # Base path\n    \"TRAIN_DIR\": \"Train\",  # Combined train dir name\n    \"TEST_DIR\": \"Test\",    # Base test dir name\n    \"IMG_SIZE\": 112,        # Image size for visualization\n    \"BATCH_SIZE\": 32,      # Batch size for dataset loading\n    \"FER_ORIGINAL_SIZE\": 48,  # Original size of FER2013 images\n    \"AFFECTNET_SIZE\": 96      # Original/common size of AffectNet images\n}\n\n# =============================================================================\n# Data Loading Functions\n# =============================================================================\n\ndef create_train_tf_dataset(directory, image_size, batch_size):\n    \"\"\"Creates tf.data.Dataset for training data analysis.\"\"\"\n    print(f\"Loading training data from: {directory}\")\n\n    # Create the dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        batch_size=batch_size,\n        shuffle=False  # Don't shuffle to maintain order for analysis\n    )\n    \n    # Get class names from the training dataset\n    class_names = train_ds.class_names\n    print(f\"Dataset loaded with classes: {class_names}\")\n    \n    return train_ds, class_names\n\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size):\n    \"\"\"Creates a tf.data.Dataset for testing by finding files in the Test/<emotion>/<target_dataset> structure.\"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\")\n    all_image_paths = []\n    all_labels = []\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) \n                   if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    \n    if not emotion_dirs:\n        print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\")\n        return None, []\n    \n    print(f\"Found emotion folders: {emotion_dirs}\")\n    \n    class_counts = {}  # Dictionary to store counts per class\n    \n    for emotion in emotion_dirs:\n        if emotion not in class_names_map:\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\")\n            continue\n            \n        label_index = class_names_map[emotion]  # Get the integer label\n        target_path = os.path.join(base_test_dir, emotion, target_dataset)\n        \n        if not tf.io.gfile.exists(target_path):\n            print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\")\n            continue\n            \n        # Find all image files\n        image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                     tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                     tf.io.gfile.glob(os.path.join(target_path, '*.jpeg'))\n                     \n        if not image_files:\n            print(f\"Warning: No image files found in '{target_path}'.\")\n            continue\n            \n        all_image_paths.extend(image_files)\n        all_labels.extend([label_index] * len(image_files))\n        \n        # Store counts for this class\n        class_counts[emotion] = len(image_files)\n        \n        print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\")\n    \n    if not all_image_paths:\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\")\n        return None, class_counts\n        \n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\")\n    \n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=len(class_names_map)))\n    \n    # Define preprocessing function for loading images\n    def load_and_preprocess_image(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n        image = tf.image.resize(image, [image_size, image_size], method='nearest')\n        image.set_shape((image_size, image_size, 3))\n        return image\n    \n    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Combine images and labels\n    test_ds = tf.data.Dataset.zip((image_ds, label_ds))\n    test_ds = test_ds.batch(batch_size)\n    \n    return test_ds, class_counts\n\n# =============================================================================\n# Analysis Functions\n# =============================================================================\n\ndef analyze_train_dataset(dataset, class_names):\n    \"\"\"Analyzes the training dataset to get class distribution and example images.\"\"\"\n    print(\"\\n=== Analyzing Training Dataset ===\")\n    \n    all_labels = []\n    example_images = {i: None for i in range(len(class_names))}\n    example_counts = {i: 0 for i in range(len(class_names))}\n    \n    # Get sample images for each class and count instances\n    print(\"Extracting class distribution and example images...\")\n    for images, labels in dataset:\n        batch_labels = np.argmax(labels.numpy(), axis=1)\n        all_labels.extend(batch_labels)\n        \n        # Store some example images (first 3 of each class)\n        for i, label in enumerate(batch_labels):\n            if example_counts[label] < 3 and example_images[label] is None:\n                example_images[label] = images[i].numpy()\n            example_counts[label] += 1\n    \n    # Calculate class distribution\n    class_counts = Counter(all_labels)\n    total_samples = len(all_labels)\n    \n    # Print the class distribution\n    print(\"\\nClass Distribution in Training Data:\")\n    print(f\"{'Class':<15} {'Name':<15} {'Count':<10} {'Percentage':<10}\")\n    print(\"-\" * 50)\n    \n    for i in range(len(class_names)):\n        count = class_counts.get(i, 0)\n        percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n        print(f\"{i:<15} {class_names[i]:<15} {count:<10} {percentage:.2f}%\")\n    \n    print(f\"\\nTotal training samples: {total_samples}\")\n    \n    # Plot the class distribution\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=list(class_names), y=[class_counts.get(i, 0) for i in range(len(class_names))])\n    plt.title('Class Distribution in Training Dataset')\n    plt.xlabel('Emotion Class')\n    plt.ylabel('Number of Samples')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig('train_class_distribution.png')\n    print(\"Saved training class distribution plot to 'train_class_distribution.png'\")\n    \n    # Plot example images\n    plt.figure(figsize=(15, 8))\n    for i, class_name in enumerate(class_names):\n        if example_images[i] is not None:\n            plt.subplot(2, 4, i+1)\n            plt.imshow(example_images[i].astype(\"uint8\"))\n            plt.title(f\"{class_name} ({class_counts.get(i, 0)})\")\n            plt.axis(\"off\")\n    \n    plt.tight_layout()\n    plt.savefig('train_example_images.png')\n    print(\"Saved example images to 'train_example_images.png'\")\n    \n    return class_counts\n\ndef analyze_test_dataset(test_counts, dataset_name, class_names):\n    \"\"\"Analyzes the test dataset class distribution.\"\"\"\n    if not test_counts:\n        print(f\"\\nNo {dataset_name} test data available for analysis.\")\n        return\n    \n    print(f\"\\n=== Analyzing {dataset_name} Test Dataset ===\")\n    \n    # Calculate total samples\n    total_samples = sum(test_counts.values())\n    \n    # Print the class distribution\n    print(f\"\\nClass Distribution in {dataset_name} Test Data:\")\n    print(f\"{'Class':<15} {'Name':<15} {'Count':<10} {'Percentage':<10}\")\n    print(\"-\" * 50)\n    \n    # Create ordered data for plotting\n    ordered_counts = []\n    for i, class_name in enumerate(class_names):\n        count = test_counts.get(class_name, 0)\n        percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n        print(f\"{i:<15} {class_name:<15} {count:<10} {percentage:.2f}%\")\n        ordered_counts.append(count)\n    \n    print(f\"\\nTotal {dataset_name} test samples: {total_samples}\")\n    \n    # Plot the class distribution\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=list(class_names), y=ordered_counts)\n    plt.title(f'Class Distribution in {dataset_name} Test Dataset')\n    plt.xlabel('Emotion Class')\n    plt.ylabel('Number of Samples')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{dataset_name.lower()}_test_class_distribution.png')\n    print(f\"Saved {dataset_name} test class distribution plot to '{dataset_name.lower()}_test_class_distribution.png'\")\n    \n    return ordered_counts\n\ndef display_class_imbalance_stats(train_counts, affectnet_counts, fer2013_counts, class_names):\n    \"\"\"Displays comparative statistics about class imbalance across datasets.\"\"\"\n    if not all([train_counts, affectnet_counts, fer2013_counts]):\n        print(\"\\nCannot perform complete class imbalance analysis due to missing data.\")\n        return\n    \n    # Convert to numpy arrays for easier manipulation\n    train_array = np.array([train_counts.get(i, 0) for i in range(len(class_names))])\n    affectnet_array = np.array(affectnet_counts)\n    fer2013_array = np.array(fer2013_counts)\n    \n    # Calculate statistics\n    mean_train = np.mean(train_array)\n    std_train = np.std(train_array)\n    cv_train = std_train / mean_train if mean_train > 0 else 0  # Coefficient of variation\n    \n    max_to_min_train = np.max(train_array) / np.min(train_array) if np.min(train_array) > 0 else np.inf\n    \n    # Print imbalance statistics\n    print(\"\\n=== Class Imbalance Statistics ===\")\n    print(f\"Training dataset:\")\n    print(f\"  Mean samples per class: {mean_train:.2f}\")\n    print(f\"  Standard deviation: {std_train:.2f}\")\n    print(f\"  Coefficient of variation: {cv_train:.4f} (higher = more imbalanced)\")\n    print(f\"  Max/Min ratio: {max_to_min_train:.2f} (higher = more imbalanced)\")\n    \n    # Plot comparative distribution\n    plt.figure(figsize=(14, 8))\n    \n    # Normalize counts to percentages for better comparison\n    train_pct = train_array / np.sum(train_array) * 100 if np.sum(train_array) > 0 else np.zeros_like(train_array)\n    affectnet_pct = affectnet_array / np.sum(affectnet_array) * 100 if np.sum(affectnet_array) > 0 else np.zeros_like(affectnet_array)\n    fer2013_pct = fer2013_array / np.sum(fer2013_array) * 100 if np.sum(fer2013_array) > 0 else np.zeros_like(fer2013_array)\n    \n    # Create dataframe for plotting\n    data = []\n    for i, class_name in enumerate(class_names):\n        data.append({\n            'Class': class_name,\n            'Train (%)': train_pct[i],\n            'AffectNet Test (%)': affectnet_pct[i],\n            'FER2013 Test (%)': fer2013_pct[i]\n        })\n    \n    df = pd.DataFrame(data)\n    \n    # Plot\n    df_melted = pd.melt(df, id_vars=['Class'], var_name='Dataset', value_name='Percentage')\n    sns.barplot(x='Class', y='Percentage', hue='Dataset', data=df_melted)\n    plt.title('Class Distribution Comparison Across Datasets')\n    plt.xlabel('Emotion Class')\n    plt.ylabel('Percentage of Samples')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig('class_distribution_comparison.png')\n    print(\"Saved comparative class distribution plot to 'class_distribution_comparison.png'\")\n\ndef find_class_outliers(train_counts, class_names):\n    \"\"\"Identifies outlier classes in terms of sample count.\"\"\"\n    counts_array = np.array([train_counts.get(i, 0) for i in range(len(class_names))])\n    mean_count = np.mean(counts_array)\n    std_count = np.std(counts_array)\n    \n    print(\"\\n=== Class Sample Count Outlier Analysis ===\")\n    print(f\"Mean samples per class: {mean_count:.2f}\")\n    print(f\"Standard deviation: {std_count:.2f}\")\n    \n    # Classes with significantly fewer samples (Z-score < -1)\n    underrepresented = []\n    for i, count in enumerate(counts_array):\n        z_score = (count - mean_count) / std_count if std_count > 0 else 0\n        if z_score < -1:\n            underrepresented.append((class_names[i], count, z_score))\n    \n    if underrepresented:\n        print(\"\\nUnderrepresented classes (Z-score < -1):\")\n        for name, count, z in underrepresented:\n            print(f\"  {name}: {count} samples (Z-score: {z:.2f})\")\n    else:\n        print(\"\\nNo significantly underrepresented classes found.\")\n    \n    # Classes with significantly more samples (Z-score > 1)\n    overrepresented = []\n    for i, count in enumerate(counts_array):\n        z_score = (count - mean_count) / std_count if std_count > 0 else 0\n        if z_score > 1:\n            overrepresented.append((class_names[i], count, z_score))\n    \n    if overrepresented:\n        print(\"\\nOverrepresented classes (Z-score > 1):\")\n        for name, count, z in overrepresented:\n            print(f\"  {name}: {count} samples (Z-score: {z:.2f})\")\n    else:\n        print(\"\\nNo significantly overrepresented classes found.\")\n\n# =============================================================================\n# New Visualization Functions for Directory Structure and Image Properties\n# =============================================================================\n\ndef visualize_directory_structure(base_path):\n    \"\"\"Creates a graphical display of the directory structure.\"\"\"\n    print(\"\\n=== Visualizing Directory Structure ===\")\n    \n    # Function to get directory structure recursively\n    def get_tree(dir_path, prefix=\"\", is_last=True, max_depth=3, current_depth=0):\n        if current_depth > max_depth:\n            return []\n        \n        output = []\n        current_prefix = prefix\n        if current_depth > 0:\n            connector = \" \" if is_last else \" \"\n            current_prefix = prefix + connector\n            output.append(current_prefix + os.path.basename(dir_path) + \"/\")\n            prefix = prefix + (\"    \" if is_last else \"   \")\n        \n        try:\n            entries = list(os.listdir(dir_path))\n            dirs = [e for e in entries if os.path.isdir(os.path.join(dir_path, e))]\n            files = [e for e in entries if not os.path.isdir(os.path.join(dir_path, e))]\n            \n            # Count files if there are too many to list\n            if len(files) > 5:\n                output.append(prefix + f\" {len(files)} files (not shown)\")\n            else:\n                for i, file in enumerate(files):\n                    connector = \" \" if i == len(files) - 1 and len(dirs) == 0 else \" \"\n                    output.append(prefix + connector + file)\n                    \n            # Process directories\n            for i, d in enumerate(dirs):\n                is_last_dir = (i == len(dirs) - 1)\n                output.extend(get_tree(os.path.join(dir_path, d), prefix, is_last_dir, \n                                      max_depth, current_depth + 1))\n                \n        except (PermissionError, FileNotFoundError) as e:\n            output.append(prefix + f\"  Error: {str(e)}\")\n        \n        return output\n    \n    # Generate the tree structure as text\n    tree_lines = get_tree(base_path)\n    \n    # Create a figure with text representation\n    plt.figure(figsize=(15, 10))\n    plt.text(0.1, 0.5, \"\\n\".join([base_path + \"/\"] + tree_lines), \n             fontsize=10, fontfamily='monospace', va='center')\n    plt.axis('off')\n    plt.title(\"Dataset Directory Structure\")\n    plt.tight_layout()\n    plt.savefig('directory_structure.png')\n    print(\"Saved directory structure visualization to 'directory_structure.png'\")\n    \n    return tree_lines\n\ndef visualize_image_dimensions(base_test_dir, class_names_map):\n    \"\"\"Creates a visualization showing image dimensions for both datasets.\"\"\"\n    print(\"\\n=== Analyzing Image Dimensions ===\")\n    \n    # Setup for counting dimensions\n    fer_dimensions = {}\n    affectnet_dimensions = {}\n    \n    # Get emotion directories\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) \n                   if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    \n    # Check images from both datasets to get dimensions\n    for emotion in emotion_dirs:\n        if emotion not in class_names_map:\n            continue\n            \n        # Check FER2013 images\n        fer_path = os.path.join(base_test_dir, emotion, \"fer2013\")\n        if tf.io.gfile.exists(fer_path):\n            image_files = tf.io.gfile.glob(os.path.join(fer_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(fer_path, '*.jpg')) + \\\n                         tf.io.gfile.glob(os.path.join(fer_path, '*.jpeg'))\n            \n            for img_path in image_files[:5]:  # Sample up to 5 images\n                try:\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    height, width = img.shape[0], img.shape[1]\n                    dim_key = f\"{height}x{width}\"\n                    if dim_key in fer_dimensions:\n                        fer_dimensions[dim_key] += 1\n                    else:\n                        fer_dimensions[dim_key] = 1\n                except Exception as e:\n                    print(f\"Error reading FER image {img_path}: {e}\")\n        \n        # Check AffectNet images\n        affectnet_path = os.path.join(base_test_dir, emotion, \"affectnet\")\n        if tf.io.gfile.exists(affectnet_path):\n            image_files = tf.io.gfile.glob(os.path.join(affectnet_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(affectnet_path, '*.jpg')) + \\\n                         tf.io.gfile.glob(os.path.join(affectnet_path, '*.jpeg'))\n            \n            for img_path in image_files[:5]:  # Sample up to 5 images\n                try:\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    height, width = img.shape[0], img.shape[1]\n                    dim_key = f\"{height}x{width}\"\n                    if dim_key in affectnet_dimensions:\n                        affectnet_dimensions[dim_key] += 1\n                    else:\n                        affectnet_dimensions[dim_key] = 1\n                except Exception as e:\n                    print(f\"Error reading AffectNet image {img_path}: {e}\")\n    \n    print(f\"FER2013 dimensions detected: {fer_dimensions}\")\n    print(f\"AffectNet dimensions detected: {affectnet_dimensions}\")\n    \n    # Create visualization of the dimensions and dataset sizes\n    plt.figure(figsize=(12, 8))\n    \n    # Size comparison visualization\n    ax1 = plt.subplot(1, 2, 1)\n    # Draw image size comparisons\n    fer_size = CONFIG[\"FER_ORIGINAL_SIZE\"]\n    affectnet_size = CONFIG[\"AFFECTNET_SIZE\"]\n    \n    # FER rectangle\n    fer_rect = patches.Rectangle((0, 0), fer_size, fer_size, linewidth=2, edgecolor='r', facecolor='none')\n    ax1.add_patch(fer_rect)\n    ax1.text(fer_size/2, fer_size/2, f\"FER2013\\n{fer_size}{fer_size}\", ha='center', va='center')\n    \n    # AffectNet rectangle\n    affectnet_rect = patches.Rectangle((fer_size + 10, 0), affectnet_size, affectnet_size, \n                                      linewidth=2, edgecolor='b', facecolor='none')\n    ax1.add_patch(affectnet_rect)\n    ax1.text(fer_size + 10 + affectnet_size/2, affectnet_size/2, \n             f\"AffectNet\\n{affectnet_size}{affectnet_size}\", ha='center', va='center')\n    \n    # Set limits and labels\n    ax1.set_xlim(-5, fer_size + affectnet_size + 20)\n    ax1.set_ylim(-5, max(fer_size, affectnet_size) + 5)\n    ax1.set_title(\"Image Size Comparison\")\n    ax1.set_xlabel(\"Width (pixels)\")\n    ax1.set_ylabel(\"Height (pixels)\")\n    ax1.grid(True, linestyle='--', alpha=0.7)\n    \n    # Area comparison\n    ax2 = plt.subplot(1, 2, 2)\n    datasets = ['FER2013', 'AffectNet']\n    sizes = [fer_size**2, affectnet_size**2]  # Area in pixels\n    bars = ax2.bar(datasets, sizes, color=['r', 'b'], alpha=0.7)\n    \n    # Add pixel count labels on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                f'{int(height)} pixels', ha='center', va='bottom')\n    \n    ax2.set_title('Image Area Comparison')\n    ax2.set_ylabel('Area (pixels)')\n    ax2.set_ylim(0, max(sizes) * 1.2)  # Add some space above bars for labels\n    \n    plt.tight_layout()\n    plt.savefig('image_dimensions_comparison.png')\n    print(\"Saved image dimensions visualization to 'image_dimensions_comparison.png'\")\n\ndef visualize_grayscale_vs_rgb(base_test_dir, class_names_map):\n    \"\"\"Creates a visualization showing examples of grayscale and RGB images.\"\"\"\n    print(\"\\n=== Visualizing Grayscale vs RGB Examples ===\")\n    \n    # Setup to store example images\n    fer_images = []\n    affectnet_images = []\n    \n    # Get emotion directories\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) \n                   if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    \n    # Get sample images from each dataset\n    for emotion in emotion_dirs:\n        if emotion not in class_names_map or len(fer_images) >= 3:\n            continue  # Stop after getting 3 examples\n            \n        # Get FER2013 image\n        fer_path = os.path.join(base_test_dir, emotion, \"fer2013\")\n        if tf.io.gfile.exists(fer_path) and len(fer_images) < 3:\n            image_files = tf.io.gfile.glob(os.path.join(fer_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(fer_path, '*.jpg'))\n            \n            if image_files:\n                try:\n                    img_path = image_files[0]\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    fer_images.append((img.numpy(), emotion))\n                except Exception as e:\n                    print(f\"Error reading FER image: {e}\")\n        \n        # Get AffectNet image\n        affectnet_path = os.path.join(base_test_dir, emotion, \"affectnet\")\n        if tf.io.gfile.exists(affectnet_path) and len(affectnet_images) < 3:\n            image_files = tf.io.gfile.glob(os.path.join(affectnet_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(affectnet_path, '*.jpg'))\n            \n            if image_files:\n                try:\n                    img_path = image_files[0]\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    affectnet_images.append((img.numpy(), emotion))\n                except Exception as e:\n                    print(f\"Error reading AffectNet image: {e}\")\n    \n    # Create figure to visualize grayscale vs RGB\n    plt.figure(figsize=(15, 10))\n    \n    # FER2013 (commonly grayscale) examples\n    for i, (img, emotion) in enumerate(fer_images):\n        if i >= 3:\n            break\n            \n        # Original image\n        plt.subplot(3, 4, i*4+1)\n        plt.imshow(img)\n        plt.title(f\"FER2013 - {emotion}\")\n        plt.axis('off')\n        \n        # Grayscale channels visualization\n        plt.subplot(3, 4, i*4+2)\n        gs_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        plt.imshow(gs_img, cmap='gray')\n        plt.title(\"Grayscale\")\n        plt.axis('off')\n        \n        # RGB channels visualization\n        # Red channel\n        plt.subplot(3, 4, i*4+3)\n        r_channel = np.zeros_like(img)\n        r_channel[:,:,0] = img[:,:,0]\n        plt.imshow(r_channel)\n        plt.title(\"Red Channel\")\n        plt.axis('off')\n        \n        # RGB histogram\n        plt.subplot(3, 4, i*4+4)\n        color = ('red', 'green', 'blue')\n        for j, col in enumerate(color):\n            histr = cv2.calcHist([img], [j], None, [256], [0,256])\n            plt.plot(histr, color=col)\n        plt.title(\"RGB Histogram\")\n        plt.xlim([0, 256])\n    \n    plt.tight_layout()\n    plt.savefig('grayscale_vs_rgb_examples.png')\n    print(\"Saved grayscale vs RGB visualization to 'grayscale_vs_rgb_examples.png'\")\n    \n    return fer_images, affectnet_images\n\ndef collect_examples_per_class(base_test_dir, class_names_map, class_names):\n    \"\"\"Collects example images from each emotion class for both datasets.\"\"\"\n    print(\"\\n=== Collecting Example Images for Each Class ===\")\n    \n    # Dictionary to store examples\n    fer_examples = {emotion: None for emotion in class_names}\n    affectnet_examples = {emotion: None for emotion in class_names}\n    \n    # Get emotion directories\n    for emotion in class_names:\n        emotion_path = os.path.join(base_test_dir, emotion)\n        if not tf.io.gfile.exists(emotion_path):\n            continue\n            \n        # Get FER2013 example\n        fer_path = os.path.join(emotion_path, \"fer2013\")\n        if tf.io.gfile.exists(fer_path):\n            image_files = tf.io.gfile.glob(os.path.join(fer_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(fer_path, '*.jpg')) + \\\n                         tf.io.gfile.glob(os.path.join(fer_path, '*.jpeg'))\n            \n            if image_files:\n                try:\n                    img_path = image_files[0]\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    img = tf.image.resize(img, [CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"]])\n                    fer_examples[emotion] = img.numpy()\n                except Exception as e:\n                    print(f\"Error reading FER example for {emotion}: {e}\")\n        \n        # Get AffectNet example\n        affectnet_path = os.path.join(emotion_path, \"affectnet\")\n        if tf.io.gfile.exists(affectnet_path):\n            image_files = tf.io.gfile.glob(os.path.join(affectnet_path, '*.png')) + \\\n                         tf.io.gfile.glob(os.path.join(affectnet_path, '*.jpg')) + \\\n                         tf.io.gfile.glob(os.path.join(affectnet_path, '*.jpeg'))\n            \n            if image_files:\n                try:\n                    img_path = image_files[0]\n                    img = tf.io.read_file(img_path)\n                    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                    img = tf.image.resize(img, [CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"]])\n                    affectnet_examples[emotion] = img.numpy()\n                except Exception as e:\n                    print(f\"Error reading AffectNet example for {emotion}: {e}\")\n    \n    # Create visualization with examples from both datasets for each class\n    plt.figure(figsize=(15, len(class_names) * 3))\n    \n    for i, emotion in enumerate(class_names):\n        # FER2013 example\n        plt.subplot(len(class_names), 2, i*2+1)\n        if fer_examples[emotion] is not None:\n            plt.imshow(fer_examples[emotion].astype(\"uint8\"))\n            plt.title(f\"FER2013 - {emotion}\")\n        else:\n            plt.text(0.5, 0.5, \"No example\", ha='center', va='center')\n            plt.title(f\"FER2013 - {emotion} (missing)\")\n        plt.axis('off')\n        \n        # AffectNet example\n        plt.subplot(len(class_names), 2, i*2+2)\n        if affectnet_examples[emotion] is not None:\n            plt.imshow(affectnet_examples[emotion].astype(\"uint8\"))\n            plt.title(f\"AffectNet - {emotion}\")\n        else:\n            plt.text(0.5, 0.5, \"No example\", ha='center', va='center')\n            plt.title(f\"AffectNet - {emotion} (missing)\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('class_examples_comparison.png')\n    print(\"Saved class examples comparison to 'class_examples_comparison.png'\")\n\n# =============================================================================\n# Main Execution\n# =============================================================================\n\ndef main():\n    # Load the training dataset\n    train_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"])\n    train_ds, class_names = create_train_tf_dataset(train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\n    \n    # Create a mapping from class name to integer index for test set loading\n    class_names_map = {name: i for i, name in enumerate(class_names)}\n    \n    # Load test datasets\n    base_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"])\n    affectnet_test_ds, affectnet_counts = create_test_dataset_from_structure(\n        base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n    )\n    \n    fer2013_test_ds, fer2013_counts = create_test_dataset_from_structure(\n        base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n    )\n    \n    # NEW: Visualize directory structure\n    visualize_directory_structure(CONFIG[\"BASE_DATA_DIR\"])\n    \n    # NEW: Visualize image dimensions\n    visualize_image_dimensions(base_test_dir_path, class_names_map)\n    \n    # NEW: Visualize grayscale vs RGB examples\n    fer_images, affectnet_images = visualize_grayscale_vs_rgb(base_test_dir_path, class_names_map)\n    \n    # NEW: Collect example images for each class from both datasets\n    collect_examples_per_class(base_test_dir_path, class_names_map, class_names)\n    \n    # Analyze datasets\n    train_counts = analyze_train_dataset(train_ds, class_names)\n    affectnet_test_counts = analyze_test_dataset(affectnet_counts, \"AffectNet\", class_names)\n    fer2013_test_counts = analyze_test_dataset(fer2013_counts, \"FER2013\", class_names)\n    \n    # Additional analyses\n    find_class_outliers(train_counts, class_names)\n    display_class_imbalance_stats(train_counts, affectnet_test_counts, fer2013_test_counts, class_names)\n    \n    print(\"\\nData analysis complete!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:56:04.303649Z","iopub.execute_input":"2025-04-19T15:56:04.304010Z","iopub.status.idle":"2025-04-19T15:57:07.499296Z","shell.execute_reply.started":"2025-04-19T15:56:04.303982Z","shell.execute_reply":"2025-04-19T15:57:07.498479Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.3 (AdamW, Label Smoothing, Ensemble Weight Optimization)\n\n#=== FINAL RESULTS === 144\n#(Using Optimized Ensemble Weights: [0.25, 0.75])\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.81\n#Ensemble FER2013 Test Accuracy: 0.57\n\n\n#=== FINAL RESULTS === 112\n#(Using Optimized Ensemble Weights: [0.4, 0.6])\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.7484, F1: 0.7463\n#EfficientNetB0 FER2013 Test Accuracy: 0.5571, F1: 0.5547\n#EfficientNetB4 AffectNet Test Accuracy: 0.8341, F1: 0.8344\n#EfficientNetB4 FER2013 Test Accuracy: 0.5599, F1: 0.5599\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.8508\n#Ensemble AffectNet F1 Score: 0.8511\n#Ensemble FER2013 Test Accuracy: 0.5915\n#Ensemble FER2013 F1 Score: 0.5906\n\n#=== FINAL RESULTS === 160\n#(Using Optimized Ensemble Weights: [0.35, 0.64])\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.7124, F1: 0.7089\n#EfficientNetB0 FER2013 Test Accuracy: 0.5722, F1: 0.5702\n#EfficientNetB4 AffectNet Test Accuracy: 0.7912, F1: 0.7912\n#EfficientNetB4 FER2013 Test Accuracy: 0.5400, F1: 0.5400\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.8090\n#Ensemble AffectNet F1 Score: 0.8083\n#Ensemble FER2013 Test Accuracy: 0.5784\n#Ensemble FER2013 F1 Score: 0.5761\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score # Added accuracy_score\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\nimport math\nimport glob # Needed for finding files\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path [cite: 1]\n    \"TRAIN_DIR\": \"Train\", # Combined train dir name [cite: 1]\n    \"TEST_DIR\": \"Test\",   # Base test dir name [cite: 2]\n    \"IMG_SIZE\": 104, # 224= accuracy 0.49 / 380=x / 336=x (160px, 192px, or 256px)\n    \"BATCH_SIZE\": 32, # Keep user's tuned value [cite: 2]\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE, # [cite: 2]\n    \"EPOCHS_HEAD\": 50, # Keep user's tuned value [cite: 2]\n    \"EPOCHS_FINE_TUNE\": 100, # Keep user's tuned value [cite: 2]\n    \"LR_HEAD\": 1e-3, # [cite: 2]\n    \"LR_FINE_TUNE_START\": 1e-4, # [cite: 2]\n    \"DROPOUT_RATE\": 0.4, # [cite: 2]\n    \"NUM_CLASSES\": 8, # [cite: 2]\n    \"MODEL_ARCH_1\": \"EfficientNetB0\", # [cite: 2]\n    \"MODEL_ARCH_2\": \"EfficientNetB4\", # [cite: 2]\n    \"ENSEMBLE_WEIGHTS\": [0.5, 0.5], # Initial value, will be optimized later [cite: 2]\n    \"FINE_TUNE_LAYERS_B0\": 15, # Keep user's tuned value [cite: 3]\n    \"FINE_TUNE_LAYERS_B4\": 20, # Keep user's tuned value [cite: 3]\n    \"LOG_DIR_BASE\": \"logs/fit/\", # [cite: 3]\n\n    # --- NEW: Added Parameters ---\n    \"WEIGHT_DECAY\": 1e-5, # Recommended value for AdamW, tune if needed\n    \"LABEL_SMOOTHING\": 0.1, # Common value for label smoothing\n}\n\n# Set Seed\ntf.random.set_seed(CONFIG[\"SEED\"]) # [cite: 3]\nnp.random.seed(CONFIG[\"SEED\"]) # [cite: 3]\n\n# =============================================================================\n# GPU Configuration & Mixed Precision (Same as before)\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU') # [cite: 3]\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True) # [cite: 3]\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\") # [cite: 3]\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\") # [cite: 3]\nelse:\n    print(\"No GPU detected. Running on CPU.\") # [cite: 3]\n\npolicy = tf.keras.mixed_precision.Policy('mixed_float16') # [cite: 4]\ntf.keras.mixed_precision.set_global_policy(policy) # [cite: 4]\nprint(\"Mixed precision enabled ('mixed_float16')\") # [cite: 4]\n\n# =============================================================================\n# Data Loading (Train/Validation - Standard; Test - Custom)\n# =============================================================================\n\ndef create_train_val_tf_dataset(directory, image_size, batch_size, validation_split=0.2): # [cite: 4]\n    \"\"\"Creates tf.data.Dataset for training and validation using image_dataset_from_directory.\"\"\"\n    print(f\"Loading training/validation data from: {directory}\") # [cite: 4]\n\n    # Create the training dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory( # [cite: 4]\n        directory,\n        labels='inferred', # [cite: 4]\n        label_mode='categorical', # [cite: 4]\n        image_size=(image_size, image_size), # [cite: 4]\n        interpolation='nearest', # [cite: 4]\n        batch_size=batch_size, # [cite: 5]\n        shuffle=True, # [cite: 5]\n        seed=CONFIG[\"SEED\"], # [cite: 5]\n        validation_split=validation_split, # [cite: 5]\n        subset=\"training\", # [cite: 5]\n    )\n\n    # Create the validation dataset\n    val_ds = tf.keras.utils.image_dataset_from_directory( # [cite: 5]\n        directory,\n        labels='inferred', # [cite: 5]\n        label_mode='categorical', # [cite: 5]\n        image_size=(image_size, image_size), # [cite: 5]\n        interpolation='nearest', # [cite: 5]\n        batch_size=batch_size, # [cite: 6]\n        shuffle=False, # No need to shuffle validation [cite: 6]\n        seed=CONFIG[\"SEED\"], # [cite: 6]\n        validation_split=validation_split, # [cite: 6]\n        subset=\"validation\", # [cite: 6]\n    )\n\n    # Get class names from the training dataset BEFORE optimization\n    class_names = train_ds.class_names # [cite: 6]\n    print(f\"Dataset loaded with classes: {class_names}\") # [cite: 6]\n\n    # Configure performance for both datasets\n    train_ds = train_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 6]\n    val_ds = val_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 6]\n\n    return train_ds, val_ds, class_names # [cite: 6]\n\n# --- NEW: Function to load test data from the specific structure ---\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size): # [cite: 7]\n    \"\"\"\n    Creates a tf.data.Dataset for testing by manually finding files in the Test/<emotion>/<target_dataset> structure. [cite: 7]\n    \"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\") # [cite: 8]\n    all_image_paths = [] # [cite: 8]\n    all_labels = [] # [cite: 8]\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) if tf.io.gfile.isdir(os.path.join(base_test_dir, d))] # [cite: 8]\n    if not emotion_dirs: # [cite: 8]\n         print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\") # [cite: 8]\n         return None # [cite: 8]\n\n    print(f\"Found emotion folders: {emotion_dirs}\") # [cite: 8]\n\n    for emotion in emotion_dirs: # [cite: 8]\n         if emotion not in class_names_map: # [cite: 9]\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\") # [cite: 9]\n            continue # [cite: 9]\n\n         label_index = class_names_map[emotion] # Get the integer label [cite: 9]\n         target_path = os.path.join(base_test_dir, emotion, target_dataset) # [cite: 9]\n\n         if not tf.io.gfile.exists(target_path): # [cite: 9]\n             print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\") # [cite: 9]\n             continue # Skip if the specific dataset subdir doesn't exist for this emotion [cite: 10]\n\n        # Find all image files (adjust extensions if needed)\n         image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpeg')) # [cite: 10]\n\n         if not image_files: # [cite: 11]\n             print(f\"Warning: No image files found in '{target_path}'.\") # [cite: 11]\n             continue # [cite: 11]\n\n         all_image_paths.extend(image_files) # [cite: 11]\n         all_labels.extend([label_index] * len(image_files)) # [cite: 11]\n         print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\") # [cite: 11]\n\n    if not all_image_paths: # [cite: 11]\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\") # [cite: 12]\n        return None # [cite: 12]\n\n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\") # [cite: 12]\n\n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths) # [cite: 12]\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=CONFIG[\"NUM_CLASSES\"])) # Convert labels to one-hot [cite: 12]\n    image_ds = path_ds.map(lambda x: load_and_preprocess_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE) # [cite: 12]\n\n    # Combine images and labels\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds)) # [cite: 12]\n\n    # Batch and prefetch\n    test_ds = image_label_ds.batch(batch_size).prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"]) # [cite: 12]\n\n    return test_ds # [cite: 12]\n\n# --- NEW: Helper function to load and preprocess images from paths ---\ndef load_and_preprocess_image(path, image_size): # [cite: 13]\n    \"\"\"Loads and preprocesses a single image file.\"\"\"\n    image = tf.io.read_file(path) # [cite: 13]\n    image = tf.image.decode_image(image, channels=3, expand_animations=False) # Decode any format [cite: 13]\n    image = tf.image.resize(image, [image_size, image_size], method='nearest') # [cite: 13]\n    image.set_shape((image_size, image_size, 3)) # [cite: 13]\n    # Rescaling: EfficientNet generally handles this internally, but if needed:\n    # image = image / 255.0\n    return image # [cite: 13]\n\n# --- Create Datasets ---\n# Training and Validation Data\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"]) # [cite: 13]\ntrain_ds, val_ds, class_names = create_train_val_tf_dataset( # [cite: 13]\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n)\n\n# Create a mapping from class name to integer index for test set loading\nclass_names_map = {name: i for i, name in enumerate(class_names)} # [cite: 14]\n\n# Test Datasets (using the new custom function)\nbase_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"]) # [cite: 14]\naffectnet_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]) # [cite: 14]\nfer2013_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]) # [cite: 14]\n\n\n# =============================================================================\n# Class Weights Calculation (Same as V3.2)\n# =============================================================================\ndef get_class_weights(dataset, class_names_list): # [cite: 14]\n    print(\"Calculating class weights...\") # [cite: 14]\n    all_labels = [] # [cite: 14]\n    num_batches = tf.data.experimental.cardinality(dataset) # [cite: 14]\n    print(f\"Approximate number of batches in training dataset: {num_batches}\") # [cite: 14]\n\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY: # [cite: 14]\n         print(\"Warning: Cannot determine dataset cardinality accurately. Iterating...\") # [cite: 16]\n         for _, labels_batch in dataset: # Iterate batches # [cite: 16]\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 16]\n         if not all_labels: # [cite: 16]\n             print(\"Error: Could not extract labels. Using uniform weights.\") # [cite: 16]\n             return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 16]\n    else:\n        for _, labels_batch in dataset: # [cite: 16]\n             all_labels.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 17]\n\n    unique_classes, counts = np.unique(all_labels, return_counts=True) # [cite: 17]\n    print(f\"Unique labels found for weight calculation: {unique_classes} with counts {counts}\") # [cite: 17]\n\n    if len(unique_classes) == 0: # [cite: 17]\n        print(\"Error: No labels found. Using uniform weights.\") # [cite: 17]\n        return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 17]\n\n    class_weights = compute_class_weight( # [cite: 17]\n        class_weight='balanced', # [cite: 17]\n        classes=unique_classes, # [cite: 17]\n        y=all_labels # [cite: 17]\n    )\n\n    class_weights_dict = {i: 0.0 for i in range(CONFIG[\"NUM_CLASSES\"])} # [cite: 18]\n    for i, cls_label in enumerate(unique_classes): # [cite: 18]\n        if cls_label < CONFIG[\"NUM_CLASSES\"]: # [cite: 18]\n             class_weights_dict[cls_label] = class_weights[i] # [cite: 18]\n        else:\n             print(f\"Warning: Label {cls_label} >= NUM_CLASSES ({CONFIG['NUM_CLASSES']}). Ignoring.\") # [cite: 19]\n\n    for i in range(CONFIG[\"NUM_CLASSES\"]): # [cite: 19]\n        if class_weights_dict[i] == 0.0 and i in unique_classes: # Check if it was actually missing or just had 0 weight [cite: 19]\n            print(f\"Warning: Class {i} ({class_names_list[i]}) had 0 weight initially. Assigning 1.0.\") # [cite: 19]\n            class_weights_dict[i] = 1.0 # Avoid 0 weight if class exists but wasn't found / calculated [cite: 19]\n        elif class_weights_dict[i] == 0.0: # [cite: 19]\n             print(f\"Info: Class {i} ({class_names_list[i]}) not found in iterated samples. Assigning weight 1.0.\") # [cite: 20]\n             class_weights_dict[i] = 1.0 # [cite: 20]\n\n\n    print(\"Class weights calculated:\", class_weights_dict) # [cite: 20]\n    return class_weights_dict # [cite: 20]\n\nclass_weights = get_class_weights(train_ds, class_names) # [cite: 20]\n\n# =============================================================================\n# Data Augmentation Layer (Same as before)\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([ # [cite: 20]\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]), # [cite: 20]\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]), # [cite: 20]\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]), # [cite: 20]\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building (Same as V3.2)\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate): # [cite: 20]\n    input_shape = (img_size, img_size, 3) # [cite: 20]\n    inputs = Input(shape=input_shape, name=\"input_layer\") # [cite: 21]\n    x = data_augmentation(inputs) # Apply augmentation first [cite: 21]\n\n    if model_arch == \"EfficientNetB0\": # [cite: 21]\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # Let EffNet handle input scaling [cite: 21]\n        x_processed = base_model(x, training=False) # Pass augmented data to base model [cite: 22]\n    elif model_arch == \"EfficientNetB4\": # [cite: 22]\n         print(f\"Warning: Building {model_arch} with image size {img_size}.\") # [cite: 22]\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # [cite: 22]\n         x_processed = base_model(x, training=False) # Pass augmented data to base model [cite: 22]\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\") # [cite: 22]\n\n    base_model.trainable = False # Freeze base model [cite: 22]\n\n    # Add classification head\n    output = Dropout(dropout_rate, name=\"top_dropout\")(x_processed) # Use output from base model [cite: 23]\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(output) # [cite: 23]\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch) # [cite: 3]\n    print(f\"{model_arch} model built successfully.\") # [cite: 23]\n    return model # [cite: 23]\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"]) # [cite: 23]\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"]) # [cite: 23]\n\n# =============================================================================\n# Training Functions (Using AdamW and Label Smoothing Loss)\n# =============================================================================\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0): # [cite: 23]\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name # [cite: 23]\n    checkpoint_path = f\"best_{model.name}.keras\" # [cite: 24]\n\n    # --- Callbacks ---\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=1) # [cite: 24]\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=9, min_lr=1e-6, verbose=1) # Keep user's patience [cite: 24]\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=18, restore_best_weights=True, verbose=1) # Keep user's patience [cite: 24]\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) # [cite: 24]\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback] # [cite: 24]\n    # --- CHANGE: Use AdamW ---\n    optimizer_choice = tf.keras.optimizers.AdamW # [cite: 24]\n\n    # Find the base model layer to control its trainability\n    base_model_layer = None # [cite: 24]\n    for layer in model.layers: # [cite: 24]\n         if layer.name.startswith(\"efficientnet\"): # Find the actual base model layer by name [cite: 25]\n            base_model_layer = layer # [cite: 25]\n            break # [cite: 25]\n    if base_model_layer is None and fine_tune: # [cite: 25]\n        print(\"Warning: Could not automatically find base model layer for fine-tuning.\") # [cite: 25]\n\n    # --- Compile Step ---\n    if fine_tune: # [cite: 25]\n        print(f\"Setting up fine-tuning for {model.name}...\") # [cite: 25]\n        if base_model_layer: # [cite: 25]\n             base_model_layer.trainable = True # Unfreeze the base model layer [cite: 26]\n             # Fine-tune only the top 'fine_tune_layers' layers within the base model\n             num_base_layers = len(base_model_layer.layers) # [cite: 26]\n             freeze_until = num_base_layers - fine_tune_layers # [cite: 26]\n             print(f\"Unfreezing top {fine_tune_layers} layers of {base_model_layer.name} (out of {num_base_layers}). Freezing up to layer {freeze_until}.\") # [cite: 27]\n             if freeze_until < 0: freeze_until = 0 # [cite: 27]\n\n             for layer in base_model_layer.layers[:freeze_until]: # [cite: 27]\n                 # Keep Batch Norm layers frozen\n                 if isinstance(layer, tf.keras.layers.BatchNormalization): # [cite: 28]\n                      layer.trainable = False # [cite: 28]\n                 else:\n                      layer.trainable = False # [cite: 29]\n\n             for layer in base_model_layer.layers[freeze_until:]: # [cite: 29]\n                 # Keep Batch Norm frozen\n                 if isinstance(layer, tf.keras.layers.BatchNormalization): # [cite: 30]\n                     layer.trainable = False # [cite: 30]\n                 else:\n                     layer.trainable = True # Unfreeze the top layers [cite: 30]\n        else: # Fallback if base model layer not found # [cite: 30]\n             model.trainable = True # Unfreeze everything if base layer not identified [cite: 31]\n\n        learning_rate_schedule = learning_rate # Use starting LR for fine-tune [cite: 31]\n        callbacks.append(reduce_lr) # [cite: 31]\n        # --- CHANGE: AdamW with weight decay ---\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule, weight_decay=CONFIG[\"WEIGHT_DECAY\"]) # [cite: 31]\n    else: # Head training # [cite: 31]\n        print(f\"Setting up head training for {model.name}.\") # [cite: 31]\n        if base_model_layer: # [cite: 31]\n            base_model_layer.trainable = False # Ensure base is frozen # [cite: 31]\n        else:\n             print(\"Warning: Could not find base model layer to freeze for head training.\") # [cite: 32]\n\n        # Cosine Decay for head training\n        train_cardinality = tf.data.experimental.cardinality(train_dataset) # [cite: 32]\n        if train_cardinality == tf.data.experimental.UNKNOWN_CARDINALITY: # [cite: 32]\n            print(\"Warning: Unknown training steps for CosineDecay. Estimating.\") # [cite: 33]\n            try: # Estimate steps # [cite: 33]\n                 # This estimate might be inaccurate if dataset not fully listed/cached\n                 steps_per_epoch = math.ceil(num_batches / CONFIG[\"BATCH_SIZE\"]) # Use num_batches if known\n            except: steps_per_epoch = 1000 # Fallback # [cite: 33]\n            total_steps = steps_per_epoch * epochs # [cite: 33]\n        else:\n            total_steps = train_cardinality.numpy() * epochs # [cite: 33]\n\n        warmup_steps = int(total_steps * 0.1) # [cite: 34]\n        learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay( # [cite: 34]\n             initial_learning_rate=learning_rate, decay_steps=max(1, total_steps - warmup_steps), alpha=0.0 # [cite: 34]\n        )\n        # --- CHANGE: AdamW with weight decay ---\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule, weight_decay=CONFIG[\"WEIGHT_DECAY\"]) # [cite: 34]\n\n    # --- CHANGE: Use Label Smoothing in Loss ---\n    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG[\"LABEL_SMOOTHING\"]) # [cite: 34]\n\n    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy']) # [cite: 34]\n    print(\"\\n--- Model Summary After Compilation ---\") # [cite: 34]\n    model.summary(line_length=120) # Print summary after compilation and setting trainability # [cite: 34]\n\n    print(f\"\\n--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} (Epochs {initial_epoch} to {initial_epoch+epochs}) ---\") # [cite: 35]\n    history = model.fit( # [cite: 35]\n        train_dataset,\n        epochs=initial_epoch + epochs, # End epoch # [cite: 35]\n        validation_data=validation_dataset, # [cite: 35]\n        class_weight=class_weights_dict, # [cite: 35]\n        callbacks=callbacks, # [cite: 35]\n        initial_epoch=initial_epoch, # Start epoch # [cite: 35]\n        verbose=1 # [cite: 35]\n    )\n    return history, model # [cite: 35]\n\n\n# =============================================================================\n# Evaluation Function (Same as V3.2)\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names_list, dataset_name): # [cite: 36]\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    if test_dataset is None: # [cite: 36]\n        print(f\"Skipping evaluation on {dataset_name}: Dataset not loaded.\") # [cite: 36]\n        return None # [cite: 36]\n\n    print(f\"\\n--- Evaluating {model.name} on {dataset_name} ---\") # [cite: 36]\n    results = model.evaluate(test_dataset, verbose=1) # [cite: 36]\n    print(f\"{model.name} {dataset_name} Test Loss: {results[0]:.4f}\") # [cite: 36]\n    print(f\"{model.name} {dataset_name} Test Accuracy: {results[1]:.4f}\") # [cite: 36]\n\n    y_pred_probs = model.predict(test_dataset) # [cite: 36]\n    y_pred = np.argmax(y_pred_probs, axis=1) # [cite: 36]\n\n    y_true = [] # [cite: 36]\n    for _, labels_batch in test_dataset: # [cite: 37]\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 37]\n    y_true = np.array(y_true) # [cite: 37]\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int) # [cite: 37]\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)] # [cite: 37]\n\n    if not present_class_names: # [cite: 37]\n         print(\"Warning: No predictable classes found in evaluation data.\") # [cite: 37]\n         return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": 0} # [cite: 37]\n\n\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0) # [cite: 37]\n    print(f\"{model.name} {dataset_name} F1 Score (Weighted): {f1:.4f}\") # [cite: 37]\n    print(f\"{model.name} {dataset_name} Classification Report:\") # [cite: 38]\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0)) # [cite: 38]\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data) # [cite: 38]\n    plt.figure(figsize=(10, 8)) # [cite: 38]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_class_names, yticklabels=present_class_names) # [cite: 38]\n    plt.title(f'{model.name} {dataset_name} Confusion Matrix') # [cite: 38]\n    plt.ylabel('Actual') # [cite: 38]\n    plt.xlabel('Predicted') # [cite: 38]\n    cm_filename = f\"{model.name}_{dataset_name}_confusion_matrix.png\" # [cite: 38]\n    plt.savefig(cm_filename) # [cite: 38]\n    print(f\"Saved confusion matrix to {cm_filename}\") # [cite: 38]\n    plt.close() # [cite: 38]\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1} # [cite: 38]\n\n# =============================================================================\n# Main Training Pipeline\n# =============================================================================\n\nprint(f\"\\nClass names being used: {class_names}\") # [cite: 38]\nprint(f\"Number of classes for models: {CONFIG['NUM_CLASSES']}\") # [cite: 39]\nif len(class_names) != CONFIG['NUM_CLASSES']: # [cite: 39]\n     print(f\"Warning: Number of classes found ({len(class_names)}) differs from CONFIG['NUM_CLASSES'] ({CONFIG['NUM_CLASSES']})\") # [cite: 39]\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\") # [cite: 39]\nhistory1_head, model1 = train_model( # [cite: 39]\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0 # [cite: 39]\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\") # [cite: 39]\nhistory2_head, model2 = train_model( # [cite: 39]\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0 # [cite: 39]\n)\n\n# Load best weights from head training before fine-tuning\nprint(\"\\n--- Loading best weights after head training ---\") # [cite: 39]\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best head weights for {model1.name}\") # [cite: 40]\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model1.name} - {e}.\") # [cite: 40]\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best head weights for {model2.name}\") # [cite: 41]\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model2.name} - {e}.\") # [cite: 41]\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\") # [cite: 41]\nhistory1_ft, model1 = train_model( # [cite: 41]\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase # [cite: 41]\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"], # [cite: 41]\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"], initial_epoch=0 # Start fine-tune epochs at 0 # [cite: 41]\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\") # [cite: 41]\nhistory2_ft, model2 = train_model( # [cite: 41]\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase # [cite: 42]\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"], # [cite: 42]\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"], initial_epoch=0 # Start fine-tune epochs at 0 # [cite: 42]\n)\n\n# Load best weights saved during the entire process\nprint(\"\\n--- Loading potentially best fine-tuned weights ---\") # [cite: 42]\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best weights for {model1.name}\") # [cite: 43]\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model1.name} - {e}\") # [cite: 43]\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best weights for {model2.name}\") # [cite: 44]\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model2.name} - {e}\") # [cite: 44]\n\n# =============================================================================\n# --- NEW: Ensemble Weight Optimization ---\n# =============================================================================\nprint(\"\\n--- Optimizing Ensemble Weights on Validation Set ---\")\n\n# Get predictions from both models on the validation set\nprint(\"Predicting on validation set with Model 1...\")\nval_preds1 = model1.predict(val_ds)\nprint(\"Predicting on validation set with Model 2...\")\nval_preds2 = model2.predict(val_ds)\n\n# Get true labels from the validation set\nprint(\"Extracting true labels from validation set...\")\ny_val_true = []\nfor _, labels_batch in val_ds: # Assuming val_ds is repeatable or cached\n    y_val_true.extend(np.argmax(labels_batch.numpy(), axis=1))\ny_val_true = np.array(y_val_true)\n\n# Ensure predictions match the number of true labels\nif len(val_preds1) != len(y_val_true) or len(val_preds2) != len(y_val_true):\n    print(f\"Warning: Validation prediction count ({len(val_preds1)}, {len(val_preds2)}) does not match true label count ({len(y_val_true)}). Skipping weight optimization.\")\n    optimal_weights = CONFIG[\"ENSEMBLE_WEIGHTS\"] # Fallback to config weights\nelse:\n    best_val_accuracy = -1\n    optimal_weights = CONFIG[\"ENSEMBLE_WEIGHTS\"] # Default\n\n    # Grid search for best weight for model1 (model2 weight = 1 - w1)\n    print(\"Searching for optimal ensemble weights...\")\n    for w1 in np.arange(0.0, 1.05, 0.05): # Search in 0.05 increments\n        w2 = 1.0 - w1\n        weighted_preds = w1 * val_preds1 + w2 * val_preds2\n        y_val_pred = np.argmax(weighted_preds, axis=1)\n        current_accuracy = accuracy_score(y_val_true, y_val_pred)\n        print(f\"  Weight [M1={w1:.2f}, M2={w2:.2f}] -> Validation Accuracy: {current_accuracy:.4f}\")\n        if current_accuracy > best_val_accuracy:\n            best_val_accuracy = current_accuracy\n            optimal_weights = [w1, w2]\n\nprint(f\"Optimal weights found: {optimal_weights} with Validation Accuracy: {best_val_accuracy:.4f}\")\n# You can override CONFIG or just use optimal_weights in evaluation\n# CONFIG[\"ENSEMBLE_WEIGHTS\"] = optimal_weights # Optional: Update config dictionary\n\n# =============================================================================\n# Build and Save the Ensemble Model Object (Using Average layer for simplicity)\n# NOTE: This saved model uses EQUAL weights. Optimized weights are applied during evaluation below.\n# =============================================================================\nprint(\"\\n--- Building Saveable Ensemble Model (Equal Weights in Saved Object) ---\") # [cite: 44]\nmodel1.trainable = False # [cite: 44]\nmodel2.trainable = False # [cite: 44]\ninput_shape = (CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], 3) # [cite: 45]\nensemble_input = tf.keras.layers.Input(shape=input_shape, name=\"ensemble_input\") # [cite: 45]\noutput1 = model1(ensemble_input) # [cite: 45]\noutput2 = model2(ensemble_input) # [cite: 45]\n# The saved model uses simple average. Optimized weights are applied at evaluation time.\nensemble_output = tf.keras.layers.Average(name=\"ensemble_average\")([output1, output2]) # [cite: 45]\nensemble_model = tf.keras.Model(inputs=ensemble_input, outputs=ensemble_output, name=\"Emotion_Ensemble_B0_B4\") # [cite: 45]\nensemble_model.summary() # [cite: 45]\nensemble_model_path = \"emotion_ensemble_final.keras\" # [cite: 45]\nensemble_model.save(ensemble_model_path) # [cite: 45]\nprint(f\"Ensemble model object saved successfully to {ensemble_model_path}\") # [cite: 45]\n\n# =============================================================================\n# Individual Model Evaluation (Same as V3.2)\n# =============================================================================\nprint(\"\\n=== Evaluating Individual Models ===\") # [cite: 46]\nmetrics_m1_affect = evaluate_model(model1, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 46]\nmetrics_m1_fer = evaluate_model(model1, fer2013_test_ds, class_names, \"FER2013\") # [cite: 46]\nmetrics_m2_affect = evaluate_model(model2, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 46]\nmetrics_m2_fer = evaluate_model(model2, fer2013_test_ds, class_names, \"FER2013\") # [cite: 46]\n\n# =============================================================================\n# Ensemble Prediction and Evaluation (Using OPTIMIZED weights)\n# =============================================================================\nmodels_to_ensemble = [model1, model2] # [cite: 46]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names_list, dataset_name): # [cite: 46]\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    if test_dataset is None: # [cite: 46]\n        print(f\"Skipping ensemble evaluation on {dataset_name}: Dataset not loaded.\") # [cite: 46]\n        return None # [cite: 46]\n\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} using Weights: {weights} ---\") # Modified print # [cite: 47]\n    all_preds_probs = [] # [cite: 47]\n    for i, model in enumerate(models): # [cite: 47]\n        print(f\"Predicting with model {i+1}: {model.name}\") # [cite: 47]\n        preds = model.predict(test_dataset) # [cite: 47]\n        all_preds_probs.append(preds) # [cite: 47]\n\n    # Weighted average of probabilities using the provided (optimized) weights\n    weighted_preds_probs = np.tensordot(weights, all_preds_probs, axes=([0],[0])) # [cite: 47]\n    y_pred = np.argmax(weighted_preds_probs, axis=1) # [cite: 47]\n\n    # Extract true labels\n    y_true = [] # [cite: 47]\n    for _, labels_batch in test_dataset: # [cite: 47]\n         y_true.extend(np.argmax(labels_batch.numpy(), axis=1)) # [cite: 48]\n    y_true = np.array(y_true) # [cite: 48]\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int) # [cite: 48]\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)] # [cite: 48]\n\n    if not present_class_names: # [cite: 48]\n         print(\"Warning: No predictable classes found in ensemble evaluation data.\") # [cite: 48]\n         return {\"accuracy\": 0, \"f1_score\": 0} # [cite: 48]\n\n    accuracy = np.mean(y_true == y_pred) # [cite: 48]\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0) # [cite: 48]\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\") # [cite: 48]\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\") # [cite: 49]\n    print(f\"Ensemble {dataset_name} Classification Report:\") # [cite: 49]\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0)) # [cite: 49]\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data) # [cite: 49]\n    plt.figure(figsize=(10, 8)) # [cite: 49]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=present_class_names, yticklabels=present_class_names) # [cite: 49]\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix') # [cite: 49]\n    plt.ylabel('Actual') # [cite: 49]\n    plt.xlabel('Predicted') # [cite: 49]\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix_w{weights[0]:.2f}_{weights[1]:.2f}.png\" # Add weights to filename\n    plt.savefig(cm_filename) # [cite: 49]\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\") # [cite: 49]\n    plt.close() # [cite: 49]\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1} # [cite: 49]\n\n# --- Evaluate Ensemble ---\nprint(\"\\n=== Evaluating Ensemble (Using Optimized Weights) ===\") # [cite: 49]\n# --- CHANGE: Use optimal_weights found previously ---\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, optimal_weights, affectnet_test_ds, class_names, \"AffectNet\") # [cite: 49]\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, optimal_weights, fer2013_test_ds, class_names, \"FER2013\") # [cite: 50]\n\n# =============================================================================\n# Final Results Printout (Same as V3.2)\n# =============================================================================\nprint(\"\\n=== FINAL RESULTS ===\") # [cite: 50]\nprint(f\"(Using Optimized Ensemble Weights: {optimal_weights})\") # Added note\nprint(\"\\n--- Individual Model Performance ---\") # [cite: 50]\nif metrics_m1_affect: print(f\"{model1.name} AffectNet Test Accuracy: {metrics_m1_affect['accuracy']:.4f}, F1: {metrics_m1_affect['f1_score']:.4f}\") # [cite: 50]\nif metrics_m1_fer: print(f\"{model1.name} FER2013 Test Accuracy: {metrics_m1_fer['accuracy']:.4f}, F1: {metrics_m1_fer['f1_score']:.4f}\") # [cite: 50]\nif metrics_m2_affect: print(f\"{model2.name} AffectNet Test Accuracy: {metrics_m2_affect['accuracy']:.4f}, F1: {metrics_m2_affect['f1_score']:.4f}\") # [cite: 50]\nif metrics_m2_fer: print(f\"{model2.name} FER2013 Test Accuracy: {metrics_m2_fer['accuracy']:.4f}, F1: {metrics_m2_fer['f1_score']:.4f}\") # [cite: 50]\n\nprint(\"\\n--- Ensemble Performance ---\") # [cite: 50]\nif affectnet_metrics_ens: # [cite: 50]\n    print(f\"Ensemble AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\") # [cite: 50]\n    print(f\"Ensemble AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\") # [cite: 50]\nif fer_metrics_ens: # [cite: 50]\n    print(f\"Ensemble FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\") # [cite: 50]\n    print(f\"Ensemble FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\") # [cite: 50]\n\nprint(\"\\nTraining and evaluation complete.\") # [cite: 50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:32:33.758995Z","iopub.execute_input":"2025-04-22T07:32:33.759480Z","execution_failed":"2025-04-22T13:04:50.315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Revised Code based on NEW_CODEx.txt and recommendations\n# Version 3.2 (Handling specific Test directory structure)\n#=== FINAL RESULTS ===\n\n#--- Individual Model Performance ---\n#EfficientNetB0 AffectNet Test Accuracy: 0.6136, F1: 0.6056\n#EfficientNetB0 FER2013 Test Accuracy: 0.5346, F1: 0.5247\n#EfficientNetB4 AffectNet Test Accuracy: 0.8172, F1: 0.8172\n#EfficientNetB4 FER2013 Test Accuracy: 0.5524, F1: 0.5543\n\n#--- Ensemble Performance ---\n#Ensemble AffectNet Test Accuracy: 0.7515\n#Ensemble AffectNet F1 Score: 0.7509\n#Ensemble FER2013 Test Accuracy: 0.5780\n#Ensemble FER2013 F1 Score: 0.5721\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\nimport math\nimport glob # Needed for finding files\n\n# =============================================================================\n# Configuration Dictionary\n# =============================================================================\nCONFIG = {\n    \"SEED\": 42,\n    \"BASE_DATA_DIR\": \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\", # Base path\n    \"TRAIN_DIR\": \"Train\", # Combined train dir name\n    \"TEST_DIR\": \"Test\",   # Base test dir name\n    \"IMG_SIZE\": 112, # 112= accuracy 0.56 / 224= accuracy 0.49 / 380=x / 336=x (160px, 192px, or 256px)\n    \"BATCH_SIZE\": 32, # 64+ / 128x / 32\n    \"BUFFER_SIZE\": tf.data.AUTOTUNE,\n    \"EPOCHS_HEAD\": 50, # 15\n    \"EPOCHS_FINE_TUNE\": 100, # 30 60+\n    \"LR_HEAD\": 1e-3,\n    \"LR_FINE_TUNE_START\": 1e-4,\n    \"DROPOUT_RATE\": 0.4,\n    \"NUM_CLASSES\": 8,\n    \"MODEL_ARCH_1\": \"EfficientNetB0\",\n    \"MODEL_ARCH_2\": \"EfficientNetB4\",\n    \"ENSEMBLE_WEIGHTS\": [0.3, 0.7], # 50/50 80/20 -- 70/30\n    \"FINE_TUNE_LAYERS_B0\": 15, # 10\n    \"FINE_TUNE_LAYERS_B4\": 20, # 15\n    \"LOG_DIR_BASE\": \"logs/fit/\"\n}\n\n# Set Seed\ntf.random.set_seed(CONFIG[\"SEED\"])\nnp.random.seed(CONFIG[\"SEED\"])\n\n# =============================================================================\n# GPU Configuration & Mixed Precision (Same as before)\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\nelse:\n    print(\"No GPU detected. Running on CPU.\")\n\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\nprint(\"Mixed precision enabled ('mixed_float16')\")\n\n# =============================================================================\n# Data Loading (Train/Validation - Standard; Test - Custom)\n# =============================================================================\n\ndef create_train_val_tf_dataset(directory, image_size, batch_size, validation_split=0.2):\n    \"\"\"Creates tf.data.Dataset for training and validation using image_dataset_from_directory.\"\"\"\n    print(f\"Loading training/validation data from: {directory}\")\n\n    # Create the training dataset\n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=True,\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"training\",\n    )\n\n    # Create the validation dataset\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=(image_size, image_size),\n        interpolation='nearest',\n        batch_size=batch_size,\n        shuffle=False, # No need to shuffle validation\n        seed=CONFIG[\"SEED\"],\n        validation_split=validation_split,\n        subset=\"validation\",\n    )\n\n    # Get class names from the training dataset BEFORE optimization\n    class_names = train_ds.class_names\n    print(f\"Dataset loaded with classes: {class_names}\")\n\n    # Configure performance for both datasets\n    train_ds = train_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n    val_ds = val_ds.cache().prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return train_ds, val_ds, class_names\n\n# --- NEW: Function to load test data from the specific structure ---\ndef create_test_dataset_from_structure(base_test_dir, target_dataset, class_names_map, image_size, batch_size):\n    \"\"\"\n    Creates a tf.data.Dataset for testing by manually finding files in the Test/<emotion>/<target_dataset> structure.\n    \"\"\"\n    print(f\"Loading test data for '{target_dataset}' from: {base_test_dir}\")\n    all_image_paths = []\n    all_labels = []\n\n    # Get emotion directories (e.g., 'anger', 'happy')\n    emotion_dirs = [d for d in tf.io.gfile.listdir(base_test_dir) if tf.io.gfile.isdir(os.path.join(base_test_dir, d))]\n    if not emotion_dirs:\n         print(f\"Warning: No subdirectories found in {base_test_dir}. Cannot load test data.\")\n         return None\n\n    print(f\"Found emotion folders: {emotion_dirs}\")\n\n    for emotion in emotion_dirs:\n        if emotion not in class_names_map:\n            print(f\"Warning: Emotion directory '{emotion}' not found in training class names map. Skipping.\")\n            continue\n\n        label_index = class_names_map[emotion] # Get the integer label\n        target_path = os.path.join(base_test_dir, emotion, target_dataset)\n\n        if not tf.io.gfile.exists(target_path):\n             print(f\"Info: Sub-directory '{target_path}' does not exist. Skipping.\")\n             continue # Skip if the specific dataset subdir doesn't exist for this emotion\n\n        # Find all image files (adjust extensions if needed)\n        image_files = tf.io.gfile.glob(os.path.join(target_path, '*.png')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpg')) + \\\n                      tf.io.gfile.glob(os.path.join(target_path, '*.jpeg'))\n\n        if not image_files:\n             print(f\"Warning: No image files found in '{target_path}'.\")\n             continue\n\n        all_image_paths.extend(image_files)\n        all_labels.extend([label_index] * len(image_files))\n        print(f\"  Found {len(image_files)} images for emotion '{emotion}' in '{target_dataset}'.\")\n\n    if not all_image_paths:\n        print(f\"Error: No images found for target dataset '{target_dataset}' in the specified structure.\")\n        return None\n\n    print(f\"Total images found for '{target_dataset}' test set: {len(all_image_paths)}\")\n\n    # Create the dataset from slices\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    label_ds = tf.data.Dataset.from_tensor_slices(tf.one_hot(all_labels, depth=CONFIG[\"NUM_CLASSES\"])) # Convert labels to one-hot\n    image_ds = path_ds.map(lambda x: load_and_preprocess_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Combine images and labels\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n\n    # Batch and prefetch\n    test_ds = image_label_ds.batch(batch_size).prefetch(buffer_size=CONFIG[\"BUFFER_SIZE\"])\n\n    return test_ds\n\n# --- NEW: Helper function to load and preprocess images from paths ---\ndef load_and_preprocess_image(path, image_size):\n    \"\"\"Loads and preprocesses a single image file.\"\"\"\n    image = tf.io.read_file(path)\n    image = tf.image.decode_image(image, channels=3, expand_animations=False) # Decode any format\n    image = tf.image.resize(image, [image_size, image_size], method='nearest')\n    image.set_shape((image_size, image_size, 3))\n    # Rescaling: EfficientNet generally handles this internally, but if needed:\n    # image = image / 255.0\n    return image\n\n# --- Create Datasets ---\n# Training and Validation Data\ntrain_dir = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TRAIN_DIR\"])\ntrain_ds, val_ds, class_names = create_train_val_tf_dataset(\n    train_dir, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"]\n)\n\n# Create a mapping from class name to integer index for test set loading\nclass_names_map = {name: i for i, name in enumerate(class_names)}\n\n# Test Datasets (using the new custom function)\nbase_test_dir_path = os.path.join(CONFIG[\"BASE_DATA_DIR\"], CONFIG[\"TEST_DIR\"])\naffectnet_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"affectnet\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\nfer2013_test_ds = create_test_dataset_from_structure(base_test_dir_path, \"fer2013\", class_names_map, CONFIG[\"IMG_SIZE\"], CONFIG[\"BATCH_SIZE\"])\n\n\n# =============================================================================\n# Class Weights Calculation (Same as V3.1, check if needs adjustment for 8 classes)\n# =============================================================================\ndef get_class_weights(dataset, class_names_list):\n    print(\"Calculating class weights...\")\n    all_labels = []\n    num_batches = tf.data.experimental.cardinality(dataset)\n    print(f\"Approximate number of batches in training dataset: {num_batches}\")\n\n    if num_batches == tf.data.experimental.UNKNOWN_CARDINALITY or num_batches == tf.data.experimental.INFINITE_CARDINALITY:\n         print(\"Warning: Cannot determine dataset cardinality accurately. Iterating...\")\n         for _, labels_batch in dataset: # Iterate batches\n              all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n         if not all_labels:\n             print(\"Error: Could not extract labels. Using uniform weights.\")\n             return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    else:\n        for _, labels_batch in dataset:\n            all_labels.extend(np.argmax(labels_batch.numpy(), axis=1))\n\n    unique_classes, counts = np.unique(all_labels, return_counts=True)\n    print(f\"Unique labels found for weight calculation: {unique_classes} with counts {counts}\")\n\n    if len(unique_classes) == 0:\n        print(\"Error: No labels found. Using uniform weights.\")\n        return {i: 1.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=unique_classes,\n        y=all_labels\n    )\n\n    class_weights_dict = {i: 0.0 for i in range(CONFIG[\"NUM_CLASSES\"])}\n    for i, cls_label in enumerate(unique_classes):\n        if cls_label < CONFIG[\"NUM_CLASSES\"]:\n             class_weights_dict[cls_label] = class_weights[i]\n        else:\n             print(f\"Warning: Label {cls_label} >= NUM_CLASSES ({CONFIG['NUM_CLASSES']}). Ignoring.\")\n\n    for i in range(CONFIG[\"NUM_CLASSES\"]):\n        if class_weights_dict[i] == 0.0 and i in unique_classes: # Check if it was actually missing or just had 0 weight\n            print(f\"Warning: Class {i} ({class_names_list[i]}) had 0 weight initially. Assigning 1.0.\")\n            class_weights_dict[i] = 1.0 # Avoid 0 weight if class exists but wasn't found / calculated\n        elif class_weights_dict[i] == 0.0:\n             print(f\"Info: Class {i} ({class_names_list[i]}) not found in iterated samples. Assigning weight 1.0.\")\n             class_weights_dict[i] = 1.0\n\n\n    print(\"Class weights calculated:\", class_weights_dict)\n    return class_weights_dict\n\nclass_weights = get_class_weights(train_ds, class_names)\n\n# =============================================================================\n# Data Augmentation Layer (Same as before)\n# =============================================================================\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\", seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomRotation(0.1, seed=CONFIG[\"SEED\"]),\n    tf.keras.layers.RandomZoom(0.1, seed=CONFIG[\"SEED\"]),\n], name=\"data_augmentation\")\n\n# =============================================================================\n# Model Building (Same as before - ensure NUM_CLASSES is correct)\n# =============================================================================\ndef build_model(model_arch, num_classes, img_size, dropout_rate):\n    input_shape = (img_size, img_size, 3)\n    inputs = Input(shape=input_shape, name=\"input_layer\")\n    x = data_augmentation(inputs) # Apply augmentation first\n\n    if model_arch == \"EfficientNetB0\":\n        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg') # Let EffNet handle input scaling\n        # Apply augmentation *after* potential base model preprocessing if needed, but usually before is fine.\n        x_processed = base_model(x, training=False) # Pass augmented data to base model\n    elif model_arch == \"EfficientNetB4\":\n         print(f\"Warning: Building {model_arch} with image size {img_size}.\")\n         base_model = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg')\n         x_processed = base_model(x, training=False) # Pass augmented data to base model\n    else:\n        raise ValueError(f\"Unsupported model architecture: {model_arch}\")\n\n    base_model.trainable = False # Freeze base model\n\n    # Add classification head\n    output = Dropout(dropout_rate, name=\"top_dropout\")(x_processed) # Use output from base model\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\", dtype='float32')(output)\n\n    model = Model(inputs=inputs, outputs=outputs, name=model_arch)\n    print(f\"{model_arch} model built successfully.\")\n    return model\n\n# --- Build individual models ---\nmodel1 = build_model(CONFIG[\"MODEL_ARCH_1\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\nmodel2 = build_model(CONFIG[\"MODEL_ARCH_2\"], CONFIG[\"NUM_CLASSES\"], CONFIG[\"IMG_SIZE\"], CONFIG[\"DROPOUT_RATE\"])\n\n# =============================================================================\n# Training Functions (Mostly same as V3.1, adjust fine-tune layer freezing logic)\n# =============================================================================\ndef train_model(model, train_dataset, validation_dataset, class_weights_dict, epochs, learning_rate, fine_tune=False, fine_tune_layers=0, initial_epoch=0):\n    \"\"\"Compiles and trains a single model.\"\"\"\n    log_dir = CONFIG[\"LOG_DIR_BASE\"] + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + model.name\n    checkpoint_path = f\"best_{model.name}.keras\"\n\n    # --- Callbacks ---\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=1)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=9, min_lr=1e-6, verbose=1) # patience=3\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=18, restore_best_weights=True, verbose=1) # patience=7\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    callbacks = [model_checkpoint, early_stopping, tensorboard_callback]\n    optimizer_choice = tf.keras.optimizers.Adam\n\n    # Find the base model layer to control its trainability\n    base_model_layer = None\n    for layer in model.layers:\n        if layer.name.startswith(\"efficientnet\"): # Find the actual base model layer by name\n            base_model_layer = layer\n            break\n    if base_model_layer is None and fine_tune:\n        print(\"Warning: Could not automatically find base model layer for fine-tuning.\")\n\n    # --- Compile Step ---\n    if fine_tune:\n        print(f\"Setting up fine-tuning for {model.name}...\")\n        if base_model_layer:\n            base_model_layer.trainable = True # Unfreeze the base model layer\n            # Fine-tune only the top 'fine_tune_layers' layers within the base model\n            num_base_layers = len(base_model_layer.layers)\n            freeze_until = num_base_layers - fine_tune_layers\n            print(f\"Unfreezing top {fine_tune_layers} layers of {base_model_layer.name} (out of {num_base_layers}). Freezing up to layer {freeze_until}.\")\n            if freeze_until < 0: freeze_until = 0\n\n            for layer in base_model_layer.layers[:freeze_until]:\n                 # Keep Batch Norm layers frozen, as is often recommended during fine-tuning\n                 # to prevent destabilization from small batch statistics.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                      layer.trainable = False\n                 else:\n                      # You might choose to freeze all layers up to this point\n                      layer.trainable = False\n                      pass # Or set layer.trainable = False if you want to be strict\n\n            for layer in base_model_layer.layers[freeze_until:]:\n                 # Keep Batch Norm frozen here too for consistency? Experiment needed.\n                 if isinstance(layer, tf.keras.layers.BatchNormalization):\n                     layer.trainable = False\n                 else:\n                     layer.trainable = True # Unfreeze the top layers\n        else: # Fallback if base model layer not found\n             model.trainable = True # Unfreeze everything if base layer not identified\n\n        learning_rate_schedule = learning_rate # Use starting LR for fine-tune\n        callbacks.append(reduce_lr)\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n    else: # Head training\n        print(f\"Setting up head training for {model.name}.\")\n        if base_model_layer:\n            base_model_layer.trainable = False # Ensure base is frozen\n        else:\n             print(\"Warning: Could not find base model layer to freeze for head training.\")\n\n        # Cosine Decay for head training\n        train_cardinality = tf.data.experimental.cardinality(train_dataset)\n        if train_cardinality == tf.data.experimental.UNKNOWN_CARDINALITY:\n            print(\"Warning: Unknown training steps for CosineDecay. Estimating.\")\n            try: # Estimate steps\n                 steps_per_epoch = len(train_ds.list_files(os.path.join(train_dir,'*/*'))) // CONFIG[\"BATCH_SIZE\"]\n            except: steps_per_epoch = 1000 # Fallback\n            total_steps = steps_per_epoch * epochs\n        else:\n            total_steps = train_cardinality.numpy() * epochs\n\n        warmup_steps = int(total_steps * 0.1)\n        learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay(\n             initial_learning_rate=learning_rate, decay_steps=max(1, total_steps - warmup_steps), alpha=0.0\n        )\n        optimizer = optimizer_choice(learning_rate=learning_rate_schedule)\n        # callbacks.append(reduce_lr) # Optionally add ReduceLR here too\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    print(\"\\n--- Model Summary After Compilation ---\")\n    model.summary(line_length=120) # Print summary after compilation and setting trainability\n\n    print(f\"\\n--- Starting {'Fine-tuning' if fine_tune else 'Head Training'} for {model.name} (Epochs {initial_epoch} to {initial_epoch+epochs}) ---\")\n    history = model.fit(\n        train_dataset,\n        epochs=initial_epoch + epochs, # End epoch\n        validation_data=validation_dataset,\n        class_weight=class_weights_dict,\n        callbacks=callbacks,\n        initial_epoch=initial_epoch, # Start epoch\n        verbose=1\n    )\n    return history, model\n\n\n# =============================================================================\n# Evaluation Function (Adapting to potentially missing classes in test sets)\n# =============================================================================\ndef evaluate_model(model, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the model on a given test dataset.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating {model.name} on {dataset_name} ---\")\n    results = model.evaluate(test_dataset, verbose=1)\n    print(f\"{model.name} {dataset_name} Test Loss: {results[0]:.4f}\")\n    print(f\"{model.name} {dataset_name} Test Accuracy: {results[1]:.4f}\")\n\n    y_pred_probs = model.predict(test_dataset)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in evaluation data.\")\n         return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": 0}\n\n\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"{model.name} {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"{model.name} {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'{model.name} {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"{model.name}_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"loss\": results[0], \"accuracy\": results[1], \"f1_score\": f1}\n\n# =============================================================================\n# Main Training Pipeline (Adjusted epoch handling)\n# =============================================================================\n\nprint(f\"\\nClass names being used: {class_names}\")\nprint(f\"Number of classes for models: {CONFIG['NUM_CLASSES']}\")\nif len(class_names) != CONFIG['NUM_CLASSES']:\n     print(f\"Warning: Number of classes found ({len(class_names)}) differs from CONFIG['NUM_CLASSES'] ({CONFIG['NUM_CLASSES']})\")\n\nprint(\"\\n=== Phase 1: Training Head of Model 1 ===\")\nhistory1_head, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\nprint(\"\\n=== Phase 1: Training Head of Model 2 ===\")\nhistory2_head, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_HEAD\"], learning_rate=CONFIG[\"LR_HEAD\"], fine_tune=False, initial_epoch=0\n)\n\n# Load best weights from head training before fine-tuning\nprint(\"\\n--- Loading best weights after head training ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best head weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model1.name} - {e}.\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best head weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best head weights for {model2.name} - {e}.\")\n\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 1 ===\")\n# Start fine-tuning epochs from 0 for this phase, but total epochs define the duration\nhistory1_ft, model1 = train_model(\n    model1, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B0\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\nprint(\"\\n=== Phase 2: Fine-tuning Model 2 ===\")\nhistory2_ft, model2 = train_model(\n    model2, train_ds, val_ds, class_weights,\n    epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], # Number of epochs FOR this phase\n    learning_rate=CONFIG[\"LR_FINE_TUNE_START\"],\n    fine_tune=True, fine_tune_layers=CONFIG[\"FINE_TUNE_LAYERS_B4\"], initial_epoch=0 # Start fine-tune epochs at 0\n)\n\n# Load best weights saved during the entire process (should be the fine-tuned ones if they improved)\nprint(\"\\n--- Loading potentially best fine-tuned weights ---\")\ntry: model1.load_weights(f\"best_{model1.name}.keras\"); print(f\"Loaded best weights for {model1.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model1.name} - {e}\")\ntry: model2.load_weights(f\"best_{model2.name}.keras\"); print(f\"Loaded best weights for {model2.name}\")\nexcept Exception as e: print(f\"Warning: Could not load best fine-tuning weights for {model2.name} - {e}\")\n\n# =============================================================================\n# Build and Save the Ensemble Model Object\n# =============================================================================\nprint(\"\\n--- Building Saveable Ensemble Model ---\")\n\n# Ensure individual models are loaded with best weights (already done previously)\n# model1 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_1']}.keras\", compile=False) # Example if re-loading needed\n# model2 = tf.keras.models.load_model(f\"best_{CONFIG['MODEL_ARCH_2']}.keras\", compile=False)\n\n# Ensure base models within the loaded models are not trainable for the ensemble definition\n# (This prevents accidental further training if the ensemble object were compiled later)\n# Set the loaded models themselves as non-trainable before combining\nmodel1.trainable = False\nmodel2.trainable = False\n\n# Define the input layer (must match the input shape of individual models)\ninput_shape = (CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], 3)\nensemble_input = tf.keras.layers.Input(shape=input_shape, name=\"ensemble_input\")\n\n# Get the outputs from the individual models for the shared input\noutput1 = model1(ensemble_input)\noutput2 = model2(ensemble_input)\n\n# Combine the outputs using Average (simple average)\n# If you want weighted average, you might need a custom layer or use tf.tensordot outside the model definition\n# Or, if using TF >= 2.11 (approx), check for tf.keras.layers.WeightedAverage\nensemble_output = tf.keras.layers.Average(name=\"ensemble_average\")([output1, output2])\n\n# Create the ensemble model\nensemble_model = tf.keras.Model(inputs=ensemble_input, outputs=ensemble_output, name=\"Emotion_Ensemble_B0_B4\")\n\nensemble_model.summary()\n\n# Save the ensemble model\nensemble_model_path = \"emotion_ensemble_final.keras\"\nensemble_model.save(ensemble_model_path)\nprint(f\"Ensemble model object saved successfully to {ensemble_model_path}\")\n\n# Optional: You can now use 'ensemble_model' directly for predictions if needed immediately\n# Example: ensemble_model.predict(some_test_data)\n\n# =============================================================================\n# Individual Model Evaluation (Optional but recommended)\n# =============================================================================\nprint(\"\\n=== Evaluating Individual Models ===\")\nmetrics_m1_affect = evaluate_model(model1, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m1_fer = evaluate_model(model1, fer2013_test_ds, class_names, \"FER2013\")\nmetrics_m2_affect = evaluate_model(model2, affectnet_test_ds, class_names, \"AffectNet\")\nmetrics_m2_fer = evaluate_model(model2, fer2013_test_ds, class_names, \"FER2013\")\n\n# =============================================================================\n# Ensemble Prediction and Evaluation (Same as V3.1, ensure test datasets loaded)\n# =============================================================================\nmodels_to_ensemble = [model1, model2]\n\ndef evaluate_ensemble(models, weights, test_dataset, class_names_list, dataset_name):\n    \"\"\"Evaluates the ensemble using weighted averaging of predictions.\"\"\"\n    if test_dataset is None:\n        print(f\"Skipping ensemble evaluation on {dataset_name}: Dataset not loaded.\")\n        return None\n\n    print(f\"\\n--- Evaluating Ensemble on {dataset_name} ---\")\n    all_preds_probs = []\n    for i, model in enumerate(models):\n        print(f\"Predicting with model {i+1}: {model.name}\")\n        preds = model.predict(test_dataset)\n        all_preds_probs.append(preds)\n\n    # Weighted average of probabilities\n    weighted_preds_probs = np.tensordot(weights, all_preds_probs, axes=([0],[0]))\n    y_pred = np.argmax(weighted_preds_probs, axis=1)\n\n    # Extract true labels\n    y_true = []\n    for _, labels_batch in test_dataset:\n        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n    y_true = np.array(y_true)\n\n    unique_labels_in_data = np.unique(np.concatenate((y_true, y_pred))).astype(int)\n    present_class_names = [class_names_list[i] for i in unique_labels_in_data if i < len(class_names_list)]\n\n    if not present_class_names:\n         print(\"Warning: No predictable classes found in ensemble evaluation data.\")\n         return {\"accuracy\": 0, \"f1_score\": 0}\n\n    accuracy = np.mean(y_true == y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted', labels=unique_labels_in_data, zero_division=0)\n    print(f\"Ensemble {dataset_name} Test Accuracy: {accuracy:.4f}\")\n    print(f\"Ensemble {dataset_name} F1 Score (Weighted): {f1:.4f}\")\n    print(f\"Ensemble {dataset_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=present_class_names, labels=unique_labels_in_data, zero_division=0))\n\n    cm = confusion_matrix(y_true, y_pred, labels=unique_labels_in_data)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=present_class_names, yticklabels=present_class_names)\n    plt.title(f'Ensemble {dataset_name} Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    cm_filename = f\"ensemble_{dataset_name}_confusion_matrix.png\"\n    plt.savefig(cm_filename)\n    print(f\"Saved ensemble confusion matrix to {cm_filename}\")\n    plt.close()\n\n    return {\"accuracy\": accuracy, \"f1_score\": f1}\n\n# --- Evaluate Ensemble ---\nprint(\"\\n=== Evaluating Ensemble ===\")\naffectnet_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], affectnet_test_ds, class_names, \"AffectNet\")\nfer_metrics_ens = evaluate_ensemble(models_to_ensemble, CONFIG[\"ENSEMBLE_WEIGHTS\"], fer2013_test_ds, class_names, \"FER2013\")\n\nprint(\"\\n=== FINAL RESULTS ===\")\nprint(\"\\n--- Individual Model Performance ---\")\nif metrics_m1_affect: print(f\"{model1.name} AffectNet Test Accuracy: {metrics_m1_affect['accuracy']:.4f}, F1: {metrics_m1_affect['f1_score']:.4f}\")\nif metrics_m1_fer: print(f\"{model1.name} FER2013 Test Accuracy: {metrics_m1_fer['accuracy']:.4f}, F1: {metrics_m1_fer['f1_score']:.4f}\")\nif metrics_m2_affect: print(f\"{model2.name} AffectNet Test Accuracy: {metrics_m2_affect['accuracy']:.4f}, F1: {metrics_m2_affect['f1_score']:.4f}\")\nif metrics_m2_fer: print(f\"{model2.name} FER2013 Test Accuracy: {metrics_m2_fer['accuracy']:.4f}, F1: {metrics_m2_fer['f1_score']:.4f}\")\n\nprint(\"\\n--- Ensemble Performance ---\")\nif affectnet_metrics_ens:\n    print(f\"Ensemble AffectNet Test Accuracy: {affectnet_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble AffectNet F1 Score: {affectnet_metrics_ens['f1_score']:.4f}\")\nif fer_metrics_ens:\n    print(f\"Ensemble FER2013 Test Accuracy: {fer_metrics_ens['accuracy']:.4f}\")\n    print(f\"Ensemble FER2013 F1 Score: {fer_metrics_ens['f1_score']:.4f}\")\n\nprint(\"\\nTraining and evaluation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:02:58.286865Z","iopub.execute_input":"2025-04-05T14:02:58.287181Z","iopub.status.idle":"2025-04-05T18:23:25.138544Z","shell.execute_reply.started":"2025-04-05T14:02:58.287156Z","shell.execute_reply":"2025-04-05T18:23:25.137442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My code 2.5 (deepcloud) best one yet\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tensorflow.keras.applications import MobileNetV2, Xception, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Concatenate, Activation, Lambda, Input\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom datetime import datetime\n\n# =============================================================================\n# Configure GPU and enable mixed precision\n# =============================================================================\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs: Memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"GPU error: {e}\")\n\n# Enable mixed precision training\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled\")\n\n# =============================================================================\n# Key parameters (Adjusted for memory optimization and stability)\n# =============================================================================\nBATCH_SIZE = 64  # Maintain balance between memory and throughput\nAUTOTUNE = tf.data.AUTOTUNE\nLOG_DIR = \"./emotion_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n# Create main log directories\nensure_dir(LOG_DIR)\nensure_dir(LOG_DIR + '/affectnet')\nensure_dir(LOG_DIR + '/fer2013')\nensure_dir(LOG_DIR + '/combined')\nensure_dir(LOG_DIR + '/ensemble')\nensure_dir(\"./model_checkpoints\")\nensure_dir(\"./cache\")\nensure_dir(\"./weights\")\n\n# Extend problematic classes for more targeted augmentation\nPROBLEMATIC_CLASSES = ['surprise', 'sad', 'disgust', 'anger', 'fear']\n\n# =============================================================================\n# Enhanced Focal Loss for better handling of class imbalance\n# =============================================================================\ndef focal_loss(gamma=2.0, alpha=None):\n    \"\"\"\n    Focal loss implementation for better handling of class imbalance.\n    Focuses training on hard examples by down-weighting easy examples.\n    \n    Args:\n        gamma: Focusing parameter (higher = more focus on hard examples)\n        alpha: Optional class weight factors\n        \n    Returns:\n        Loss function\n    \"\"\"\n    def loss_fn(y_true, y_pred):\n        # Add small epsilon to avoid log(0)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n        \n        # Basic cross entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Apply class weighting if provided\n        if alpha is not None:\n            # Convert alpha to proper shape if it's a dict\n            if isinstance(alpha, dict):\n                # Create a tensor of appropriate shape filled with ones\n                alpha_tensor = tf.ones_like(y_true)\n                \n                # For each class index in the alpha dict, update the corresponding\n                # position in alpha_tensor with the weight value\n                for class_idx, weight in alpha.items():\n                    # Create a mask for the current class\n                    class_mask = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), class_idx), tf.float32)\n                    \n                    # Reshape to broadcast properly\n                    class_mask = tf.expand_dims(class_mask, axis=-1)\n                    \n                    # Update weights for this class\n                    alpha_tensor = alpha_tensor * (1 - class_mask) + weight * class_mask\n                \n                cross_entropy = alpha_tensor * cross_entropy\n            else:\n                cross_entropy = alpha * cross_entropy\n        \n        # Apply focusing parameter\n        focal_weight = tf.pow(1 - y_pred, gamma)\n        focal_loss = focal_weight * cross_entropy\n        \n        # Sum over classes\n        return tf.reduce_sum(focal_loss, axis=-1)\n    \n    return loss_fn\n\n# =============================================================================\n# Enhanced Image Preprocessing with Stronger Augmentation for Hard Classes\n# =============================================================================\ndef preprocess_image(file_path, label, source, training=True):\n    \"\"\"\n    Enhanced preprocessing with stronger augmentation for difficult classes.\n    \n    Args:\n        file_path: Path to the image file\n        label: Emotion label (as index)\n        source: Dataset source ('fer2013' or 'affectnet')\n        training: Whether to apply augmentation\n        \n    Returns:\n        Preprocessed image (consistent size) and one-hot encoded label\n    \"\"\"\n    # Read the file\n    img = tf.io.read_file(file_path)\n    \n    # Decode image with proper error handling\n    def decode_image():\n        try:\n            decoded = tf.image.decode_image(img, channels=3, expand_animations=False)\n            decoded = tf.ensure_shape(decoded, [None, None, 3])\n            return decoded\n        except:\n            # Return blank image if decoding fails\n            return tf.zeros([224, 224, 3], dtype=tf.uint8)\n    \n    img = decode_image()\n    img = tf.cast(img, tf.float32)\n    \n    # Dataset-specific preprocessing for grayscale/RGB\n    if source == 'fer2013':\n        # Properly handle grayscale images\n        if tf.shape(img)[-1] == 1:\n            img = tf.tile(img, [1, 1, 3])  # Expand to 3 channels\n        else:\n            # Convert to grayscale then back to 3 channels for consistency\n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.tile(img, [1, 1, 3])\n    \n    # Resize ALL images to a standard intermediate size\n    # 224x224 is chosen as it's compatible with EfficientNetB0\n    img = tf.image.resize(img, [224, 224], method='bilinear')\n    \n    # Apply enhanced augmentation during training\n    if training:\n        # Basic augmentations for all images\n        img = tf.image.random_flip_left_right(img)\n        \n        # Enhanced brightness and contrast adjustment\n        img = tf.image.random_brightness(img, 0.3)  # Increased from 0.2\n        img = tf.image.random_contrast(img, 0.7, 1.3)  # Wider range\n        \n        # Add random rotation (15 degrees)\n        angle = tf.random.uniform([], -0.261799, 0.261799)  # 15 degrees in radians\n        img = tf.image.rot90(img, k=tf.cast(angle / (np.pi/2) * 4, tf.int32))\n        \n        # Random zoom\n        zoom_factor = tf.random.uniform([], 0.8, 1.0)\n        h, w = tf.shape(img)[0], tf.shape(img)[1]\n        crop_size_h = tf.cast(tf.cast(h, tf.float32) * zoom_factor, tf.int32)\n        crop_size_w = tf.cast(tf.cast(w, tf.float32) * zoom_factor, tf.int32)\n        img = tf.image.random_crop(img, [crop_size_h, crop_size_w, 3])\n        img = tf.image.resize(img, [224, 224])\n        \n        # Add more aggressive noise\n        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.015)  # Increased from 0.01\n        img = img + noise\n        \n        # Ensure valid range\n        img = tf.clip_by_value(img, 0.0, 255.0)\n    \n    # Basic normalization to [0,1] range for consistency\n    img = img / 255.0\n    \n    # One-hot encode label\n    label = tf.one_hot(label, depth=8)  # 8 emotion classes\n    \n    return img, label\n\n# =============================================================================\n# Build DataFrame from dataset directory structure\n# =============================================================================\ndef build_image_df(root_dir, subfolders=[\"fer2013\", \"affectnet\"]):\n    \"\"\"\n    Scans the directory and returns a DataFrame with file paths and labels.\n    \"\"\"\n    data = []\n    emotions = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    print(f\"Found {len(emotions)} emotion categories: {emotions}\")\n    \n    for emotion in emotions:\n        emotion_path = os.path.join(root_dir, emotion)\n        for sub in subfolders:\n            sub_path = os.path.join(emotion_path, sub)\n            if os.path.exists(sub_path):\n                img_files = [f for f in os.listdir(sub_path) \n                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n                \n                print(f\"Found {len(img_files)} images in {emotion}/{sub}\")\n                \n                for img_file in img_files:\n                    data.append({\n                        \"filepath\": os.path.join(sub_path, img_file),\n                        \"label\": emotion,\n                        \"source\": sub\n                    })\n    \n    df = pd.DataFrame(data)\n    print(f\"Total images: {len(df)}\")\n    return df\n\n# =============================================================================\n# FIXED: Enhanced dataset creation with proper repeat mechanism\n# =============================================================================\ndef create_dataset(dataframe, is_training=True, dataset_type=None, cache=False):\n    \"\"\"Memory-optimized dataset creation with proper repeat mechanism\"\"\"\n    if dataset_type:\n        dataframe = dataframe[dataframe['source'] == dataset_type].reset_index(drop=True)\n    \n    class_indices = {cls: i for i, cls in enumerate(sorted(dataframe[\"label\"].unique()))}\n    labels = [class_indices[label] for label in dataframe[\"label\"]]\n    \n    ds = tf.data.Dataset.from_tensor_slices((\n        dataframe[\"filepath\"].values,\n        labels,\n        dataframe[\"source\"].values\n    ))\n    \n    # Memory optimization: Only cache validation/test datasets\n    if cache and not is_training:\n        cache_name = f'./cache/{dataset_type}_cache' if dataset_type else './cache/combined_cache'\n        ds = ds.cache(cache_name)\n    \n    training_value = tf.constant(is_training)\n    ds = ds.map(\n        lambda path, label, source: preprocess_image(path, label, source, training=training_value),\n        num_parallel_calls=AUTOTUNE\n    )\n    \n    if is_training:\n        # Better shuffling with larger buffer\n        buffer_size = min(len(dataframe), 20000)  # Increased from 10000\n        ds = ds.shuffle(buffer_size=buffer_size)\n        # IMPORTANT: Create an infinite dataset by repeating\n        ds = ds.repeat()\n    \n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    return ds, class_indices\n\n# =============================================================================\n# Create more aggressively balanced dataset with emphasis classes\n# =============================================================================\ndef create_emphasis_dataset(dataframe, is_training=True, emphasis_classes=PROBLEMATIC_CLASSES):\n    \"\"\"\n    Creates a more aggressively balanced dataset with emphasis on problematic classes.\n    \n    Args:\n        dataframe: Input DataFrame\n        is_training: Whether to apply training augmentations\n        emphasis_classes: List of classes to emphasize (oversample)\n        \n    Returns:\n        Balanced tf.data.Dataset with emphasis on specified classes\n    \"\"\"\n    balanced_data = []\n    \n    # Sample from each class with emphasis on problematic ones\n    for class_name in sorted(dataframe[\"label\"].unique()):\n        class_df = dataframe[dataframe[\"label\"] == class_name]\n        \n        # Base sampling - reduced for non-problematic classes\n        samples_per_class = 350  \n        \n        # Significantly increase samples for emphasis classes\n        if class_name in emphasis_classes:\n            # 100% more samples for problematic classes (from 400 to 700)\n            samples_per_class = 700  \n            \n        # Sample with replacement if needed\n        if len(class_df) <= samples_per_class:\n            sampled = class_df.sample(n=samples_per_class, replace=True)\n        else:\n            sampled = class_df.sample(n=samples_per_class, replace=False)\n            \n        balanced_data.append(sampled)\n    \n    # Combine all balanced samples\n    balanced_df = pd.concat(balanced_data, ignore_index=True)\n    print(f\"Created balanced dataset with {len(balanced_df)} samples (with emphasis on {emphasis_classes})\")\n    \n    # Create dataset with properly working repeat\n    return create_dataset(balanced_df, is_training=is_training, cache=False)\n\n# =============================================================================\n# Enhanced Confusion Matrix Callback with Class-Specific Monitoring\n# =============================================================================\nclass EnhancedConfusionMatrixCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Enhanced callback to monitor class-specific metrics during training.\n    \"\"\"\n    def __init__(self, validation_data, class_names, log_dir, model_name=\"model\", freq=5):\n        super(EnhancedConfusionMatrixCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.freq = freq\n        self.log_dir = log_dir\n        self.model_name = model_name\n        self.zero_prediction_classes = set()  # Track classes with zero predictions\n        self.class_metrics_history = {cls: [] for cls in class_names}  # Track per-class metrics\n        \n        # Ensure log directory exists\n        ensure_dir(self.log_dir)\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate and log class-specific metrics every epoch\n        val_steps = 20  # Limit computation\n        y_true = []\n        y_pred = []\n        \n        # Get predictions for validation data\n        for i, (images, labels) in enumerate(self.validation_data):\n            if i >= val_steps:\n                break\n            batch_preds = self.model.predict(images, verbose=0)\n            y_pred.append(np.argmax(batch_preds, axis=1))\n            y_true.append(np.argmax(labels.numpy(), axis=1))\n        \n        # Flatten the lists\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n        \n        # Calculate per-class metrics\n        class_accuracies = np.zeros(len(self.class_names))\n        for i in range(len(self.class_names)):\n            if np.sum(y_true == i) > 0:  # Avoid division by zero\n                class_accuracies[i] = cm[i, i] / np.sum(y_true == i)\n                \n            # Track metrics history\n            self.class_metrics_history[self.class_names[i]].append(class_accuracies[i])\n        \n        # Check for classes with zero predictions\n        zero_pred_classes = []\n        for i, class_name in enumerate(self.class_names):\n            if np.sum(cm[:, i]) == 0:\n                zero_pred_classes.append(class_name)\n                self.zero_prediction_classes.add(class_name)\n        \n        # Log warnings for zero prediction classes\n        if zero_pred_classes:\n            warning_msg = f\"\\n WARNING: Zero predictions for classes: {', '.join(zero_pred_classes)}\"\n            print(warning_msg)\n            \n            # Save warning to log file\n            with open(f\"{self.log_dir}/warnings.txt\", \"a\") as f:\n                f.write(f\"Epoch {epoch+1}: {warning_msg}\\n\")\n       \n        # Save visualizations and detailed reports on the specified frequency\n        if (epoch + 1) % self.freq == 0:\n            # Print confusion matrix\n            print(\"\\nConfusion Matrix:\")\n            print(cm)\n            \n            # Print per-class accuracy\n            for i, (name, acc) in enumerate(zip(self.class_names, class_accuracies)):\n                print(f\"{name}: {acc:.4f}\", end=\"  \")\n                if (i + 1) % 4 == 0:\n                    print()  # New line for readability\n            print(\"\\n\")\n            \n            # Print class accuracy trends\n            print(\"Class Accuracy Trends:\")\n            for class_name in self.class_names:\n                history = self.class_metrics_history[class_name]\n                trend = \", \".join([f\"{acc:.4f}\" for acc in history[-5:]])  # Show last 5 epochs\n                print(f\"{class_name}: [{trend}]\")\n            print()\n            \n            # Save confusion matrix visualization\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.class_names,\n                       yticklabels=self.class_names)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title(f'Confusion Matrix - {self.model_name} - Epoch {epoch+1}')\n            plt.tight_layout()\n            \n            try:\n                plt.savefig(f'{self.log_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            except Exception as e:\n                print(f\"Warning: Could not save confusion matrix plot: {e}\")\n            \n            plt.close()\n\n        # Add memory cleanup\n        del images, labels, batch_preds\n        gc.collect()\n\n# =============================================================================\n# FIXED: Create Ensemble Model Architecture with Internal Preprocessing\n# =============================================================================\ndef create_ensemble_model(num_classes=8, freeze_base=True):\n    \"\"\"\n    Create an ensemble with model-specific preprocessing layers.\n    All preprocessing happens inside the model, which expects\n    a standard size input (224x224x3 normalized to [0,1]).\n    \n    Args:\n        num_classes: Number of emotion classes\n        freeze_base: Whether to freeze base models initially\n        \n    Returns:\n        Compiled Keras ensemble model\n    \"\"\"\n    # Create inputs for consistently sized images\n    inputs = keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    \n    # ==================== MobileNetV2 Branch ====================\n    mobilenet_preprocess = Lambda(\n        lambda x: tf.image.resize(x*255.0, [96, 96]) / 127.5 - 1,\n        name='mobilenet_preprocess'\n    )(inputs)\n    \n    # Create MobileNetV2 as standalone model\n    try:\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n    except Exception as e:\n        print(f\"MobileNetV2 imagenet weights failed to load: {e}\")\n        mobilenet_core = MobileNetV2(\n            include_top=False,\n            weights=None,\n            input_shape=(96, 96, 3),\n            name='mobilenet_core'\n        )\n        mobilenet_core.load_weights('weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n    \n    mobilenet_features = mobilenet_core(mobilenet_preprocess)\n    mobilenet_features = GlobalAveragePooling2D(name='mobilenet_gap')(mobilenet_features)\n\n    # ==================== Xception Branch ====================\n    xception_preprocess = Lambda(\n        lambda x: tf.keras.applications.xception.preprocess_input(\n            tf.image.resize(x*255.0, [299, 299])\n        ),\n        name='xception_preprocess'\n    )(inputs)\n    \n    # Create Xception as standalone model\n    try:\n        xception_core = Xception(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n    except Exception as e:\n        print(f\"Xception imagenet weights failed to load: {e}\")\n        xception_core = Xception(\n            include_top=False,\n            weights=None,\n            input_shape=(299, 299, 3),\n            name='xception_core'\n        )\n        xception_core.load_weights('weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n    \n    xception_features = xception_core(xception_preprocess)\n    xception_features = GlobalAveragePooling2D(name='xception_gap')(xception_features)\n\n    # ==================== EfficientNetB0 Branch ====================\n    efficientnet_preprocess = Lambda(\n        lambda x: tf.keras.applications.efficientnet.preprocess_input(x*255.0),\n        name='efficientnet_preprocess'\n    )(inputs)\n    \n    # Create EfficientNetB0 as standalone model\n    try:\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights='imagenet',\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n    except Exception as e:\n        print(f\"EfficientNetB0 imagenet weights failed to load: {e}\")\n        efficientnet_core = EfficientNetB0(\n            include_top=False,\n            weights=None,\n            input_shape=(224, 224, 3),\n            name='efficientnet_core'\n        )\n        try:\n            efficientnet_core.load_weights('/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5')\n        except Exception as e:\n            print(f\"Failed to load local EfficientNetB0 weights: {e}\")\n            print(\"Continuing with random initialization for EfficientNetB0\")\n    \n    efficientnet_features = efficientnet_core(efficientnet_preprocess)\n    efficientnet_features = GlobalAveragePooling2D(name='efficientnet_gap')(efficientnet_features)\n\n    # ==================== Feature Processing ====================\n    def create_projection_head(inputs, name):\n        x = Dense(128, name=f'{name}_projection')(inputs)\n        x = BatchNormalization()(x)\n        return Activation('relu', dtype='float32')(x)\n    \n    mobilenet_features = create_projection_head(mobilenet_features, 'mobilenet')\n    xception_features = create_projection_head(xception_features, 'xception')\n    efficientnet_features = create_projection_head(efficientnet_features, 'efficientnet')\n\n    # ==================== Feature Fusion ====================\n    merged_features = Concatenate(name='feature_fusion')([\n        mobilenet_features,\n        xception_features,\n        efficientnet_features\n    ])\n\n    # ==================== Classification Head ====================\n    x = Dense(256, name='fusion_dense1')(merged_features)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(128, name='fusion_dense2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu', dtype='float32')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(num_classes, activation='softmax', dtype='float32', name='emotion_output')(x)\n\n    # ==================== Model Assembly ====================\n    model = keras.Model(\n        inputs=inputs,\n        outputs=outputs,\n        name='emotion_ensemble'\n    )\n    \n    # Freeze base models if requested\n    if freeze_base:\n        mobilenet_core.trainable = False\n        xception_core.trainable = False\n        efficientnet_core.trainable = False\n\n    # ==================== Compilation ====================\n    # Use a gentler initial learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n        keras.optimizers.Adam(\n            tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=5e-4,  # Reduced from 1e-3\n                decay_steps=15000\n            )\n        )\n    )\n    \n    # Add gradient clipping to avoid instability\n    optimizer.clipnorm = 1.0\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),  # Increased from 2.0 for better focus on hard examples\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# =============================================================================\n# Evaluation function\n# =============================================================================\ndef evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name=\"\"):\n    \"\"\"\n    Evaluate model with detailed metrics and visualizations.\n    \"\"\"\n    print(f\"\\nEvaluating model on {dataset_name} test set\")\n    \n    # Get predictions\n    y_true = []\n    y_pred = []\n    \n    # Loop through test batches\n    for i, (images, labels) in enumerate(test_ds):\n        if i >= steps:\n            break\n        batch_preds = model.predict(images, verbose=0)\n        y_pred.append(np.argmax(batch_preds, axis=1))\n        y_true.append(np.argmax(labels.numpy(), axis=1))\n    \n    # Concatenate\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    # Calculate metrics\n    test_accuracy = np.mean(y_pred == y_true)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"{dataset_name} Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"{dataset_name} Weighted F1-Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=class_names,\n               yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {dataset_name} Test Set')\n    plt.tight_layout()\n    \n    try:\n        plt.savefig(f'{log_dir}/confusion_matrix_{dataset_name}_test.png')\n    except Exception as e:\n        print(f\"Warning: Could not save confusion matrix plot: {e}\")\n        \n    plt.close()\n    \n    # Print classification report\n    print(f\"\\n{dataset_name} Classification Report:\")\n    report = classification_report(\n        y_true, \n        y_pred, \n        target_names=class_names,\n        zero_division=0\n    )\n    print(report)\n    \n    # Save report to file\n    with open(f'{log_dir}/classification_report_{dataset_name}.txt', 'w') as f:\n        f.write(report)\n    \n    return {\n        'accuracy': test_accuracy,\n        'f1_score': f1,\n        'confusion_matrix': cm\n    }\n\n# =============================================================================\n# FIXED: Progressive Training Strategy for Ensemble with Corrected Layer Names\n# =============================================================================\ndef train_ensemble_with_progressive_strategy(model, train_ds, val_ds, \n                                           steps_per_epoch, val_steps,\n                                           total_epochs=30,\n                                           callbacks=None,\n                                           class_weights=None):\n    \"\"\"\n    Three-stage training approach for ensemble:\n    1. Train only the fusion layers (all base models frozen)\n    2. Unfreeze and train EfficientNet and MobileNet (keep Xception frozen)\n    3. Unfreeze and fine-tune all models\n    \n    Args:\n        model: The ensemble model\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        steps_per_epoch: Steps per training epoch\n        val_steps: Validation steps\n        total_epochs: Total epochs across all stages\n        callbacks: List of callbacks\n        class_weights: Class weights for handling imbalance\n        \n    Returns:\n        Training history\n    \"\"\"\n    histories = []\n    \n    # Stage 1: Train only fusion layers (15% of total epochs)\n    stage1_epochs = max(5, int(total_epochs * 0.15))  # Increased from 10% to 15%\n    print(f\"\\nStage 1: Training only fusion layers ({stage1_epochs} epochs)\")\n    \n    # Ensure base models are frozen\n    mobilenet_core = model.get_layer('mobilenet_core')\n    xception_core = model.get_layer('xception_core')\n    efficientnet_core = model.get_layer('efficientnet_core')\n    \n    # Explicitly freeze all base models\n    mobilenet_core.trainable = False\n    xception_core.trainable = False\n    efficientnet_core.trainable = False\n    \n    # Recompile with stable initial learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train fusion layers\n    history1 = model.fit(\n        train_ds,\n        epochs=stage1_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history1)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 2: Unfreeze and train EfficientNet and MobileNet (35% of total epochs)\n    stage2_epochs = max(7, int(total_epochs * 0.35))  # Increased from 30% to 35%\n    print(f\"\\nStage 2: Training EfficientNet and MobileNet branches ({stage2_epochs} epochs)\")\n    \n    # Get reference to actual base models (FIXED: using correct layer names)\n    mobilenet = model.get_layer('mobilenet_core')\n    efficientnet = model.get_layer('efficientnet_core')\n    \n    # Partially unfreeze base models (last 30 layers of each)\n    for base_model in [mobilenet, efficientnet]:\n        base_model.trainable = True\n        # Freeze early layers, unfreeze later layers\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 30)\n    \n    # Keep Xception frozen\n    xception_core.trainable = False\n    \n    # Recompile with lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=8e-5)  # Gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Train with partial unfreezing\n    history2 = model.fit(\n        train_ds,\n        epochs=stage2_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history2)\n\n    K.clear_session()\n    gc.collect()\n    \n    # Stage 3: Unfreeze all models and fine-tune (remaining epochs)\n    stage3_epochs = total_epochs - stage1_epochs - stage2_epochs\n    print(f\"\\nStage 3: Fine-tuning all models ({stage3_epochs} epochs)\")\n    \n    # Unfreeze Xception (FIXED: using correct layer name)\n    xception = model.get_layer('xception_core')\n    xception.trainable = True\n    \n    # Partially unfreeze layers (last 50 layers of each model)\n    for base_model in [mobilenet, efficientnet, xception]:\n        for i, layer in enumerate(base_model.layers):\n            layer.trainable = (i >= len(base_model.layers) - 50)\n    \n    # Recompile with even lower learning rate\n    optimizer = keras.optimizers.Adam(learning_rate=3e-5)  # Even gentler learning rate\n    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n    optimizer.clipnorm = 1.0  # Add gradient clipping\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=focal_loss(gamma=2.5),\n        metrics=['accuracy']\n    )\n    \n    # Final fine-tuning\n    history3 = model.fit(\n        train_ds,\n        epochs=stage3_epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_ds,\n        validation_steps=val_steps,\n        callbacks=callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    histories.append(history3)\n\n    K.clear_session()\n    gc.collect()\n    \n    return histories\n\n# =============================================================================\n# Main training pipeline with ensemble\n# =============================================================================\ndef train_emotion_ensemble(data_dir):\n    \"\"\"\n    Enhanced sequential training pipeline for emotion recognition ensemble.\n    \n    Args:\n        data_dir: Path to dataset directory\n        \n    Returns:\n        Trained ensemble model and evaluation metrics\n    \"\"\"\n    print(\"Starting enhanced ensemble training for emotion recognition\")\n    \n    # 1. Load and prepare data\n    print(\"\\n1. Loading datasets\")\n    train_dir = os.path.join(data_dir, \"Train\")\n    test_dir = os.path.join(data_dir, \"Test\")\n    \n    train_df_full = build_image_df(train_dir)\n    test_df = build_image_df(test_dir)\n    \n    # Show dataset distributions\n    print(\"\\nAffectNet training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'affectnet']['label'].value_counts())\n    \n    print(\"\\nFER2013 training distribution:\")\n    print(train_df_full[train_df_full['source'] == 'fer2013']['label'].value_counts())\n    \n    # 2. Split test set by dataset source\n    test_affectnet_df = test_df[test_df['source'] == 'affectnet']\n    test_fer_df = test_df[test_df['source'] == 'fer2013']\n    \n    print(f\"\\nTest sets: AffectNet={len(test_affectnet_df)}, FER2013={len(test_fer_df)}\")\n    \n    # Get classes for later use\n    classes = sorted(train_df_full[\"label\"].unique())\n    num_classes = len(classes)\n    print(f\"Classes: {classes}\")\n    \n    # 3. Create validation splits\n    # For AffectNet\n    affectnet_train_df = train_df_full[train_df_full['source'] == 'affectnet']\n    affectnet_train_df, affectnet_val_df = train_test_split(\n        affectnet_train_df, \n        test_size=0.15, \n        stratify=affectnet_train_df[\"label\"], \n        random_state=42\n    )\n    \n    # For FER2013\n    fer_train_df = train_df_full[train_df_full['source'] == 'fer2013']\n    fer_train_df, fer_val_df = train_test_split(\n        fer_train_df, \n        test_size=0.15, \n        stratify=fer_train_df[\"label\"], \n        random_state=42\n    )\n    \n    print(f\"AffectNet: {len(affectnet_train_df)} train, {len(affectnet_val_df)} validation\")\n    print(f\"FER2013: {len(fer_train_df)} train, {len(fer_val_df)} validation\")\n    \n    # 4. Create datasets with emphasis on problematic classes and caching\n    print(\"\\n2. Creating enhanced data pipelines with caching\")\n    \n    # Create datasets with memory optimizations\n    affectnet_train_ds, class_indices = create_emphasis_dataset(\n        affectnet_train_df, is_training=True\n    )\n    \n    affectnet_val_ds, _ = create_dataset(\n        affectnet_val_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    affectnet_test_ds, _ = create_dataset(\n        test_affectnet_df, is_training=False, \n        dataset_type='affectnet', cache=True\n    )\n    \n    fer_train_ds, _ = create_emphasis_dataset(\n        fer_train_df, is_training=True\n    )\n    \n    fer_val_ds, _ = create_dataset(\n        fer_val_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    fer_test_ds, _ = create_dataset(\n        test_fer_df, is_training=False, \n        dataset_type='fer2013', cache=True\n    )\n    \n    # Create combined test dataset\n    combined_test_ds, _ = create_dataset(\n        test_df, is_training=False, cache=True)\n\n    # Add periodic garbage collection during evaluation\n    def evaluate_with_gc(model, test_ds, steps, class_names, log_dir, dataset_name):\n        metrics = evaluate_model(model, test_ds, steps, class_names, log_dir, dataset_name)\n        K.clear_session()\n        gc.collect()\n        return metrics\n    \n    # 5. Calculate steps with increased batch size\n    affectnet_steps_per_epoch = len(affectnet_train_df) // BATCH_SIZE\n    affectnet_val_steps = len(affectnet_val_df) // BATCH_SIZE\n    affectnet_test_steps = len(test_affectnet_df) // BATCH_SIZE\n    \n    fer_steps_per_epoch = len(fer_train_df) // BATCH_SIZE\n    fer_val_steps = len(fer_val_df) // BATCH_SIZE\n    fer_test_steps = len(test_fer_df) // BATCH_SIZE\n    \n    combined_test_steps = len(test_df) // BATCH_SIZE\n    \n    # 6. Create ensemble model\n    print(\"\\n3. Creating ensemble model architecture\")\n    ensemble_model = create_ensemble_model(num_classes=num_classes, freeze_base=True)\n    print(f\"Ensemble model created with {ensemble_model.count_params():,} parameters\")\n    \n    # 7. Compute class weights for each dataset with more aggressive adjustments\n    print(\"\\n4. Computing class weights with more aggressive adjustments for problematic classes\")\n    \n    # AffectNet class weights\n    affectnet_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(affectnet_train_df[\"label\"].values),\n        y=affectnet_train_df[\"label\"].values\n    )\n    affectnet_class_weights = {class_indices[label]: weight for label, weight in \n                     zip(np.unique(affectnet_train_df[\"label\"].values), affectnet_weights)}\n    \n    # FER2013 class weights\n    fer_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(fer_train_df[\"label\"].values),\n        y=fer_train_df[\"label\"].values\n    )\n    fer_class_weights = {class_indices[label]: weight for label, weight in \n                zip(np.unique(fer_train_df[\"label\"].values), fer_weights)}\n    \n    # Increase weights for problematic classes more aggressively\n    for problem_class in PROBLEMATIC_CLASSES:\n        if problem_class in class_indices:\n            class_idx = class_indices[problem_class]\n            # Increase the weight by 50% (up from 20%)\n            if class_idx in affectnet_class_weights:\n                affectnet_class_weights[class_idx] *= 1.5\n            if class_idx in fer_class_weights:\n                fer_class_weights[class_idx] *= 1.5\n    \n    print(\"Enhanced AffectNet class weights:\", affectnet_class_weights)\n    print(\"Enhanced FER2013 class weights:\", fer_class_weights)\n    \n    # 8. Setup callbacks with enhanced monitoring\n    print(\"\\n5. Setting up enhanced callbacks\")\n    \n    # Base callbacks shared across training phases\n    base_callbacks = [\n        # Early stopping with more patience\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,  # Increased from 8\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate monitoring\n        tf.keras.callbacks.TensorBoard(\n            log_dir=LOG_DIR + '/ensemble',\n            histogram_freq=1,\n            update_freq='epoch'\n        ),\n        # Learning rate reduction on plateau\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            verbose=1,\n            min_lr=1e-7\n        )\n    ]\n    \n    # AffectNet-specific callbacks\n    affectnet_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/affectnet_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            affectnet_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/affectnet',\n            model_name=\"AffectNet Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'affectnet_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # FER2013-specific callbacks\n    fer_callbacks = base_callbacks + [\n        # Model checkpoint\n        tf.keras.callbacks.ModelCheckpoint(\n            'model_checkpoints/fer2013_ensemble_best.weights.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        # Enhanced confusion matrix monitoring\n        EnhancedConfusionMatrixCallback(\n            fer_val_ds, \n            classes, \n            log_dir=LOG_DIR + '/fer2013',\n            model_name=\"FER2013 Ensemble\",\n            freq=3\n        ),\n        # CSV Logger\n        tf.keras.callbacks.CSVLogger(\n            'fer2013_ensemble_training_log.csv', \n            append=True\n        )\n    ]\n    \n    # 9. STAGE 1: Train ensemble on AffectNet using progressive strategy\n    print(\"\\n6. STAGE 1: Training ensemble on AffectNet with progressive strategy\")\n    \n    affectnet_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        affectnet_train_ds, affectnet_val_ds,\n        affectnet_steps_per_epoch, affectnet_val_steps,\n        total_epochs=20,  # Adjust as needed\n        callbacks=affectnet_callbacks,\n        class_weights=affectnet_class_weights\n    )\n    \n    # Save AffectNet model\n    ensemble_model.save(\"affectnet_ensemble_model.h5\")\n    print(\"AffectNet ensemble model saved to 'affectnet_ensemble_model.h5'\")\n    \n    # 10. Evaluate on AffectNet test set\n    affectnet_metrics = evaluate_with_gc(\n        ensemble_model, \n        affectnet_test_ds, \n        affectnet_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"AffectNet\"\n    )\n    \n    # 11. STAGE 2: Fine-tune on FER2013 with progressive strategy\n    print(\"\\n7. STAGE 2: Fine-tuning ensemble on FER2013 with progressive strategy\")\n    \n    fer_histories = train_ensemble_with_progressive_strategy(\n        ensemble_model,\n        fer_train_ds, fer_val_ds,\n        fer_steps_per_epoch, fer_val_steps,\n        total_epochs=15,  # Adjust as needed\n        callbacks=fer_callbacks,\n        class_weights=fer_class_weights\n    )\n    \n    # 12. Evaluate on FER2013 test set\n    fer_metrics = evaluate_with_gc(\n        ensemble_model, \n        fer_test_ds, \n        fer_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"FER2013\"\n    )\n    \n    # 13. Evaluate on combined test set\n    combined_metrics = evaluate_with_gc(\n        ensemble_model, \n        combined_test_ds, \n        combined_test_steps,\n        classes,\n        LOG_DIR,\n        dataset_name=\"Combined\"\n    )\n    \n    # 14. Save the final model\n    ensemble_model.save(\"final_emotion_ensemble.h5\")\n    print(\"Final ensemble model saved to 'final_emotion_ensemble.h5'\")\n    \n    # Return model and metrics\n    return ensemble_model, {\n        'affectnet': affectnet_metrics,\n        'fer2013': fer_metrics,\n        'combined': combined_metrics\n    }\n\n# =============================================================================\n# Main entry point\n# =============================================================================\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/custom-fer2013affectnet/Custom_ferAffect2013net\"\n    \n    # Add memory monitoring\n    print(\"Initial RAM usage:\", psutil.virtual_memory().percent)\n    model, metrics = train_emotion_ensemble(data_dir)\n    print(\"Final RAM usage:\", psutil.virtual_memory().percent)\n      \n    # Print final results\n    print(\"\\n=== FINAL RESULTS ===\")\n    print(f\"AffectNet Test Accuracy: {metrics['affectnet']['accuracy']:.4f}\")\n    print(f\"AffectNet F1 Score: {metrics['affectnet']['f1_score']:.4f}\")\n    print(f\"FER2013 Test Accuracy: {metrics['fer2013']['accuracy']:.4f}\")\n    print(f\"FER2013 F1 Score: {metrics['fer2013']['f1_score']:.4f}\")\n    print(f\"Combined Test Accuracy: {metrics['combined']['accuracy']:.4f}\")\n    print(f\"Combined F1 Score: {metrics['combined']['f1_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fer_dirs = glob.glob(os.path.join(train_dir, \"*\", \"fer2013\"))\naff_dirs = glob.glob(os.path.join(train_dir, \"*\", \"affectnet\"))\nprint(\"FER directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"fer2013\")))\nprint(\"AffectNet directories found:\", glob.glob(os.path.join(train_dir, \"*\", \"affectnet\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}